import sys
sys.path.insert(0, '../../packages/data_handlers')
from DustToPandasHandler_MultiStations import *
import numpy as np

data_dir = "../../data"
dust_pm10_filename = f"{data_dir}/data_pm10_all_stations.csv"
dust_pm25_filename = f"{data_dir}/data_pm25_all_stations.csv"
result_filename = f"{data_dir}/dust_multistations/dust_df_all_stations_2000_to_2018_pm10_pm25_past7d_future7d"
metadata_base_filename = f"{data_dir}/dust_multistations/metadata/dust_df_all_stations_2000_to_2018_pm10_pm25_past7d_future7d_metadata"
debug_result_filename = f"{data_dir}/dust_multistations/debug_dataframe.pkl"

# lags is in hours!
lags = [0]+[i for i in range(-7*24,0,3)]+[i for i in range(3,7*24+3,3)]
print(lags, len(lags))

loaded_files = [
    DustToPandasHandler_MultiStations.load_csv_file_without_handler(dust_pm10_filename),
    DustToPandasHandler_MultiStations.load_csv_file_without_handler(dust_pm25_filename),
]

from joblib import Parallel, delayed 

years=list(range(2000,2020))

Parallel(n_jobs=3,verbose=100)(delayed(DustToPandasHandler_MultiStations)(
        num_hours_to_avg="3h", lags=lags, delta_hours=3, saveto=f"{result_filename}_{year}.pkl", 
        avg_th=0, debug=False, keep_na=False, verbose=0, 
        stations_num_values_th=0, station_metadata_cols=None, pm_types=["PM10","PM25"], 
        metadata_base_filename=metadata_base_filename, csv_filenames=None,
        years=[year], loaded_files=loaded_files
    )    
    for year in years) 

# for year in tqdm(years):
#     print(f"\n\n####### YEARS: {year} #######")
#     dust_handler = DustToPandasHandler_MultiStations(
#         num_hours_to_avg="3h", lags=lags, delta_hours=3, saveto=f"{result_filename}_{year}.pkl", 
#         avg_th=0, debug=False, keep_na=False, verbose=0, 
#         stations_num_values_th=0, station_metadata_cols=None, pm_types=["PM10","PM25"], 
#         metadata_base_filename=metadata_base_filename, csv_filenames=None,
#         years=[year], loaded_files=loaded_files
#     )

# debug_dataframe = dust_handler.combined_dataframe#[20000:30000]
# debug_dataframe

# torch.save(debug_dataframe,debug_result_filename)






