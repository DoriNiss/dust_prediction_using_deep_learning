<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>draft_models_presentation_sequence</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span><span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">timm.models.vision_transformer</span> <span class="k">as</span> <span class="nn">ViT</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.dataloader</span> <span class="kn">import</span> <span class="n">default_collate</span> 
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingWarmRestarts</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;../../packages/&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">data_handlers.DustPredictionDataset</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">training.train_model</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">data_handlers.augmentations</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[96]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !nvidia-smi</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[97]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !pip install git+https://github.com/pvigier/perlin-numpy</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[98]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># paths - to be saved in a file?</span>

<span class="n">debug_dir</span> <span class="o">=</span> <span class="s2">&quot;../../data/tensors_debug_1/&quot;</span>
<span class="n">debug_meteorology_train_path</span> <span class="o">=</span> <span class="n">debug_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span>
<span class="n">debug_meteorology_valid_path</span> <span class="o">=</span> <span class="n">debug_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span>
<span class="n">debug_dust_train_path</span> <span class="o">=</span> <span class="n">debug_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span>
<span class="n">debug_dust_valid_path</span> <span class="o">=</span> <span class="n">debug_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;../../data/tensors_meteo20000101to20210630_dust_0_m24_24_48_72/&quot;</span>
<span class="n">split1_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">+</span><span class="s2">&quot;split1_ordered/&quot;</span>
<span class="n">split2_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">+</span><span class="s2">&quot;split2_extreme_ratios_in_train_avg_in_valid/&quot;</span>
<span class="n">split3_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">+</span><span class="s2">&quot;split3_extreme_ratios_in_valid_avg_in_train/&quot;</span>
<span class="n">split4_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">+</span><span class="s2">&quot;split4_max_num_events_in_train_avg_valid/&quot;</span>
<span class="n">split5_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">+</span><span class="s2">&quot;split5_train_distant_years_valid_between/&quot;</span>
<span class="n">split_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="n">split1_dir</span><span class="p">,</span><span class="n">split2_dir</span><span class="p">,</span><span class="n">split3_dir</span><span class="p">,</span><span class="n">split4_dir</span><span class="p">,</span><span class="n">split5_dir</span><span class="p">]</span>
<span class="n">meteorology_train_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">split_dirs</span><span class="p">]</span>
<span class="n">meteorology_valid_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">split_dirs</span><span class="p">]</span>
<span class="n">dust_train_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">split_dirs</span><span class="p">]</span>
<span class="n">dust_valid_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">split_dirs</span><span class="p">]</span>

<span class="n">metadata_dir</span> <span class="o">=</span> <span class="s2">&quot;../../data/metadata_meteo20000101to20210630_dust_0_m24_24_48_72/&quot;</span>
<span class="n">metadata_columns_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;all_columns.pkl&quot;</span>
<span class="n">metadata_yearly_statistics_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;yearly_statistics.pkl&quot;</span>
<span class="n">metadata_all_times_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;all_times.pkl&quot;</span>
<span class="n">metadata_times_split1_train_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split1_ordered_train_times.pkl&quot;</span>
<span class="n">metadata_times_split2_train_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split2_extreme_ratios_in_train_avg_in_valid_train_times.pkl&quot;</span>
<span class="n">metadata_times_split3_train_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split3_extreme_ratios_in_valid_avg_in_train_train_times.pkl&quot;</span>
<span class="n">metadata_times_split4_train_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split4_max_num_events_in_train_avg_valid_train_times.pkl&quot;</span>
<span class="n">metadata_times_split5_train_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split5_train_distant_years_valid_between_train_times.pkl&quot;</span>
<span class="n">metadata_times_train_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">metadata_times_split1_train_path</span><span class="p">,</span><span class="n">metadata_times_split2_train_path</span><span class="p">,</span>
                              <span class="n">metadata_times_split3_train_path</span><span class="p">,</span><span class="n">metadata_times_split4_train_path</span><span class="p">,</span>
                              <span class="n">metadata_times_split5_train_path</span><span class="p">]</span>
<span class="n">metadata_times_split1_valid_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split1_ordered_valid_times.pkl&quot;</span>
<span class="n">metadata_times_split2_valid_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split2_extreme_ratios_in_train_avg_in_valid_valid_times.pkl&quot;</span>
<span class="n">metadata_times_split3_valid_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split3_extreme_ratios_in_valid_avg_in_train_valid_times.pkl&quot;</span>
<span class="n">metadata_times_split4_valid_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split4_max_num_events_in_train_avg_valid_valid_times.pkl&quot;</span>
<span class="n">metadata_times_split5_valid_path</span> <span class="o">=</span> <span class="n">metadata_dir</span><span class="o">+</span><span class="s2">&quot;split5_train_distant_years_valid_between_valid_times.pkl&quot;</span>
<span class="n">metadata_times_valid_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">metadata_times_split1_valid_path</span><span class="p">,</span><span class="n">metadata_times_split2_valid_path</span><span class="p">,</span>
                              <span class="n">metadata_times_split3_valid_path</span><span class="p">,</span><span class="n">metadata_times_split4_valid_path</span><span class="p">,</span>
                              <span class="n">metadata_times_split5_valid_path</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[99]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TBD: get times of debug dataset</span>
<span class="c1"># import sys</span>
<span class="c1"># sys.path.insert(0, &#39;../../packages/data_handlers&#39;)</span>
<span class="c1"># from Dataset_handler import *</span>
<span class="c1"># debug_dataset_handler = torch.load(debug_dir+&quot;dummy_dataset_handler.pkl&quot;) # not working</span>
<span class="n">times_debug</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_split1_valid_path</span><span class="p">)[:</span><span class="mi">48</span><span class="p">]</span> <span class="c1"># incorrect times - to be corrected</span>
<span class="nb">len</span><span class="p">(</span><span class="n">times_debug</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[99]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>48</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[100]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_columns_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[100]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>Index([&#39;SLP&#39;, &#39;Z&#39;, &#39;U&#39;, &#39;V&#39;, &#39;PV&#39;, &#39;dust_0&#39;, &#39;delta_0&#39;, &#39;dust_m24&#39;,
       &#39;delta_m24&#39;, &#39;dust_24&#39;, &#39;delta_24&#39;, &#39;dust_48&#39;, &#39;delta_48&#39;, &#39;dust_72&#39;,
       &#39;delta_72&#39;],
      dtype=&#39;object&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[101]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">device</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[101]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>device(type=&#39;cuda&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[102]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # debug</span>
<span class="c1"># train_dataset = DustPredictionDataset(torch.load(debug_meteorology_train_path),</span>
<span class="c1">#                                       torch.load(debug_dust_train_path),times_debug)</span>
<span class="c1"># valid_dataset = DustPredictionDataset(torch.load(debug_meteorology_valid_path),</span>
<span class="c1">#                                       torch.load(debug_dust_valid_path),times_debug)</span>
<span class="c1"># train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=train_dataset.collate_fn)</span>
<span class="c1"># valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True,collate_fn=valid_dataset.collate_fn)</span>

<span class="c1"># model = ViT.VisionTransformer(img_size=(81,81), patch_size=(9,9), in_chans=17, num_classes=10, embed_dim=512, depth=8,</span>
<span class="c1">#                  num_heads=8, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,</span>
<span class="c1">#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.)</span>
<span class="c1"># model = model.to(device)</span>

<span class="c1"># lr = 0.01</span>
<span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span>

<span class="c1"># debug_best_path = debug_dir+&quot;best_debug_model.pt&quot;</span>
<span class="c1"># debug_last_path = debug_dir+&quot;last_debug_model.pt&quot;</span>

<span class="c1"># num_epochs = 8</span>
<span class="c1"># (debug_train_losses,debug_valid_losses,</span>
<span class="c1">#  debug_train_lags_loss,debug_train_delta_lags_losses,</span>
<span class="c1">#  debug_valid_lags_loss,debug_valid_delta_lags_losses) = train_loop(model, optimizer, train_loader, valid_loader, </span>
<span class="c1">#                                                                    device, epochs=num_epochs, valid_every=1, </span>
<span class="c1">#                                                                    loss_cfg=None, sample_predictions_every=2, </span>
<span class="c1">#                                                                    sample_size=5, sample_cols=[0],loss_plot_end=True,</span>
<span class="c1">#                                                                    save_best_model_dict_to=debug_best_path, </span>
<span class="c1">#                                                                    save_last_model_dict_to=debug_last_path)</span>
<span class="c1"># debug_train_lags_loss.shape,debug_train_delta_lags_losses.shape,debug_valid_delta_lags_losses.shape</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[103]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # checking augmentation:</span>

<span class="c1"># perlin_augmentation = PerlinAugmentation(torch.load(debug_meteorology_train_path)[:11,:,:,:], </span>
<span class="c1">#                                          torch.load(debug_dust_train_path)[:11,:], debug=True)</span>

<span class="c1"># train_dataset = DustPredictionDataset(torch.load(debug_meteorology_train_path),</span>
<span class="c1">#                                       torch.load(debug_dust_train_path),times_debug, augmentation=perlin_augmentation)</span>
<span class="c1"># valid_dataset = DustPredictionDataset(torch.load(debug_meteorology_valid_path),</span>
<span class="c1">#                                       torch.load(debug_dust_valid_path),times_debug)</span>
<span class="c1"># train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=train_dataset.collate_fn)</span>
<span class="c1"># valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True,collate_fn=valid_dataset.collate_fn)</span>

<span class="c1"># model = ViT.VisionTransformer(img_size=(81,81), patch_size=(9,9), in_chans=17, num_classes=10, embed_dim=512, depth=8,</span>
<span class="c1">#                  num_heads=8, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,</span>
<span class="c1">#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.)</span>
<span class="c1"># model = model.to(device)</span>

<span class="c1"># lr = 0.01</span>
<span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span>

<span class="c1"># debug_best_path = debug_dir+&quot;best_debug_model.pt&quot;</span>
<span class="c1"># debug_last_path = debug_dir+&quot;last_debug_model.pt&quot;</span>

<span class="c1"># num_epochs = 8</span>
<span class="c1"># (debug_train_losses,debug_valid_losses,</span>
<span class="c1">#  debug_train_lags_loss,debug_train_delta_lags_losses,</span>
<span class="c1">#  debug_valid_lags_loss,debug_valid_delta_lags_losses) = train_loop(model, optimizer, train_loader, valid_loader, </span>
<span class="c1">#                                                                    device, epochs=num_epochs, valid_every=1, </span>
<span class="c1">#                                                                    loss_cfg=None, sample_predictions_every=2, </span>
<span class="c1">#                                                                    sample_size=5, sample_cols=[0],loss_plot_end=True,</span>
<span class="c1">#                                                                    save_best_model_dict_to=debug_best_path, </span>
<span class="c1">#                                                                    save_last_model_dict_to=debug_last_path)</span>
<span class="c1"># debug_train_lags_loss.shape,debug_train_delta_lags_losses.shape,debug_valid_delta_lags_losses.shape</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[104]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !nvidia-smi</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[105]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results_dir</span> <span class="o">=</span> <span class="s2">&quot;../../results_models/presentation/&quot;</span>
<span class="c1">#</span>
<span class="n">presentation_dir</span> <span class="o">=</span> <span class="n">data_dir</span><span class="o">+</span><span class="s2">&quot;presentation_set/&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[106]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # Presentation set, no augmentation, weighted loss with high deltas weights (10)</span>

<span class="c1"># results_dir_specific = results_dir+&quot;presentation_set_e600_lr0p0001_noaugmentation_weighted_loss_high_deltas/&quot;</span>

<span class="c1"># loss_cfg = LossConfig(device, weights_lags=[10.,1.,1.,1.,1.], weights_delta_lags=[50.,50.,50.,50.,50.])</span>


<span class="c1"># best_model_path = results_dir_specific+&quot;best_model_state.pt&quot;</span>
<span class="c1"># last_model_path = results_dir_specific+&quot;last_model_state.pt&quot;</span>

<span class="c1"># train_losses_path = results_dir_specific+&quot;train_losses.pkl&quot;</span>
<span class="c1"># train_lags_losses_path = results_dir_specific+&quot;train_lags_losses.pkl&quot;</span>
<span class="c1"># train_delta_lags_losses_path = results_dir_specific+&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="c1"># valid_losses_path = results_dir_specific+&quot;valid_losses.pkl&quot;</span>
<span class="c1"># valid_lags_losses_path = results_dir_specific+&quot;valid_lags_losses.pkl&quot;</span>
<span class="c1"># valid_delta_lags_losses_path = results_dir_specific+&quot;valid_delta_lags_losses.pkl&quot;</span>



<span class="c1"># train_dataset = DustPredictionDataset(torch.load(presentation_dir+&quot;tensor_train_meteorology.pkl&quot;),</span>
<span class="c1">#                                       torch.load(presentation_dir+&quot;tensor_train_dust.pkl&quot;),</span>
<span class="c1">#                                       torch.load(presentation_dir+&quot;times_train.pkl&quot;))</span>
<span class="c1"># valid_dataset = DustPredictionDataset(torch.load(presentation_dir+&quot;tensor_valid_meteorology.pkl&quot;),</span>
<span class="c1">#                                       torch.load(presentation_dir+&quot;tensor_valid_dust.pkl&quot;),</span>
<span class="c1">#                                       torch.load(presentation_dir+&quot;times_valid.pkl&quot;))</span>

<span class="c1"># train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=train_dataset.collate_fn)</span>
<span class="c1"># valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=True, collate_fn=valid_dataset.collate_fn)</span>

<span class="c1"># model = ViT.VisionTransformer(img_size=(81,81), patch_size=(9,9), in_chans=17, num_classes=10, embed_dim=512, depth=8,</span>
<span class="c1">#                               num_heads=8, mlp_ratio=2., qkv_bias=True, representation_size=None, distilled=False,</span>
<span class="c1">#                               drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1)</span>
<span class="c1"># model = model.to(device)</span>

<span class="c1"># lr = 0.0001</span>

<span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span>
<span class="c1"># scheduler = CosineAnnealingWarmRestarts(optimizer, 50, 2)</span>

<span class="c1"># num_epochs = 2</span>

<span class="c1"># (train_losses,valid_losses,</span>
<span class="c1">#  train_lags_losses,train_delta_lags_losses,</span>
<span class="c1">#  valid_lags_losses,valid_delta_lags_losses) = train_loop(model, optimizer, train_loader, valid_loader, </span>
<span class="c1">#                                                          device, epochs=num_epochs, valid_every=1, </span>
<span class="c1">#                                                          loss_cfg=loss_cfg, sample_predictions_every=2, </span>
<span class="c1">#                                                          sample_size=5, sample_cols=[0],loss_plot_end=True,</span>
<span class="c1">#                                                          save_best_model_dict_to=best_model_path, </span>
<span class="c1">#                                                          save_last_model_dict_to=last_model_path, debug=False,</span>
<span class="c1">#                                                          scheduler=scheduler, losses_dir=results_dir_specific)</span>

<span class="c1"># torch.save(train_losses,train_losses_path)</span>
<span class="c1"># torch.save(train_lags_losses,train_lags_losses_path)</span>
<span class="c1"># torch.save(train_delta_lags_losses,train_delta_lags_losses_path)</span>
<span class="c1"># torch.save(valid_losses,valid_losses_path)</span>
<span class="c1"># torch.save(valid_lags_losses,valid_lags_losses_path)</span>
<span class="c1"># torch.save(valid_delta_lags_losses,valid_delta_lags_losses_path)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[107]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># train_lags_losses, train_delta_lags_losses</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[108]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 (1, 1)
1 (2, 2)
2 (3, 3)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[109]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">meteorology_presentation_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span><span class="p">)</span>
<span class="n">dust_presentation_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span><span class="p">)</span>
<span class="n">times_presentation_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_train.pkl&quot;</span><span class="p">)</span>
<span class="n">meteorology_presentation_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span><span class="p">)</span>
<span class="n">dust_presentation_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span><span class="p">)</span>
<span class="n">times_presentation_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_valid.pkl&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[110]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">meteorology_presentation_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">meteorology_presentation_valid</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">times_presentation_train</span><span class="p">,</span><span class="n">times_presentation_valid</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[110]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([37469, 17, 81, 81]),
 torch.Size([2183, 17, 81, 81]),
 DatetimeIndex([&#39;2000-06-16 00:00:00+00:00&#39;, &#39;2000-06-16 03:00:00+00:00&#39;,
                &#39;2000-06-16 06:00:00+00:00&#39;, &#39;2000-06-16 09:00:00+00:00&#39;,
                &#39;2000-06-16 12:00:00+00:00&#39;, &#39;2000-06-16 15:00:00+00:00&#39;,
                &#39;2000-06-16 18:00:00+00:00&#39;, &#39;2000-06-16 21:00:00+00:00&#39;,
                &#39;2000-06-17 00:00:00+00:00&#39;, &#39;2000-06-17 03:00:00+00:00&#39;,
                ...
                &#39;2019-12-30 18:00:00+00:00&#39;, &#39;2019-12-30 21:00:00+00:00&#39;,
                &#39;2019-12-31 00:00:00+00:00&#39;, &#39;2019-12-31 03:00:00+00:00&#39;,
                &#39;2019-12-31 06:00:00+00:00&#39;, &#39;2019-12-31 09:00:00+00:00&#39;,
                &#39;2019-12-31 12:00:00+00:00&#39;, &#39;2019-12-31 15:00:00+00:00&#39;,
                &#39;2019-12-31 18:00:00+00:00&#39;, &#39;2019-12-31 21:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, length=37469, freq=None),
 DatetimeIndex([&#39;2017-01-01 00:00:00+00:00&#39;, &#39;2017-01-01 09:00:00+00:00&#39;,
                &#39;2017-01-01 12:00:00+00:00&#39;, &#39;2017-01-01 21:00:00+00:00&#39;,
                &#39;2017-01-02 00:00:00+00:00&#39;, &#39;2017-01-02 09:00:00+00:00&#39;,
                &#39;2017-01-02 12:00:00+00:00&#39;, &#39;2017-01-02 21:00:00+00:00&#39;,
                &#39;2017-01-03 00:00:00+00:00&#39;, &#39;2017-01-03 09:00:00+00:00&#39;,
                ...
                &#39;2017-12-30 18:00:00+00:00&#39;, &#39;2017-12-30 21:00:00+00:00&#39;,
                &#39;2017-12-31 00:00:00+00:00&#39;, &#39;2017-12-31 03:00:00+00:00&#39;,
                &#39;2017-12-31 06:00:00+00:00&#39;, &#39;2017-12-31 09:00:00+00:00&#39;,
                &#39;2017-12-31 12:00:00+00:00&#39;, &#39;2017-12-31 15:00:00+00:00&#39;,
                &#39;2017-12-31 18:00:00+00:00&#39;, &#39;2017-12-31 21:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, length=2183, freq=None))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[111]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">all_times</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">times_presentation_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">times_presentation_train</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">freq</span><span class="o">=</span><span class="s2">&quot;3H&quot;</span><span class="p">)</span>
<span class="n">all_times</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_times</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[111]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(DatetimeIndex([&#39;2000-06-16 00:00:00+00:00&#39;, &#39;2000-06-16 03:00:00+00:00&#39;,
                &#39;2000-06-16 06:00:00+00:00&#39;, &#39;2000-06-16 09:00:00+00:00&#39;,
                &#39;2000-06-16 12:00:00+00:00&#39;, &#39;2000-06-16 15:00:00+00:00&#39;,
                &#39;2000-06-16 18:00:00+00:00&#39;, &#39;2000-06-16 21:00:00+00:00&#39;,
                &#39;2000-06-17 00:00:00+00:00&#39;, &#39;2000-06-17 03:00:00+00:00&#39;,
                ...
                &#39;2019-12-30 18:00:00+00:00&#39;, &#39;2019-12-30 21:00:00+00:00&#39;,
                &#39;2019-12-31 00:00:00+00:00&#39;, &#39;2019-12-31 03:00:00+00:00&#39;,
                &#39;2019-12-31 06:00:00+00:00&#39;, &#39;2019-12-31 09:00:00+00:00&#39;,
                &#39;2019-12-31 12:00:00+00:00&#39;, &#39;2019-12-31 15:00:00+00:00&#39;,
                &#39;2019-12-31 18:00:00+00:00&#39;, &#39;2019-12-31 21:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, length=57104, freq=&#39;3H&#39;),
 57104)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[112]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">idxs_to_cat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="o">-</span><span class="mi">12</span><span class="p">,</span><span class="o">-</span><span class="mi">14</span><span class="p">,</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span><span class="o">-</span><span class="mi">18</span><span class="p">])</span> <span class="c1"># these will be the dates to be concated (17*12 channels)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[113]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">all_times</span><span class="p">[</span><span class="n">idxs_to_cat</span><span class="o">+</span><span class="mi">500</span><span class="p">],</span> <span class="n">all_times</span><span class="p">[</span><span class="mi">500</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[113]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(DatetimeIndex([&#39;2000-08-17 12:00:00+00:00&#39;, &#39;2000-08-17 06:00:00+00:00&#39;,
                &#39;2000-08-17 00:00:00+00:00&#39;, &#39;2000-08-16 18:00:00+00:00&#39;,
                &#39;2000-08-16 12:00:00+00:00&#39;, &#39;2000-08-16 06:00:00+00:00&#39;,
                &#39;2000-08-16 00:00:00+00:00&#39;, &#39;2000-08-15 18:00:00+00:00&#39;,
                &#39;2000-08-15 12:00:00+00:00&#39;, &#39;2000-08-15 06:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, freq=None),
 Timestamp(&#39;2000-08-17 12:00:00+0000&#39;, tz=&#39;UTC&#39;, freq=&#39;3H&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[114]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">conditions</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_times</span><span class="p">[</span><span class="n">idxs_to_cat</span><span class="o">+</span><span class="mi">500</span><span class="p">]</span><span class="o">==</span><span class="n">all_times</span><span class="p">[</span><span class="n">idxs_to_cat</span><span class="o">+</span><span class="mi">501</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">conditions</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[114]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>False</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[115]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">date</span> <span class="o">=</span> <span class="n">all_times</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># date,all_times.loc(date)</span>
<span class="p">(</span><span class="n">times_presentation_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">date</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_times</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[115]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(True, 57104)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[116]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">correct_times</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="n">periods</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s2">&quot;-6H&quot;</span><span class="p">)</span>
<span class="n">date</span><span class="p">,</span> <span class="n">correct_times</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[116]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(Timestamp(&#39;2000-06-16 00:00:00+0000&#39;, tz=&#39;UTC&#39;, freq=&#39;3H&#39;),
 DatetimeIndex([&#39;2000-06-16 00:00:00+00:00&#39;, &#39;2000-06-15 18:00:00+00:00&#39;,
                &#39;2000-06-15 12:00:00+00:00&#39;, &#39;2000-06-15 06:00:00+00:00&#39;,
                &#39;2000-06-15 00:00:00+00:00&#39;, &#39;2000-06-14 18:00:00+00:00&#39;,
                &#39;2000-06-14 12:00:00+00:00&#39;, &#39;2000-06-14 06:00:00+00:00&#39;,
                &#39;2000-06-14 00:00:00+00:00&#39;, &#39;2000-06-13 18:00:00+00:00&#39;,
                &#39;2000-06-13 12:00:00+00:00&#39;, &#39;2000-06-13 06:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, freq=&#39;-6H&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[117]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">correct_times_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">correct_times</span><span class="p">]</span>
<span class="n">correct_times_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">all_times</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[117]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[118]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dates_sequences_idxs</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">periods</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s2">&quot;-6H&quot;</span><span class="p">):</span>
    <span class="n">times_as_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">times</span><span class="p">]</span>
    <span class="n">idxs_of_sequences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">time</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">times</span><span class="p">)):</span>
        <span class="n">sequence_times_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">periods</span><span class="o">=</span><span class="n">periods</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="n">freq</span><span class="p">)]</span>
        <span class="n">idxs_of_sequence</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq_time</span> <span class="ow">in</span> <span class="n">sequence_times_list</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">idxs_of_sequence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">times_as_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">seq_time</span><span class="p">))</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs_of_sequence</span><span class="p">)</span><span class="o">&lt;</span><span class="n">periods</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">idxs_of_sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idxs_of_sequence</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">idxs_of_sequences</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[119]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># get_dates_sequences_idxs(correct_times)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[120]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># sample_sequences_idxs = get_dates_sequences_idxs(all_times[:100])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[121]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># for i in sample_sequences_idxs[3]:</span>
<span class="c1">#     print(i, all_times[i])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[122]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># all_sequences_idxs_train = get_dates_sequences_idxs(times_presentation_train)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[123]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># all_sequences_idxs_valid = get_dates_sequences_idxs(times_presentation_valid)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[124]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># torch.save(all_sequences_idxs_train, presentation_dir+&quot;sequences_idxs_train.pkl&quot;)</span>
<span class="c1"># torch.save(all_sequences_idxs_valid, presentation_dir+&quot;sequences_idxs_valid.pkl&quot;)</span>

<span class="n">all_sequences_idxs_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;sequences_idxs_train.pkl&quot;</span><span class="p">)</span>
<span class="n">all_sequences_idxs_valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;sequences_idxs_valid.pkl&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[125]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">all_sequences_idxs_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_sequences_idxs_valid</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[125]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(27006, 1636)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[126]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_sequence</span> <span class="o">=</span> <span class="n">all_sequences_idxs_train</span><span class="p">[</span><span class="mi">20000</span><span class="p">]</span>
<span class="n">sample_sequence</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[126]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[27272,
 27270,
 27268,
 27266,
 27264,
 27262,
 27260,
 27258,
 27256,
 27254,
 27252,
 27250]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[127]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">([</span><span class="n">times_presentation_train</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sample_sequence</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[Timestamp(&#39;2014-02-07 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 18:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 06:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 18:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 06:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 18:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 06:00:00+0000&#39;, tz=&#39;UTC&#39;)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[128]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">dilute_sequence</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">idxs_to_keep</span><span class="p">):</span>
    <span class="n">sequences_to_keep</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">:</span>
        <span class="n">sequences_to_keep</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs_to_keep</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">sequences_to_keep</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[129]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_sequential_dataset</span><span class="p">(</span><span class="n">meteorology</span><span class="p">,</span> <span class="n">dust</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="n">sequences_idxs</span><span class="p">):</span>
    <span class="n">meteorology_all_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dust_all_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">times_all_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">sequences_idxs</span><span class="p">):</span>
        <span class="n">time_of_row_idx</span> <span class="o">=</span> <span class="n">sequence</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">meteorology_sequence</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">:</span>
            <span class="n">meteorology_sequence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">meteorology</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">meteorology_all_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">meteorology_sequence</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">dust_all_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dust</span><span class="p">[</span><span class="n">time_of_row_idx</span><span class="p">,:])</span>
        <span class="n">times_all_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">times</span><span class="p">[</span><span class="n">time_of_row_idx</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">meteorology_all_list</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">dust_all_list</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">times_all_list</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[130]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_times_sequence</span> <span class="o">=</span> <span class="p">[</span><span class="n">times_presentation_train</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sample_sequence</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_times_sequence</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_times_sequence</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dilute_sequence</span><span class="p">([</span><span class="n">sample_times_sequence</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">11</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[Timestamp(&#39;2014-02-07 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 18:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 06:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 18:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 06:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 18:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 06:00:00+0000&#39;, tz=&#39;UTC&#39;)] 12
[[Timestamp(&#39;2014-02-07 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 12:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-06 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-05 00:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2014-02-04 06:00:00+0000&#39;, tz=&#39;UTC&#39;)]]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[131]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mi">9</span><span class="o">*</span><span class="mi">9</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">17</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[131]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>6885</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[132]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">idxs_to_keep</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">]</span>
<span class="n">sequences_idxs_train_0m12243648</span> <span class="o">=</span> <span class="n">dilute_sequence</span><span class="p">(</span><span class="n">all_sequences_idxs_train</span><span class="p">,</span> <span class="n">idxs_to_keep</span><span class="p">)</span>
<span class="n">sequences_idxs_valid_0m12243648</span> <span class="o">=</span> <span class="n">dilute_sequence</span><span class="p">(</span><span class="n">all_sequences_idxs_valid</span><span class="p">,</span> <span class="n">idxs_to_keep</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sequences_idxs_train_0m12243648</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sequences_idxs_train_0m12243648</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences_idxs_train_0m12243648</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequences_idxs_valid_0m12243648</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sequences_idxs_valid_0m12243648</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences_idxs_valid_0m12243648</span><span class="p">))</span>
<span class="nb">print</span><span class="p">([</span><span class="n">times_presentation_train</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sequences_idxs_train_0m12243648</span><span class="p">[</span><span class="mi">9900</span><span class="p">]])</span>
<span class="c1"># all_sequences_idxs_train_0m12244872 = torch.load(presentation_dir+&quot;sequences_idxs_train.pkl&quot;)</span>
<span class="c1"># all_sequences_idxs_valid_0m12244872 = torch.load(presentation_dir+&quot;sequences_idxs_valid.pkl&quot;)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[22, 18, 14, 10, 6] [30, 26, 22, 18, 14] 27006
[51, 47, 43, 39, 35] [87, 83, 79, 75, 71] 1636
[Timestamp(&#39;2007-04-10 09:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-09 21:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-09 09:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-08 21:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-08 09:00:00+0000&#39;, tz=&#39;UTC&#39;)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[133]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># torch.save(sequences_idxs_train_0m12243648, presentation_dir+&quot;sequences_idxs_train_0m12243648.pkl&quot;)</span>
<span class="c1"># torch.save(sequences_idxs_valid_0m12243648, presentation_dir+&quot;sequences_idxs_valid_0m12243648.pkl&quot;)</span>

<span class="n">sequences_idxs_train_0m12243648</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;sequences_idxs_train_0m12243648.pkl&quot;</span><span class="p">)</span>
<span class="n">sequences_idxs_valid_0m12243648</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;sequences_idxs_valid_0m12243648.pkl&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[134]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sequences_idxs_train_0m12243648</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sequences_idxs_train_0m12243648</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences_idxs_train_0m12243648</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequences_idxs_valid_0m12243648</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sequences_idxs_valid_0m12243648</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences_idxs_valid_0m12243648</span><span class="p">))</span>
<span class="nb">print</span><span class="p">([</span><span class="n">times_presentation_train</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sequences_idxs_train_0m12243648</span><span class="p">[</span><span class="mi">9900</span><span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[22, 18, 14, 10, 6] [30, 26, 22, 18, 14] 27006
[51, 47, 43, 39, 35] [87, 83, 79, 75, 71] 1636
[Timestamp(&#39;2007-04-10 09:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-09 21:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-09 09:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-08 21:00:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2007-04-08 09:00:00+0000&#39;, tz=&#39;UTC&#39;)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[135]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">meteorology_sequential_valid</span><span class="p">,</span>
 <span class="n">dust_sequential_valid</span><span class="p">,</span>
 <span class="n">times_sequential_valid</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_sequential_dataset</span><span class="p">(</span><span class="n">meteorology_presentation_valid</span><span class="p">,</span> 
                                                  <span class="n">dust_presentation_valid</span><span class="p">,</span> 
                                                  <span class="n">times_presentation_valid</span><span class="p">,</span> <span class="n">sequences_idxs_valid_0m12243648</span><span class="p">)</span>

<span class="n">meteorology_sequential_valid</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dust_sequential_valid</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">times_sequential_valid</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|| 1636/1636 [00:00&lt;00:00, 1660.76it/s]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[135]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1636, 85, 81, 81]), torch.Size([1636, 10]), 1636)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[136]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">meteorology_sequential_train</span><span class="p">,</span>
 <span class="n">dust_sequential_train</span><span class="p">,</span>
 <span class="n">times_sequential_train</span><span class="p">)</span> <span class="o">=</span> <span class="n">get_sequential_dataset</span><span class="p">(</span><span class="n">meteorology_presentation_train</span><span class="p">,</span> 
                                                  <span class="n">dust_presentation_train</span><span class="p">,</span> 
                                                  <span class="n">times_presentation_train</span><span class="p">,</span> <span class="n">sequences_idxs_train_0m12243648</span><span class="p">)</span>

<span class="n">meteorology_sequential_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dust_sequential_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">times_sequential_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|| 27006/27006 [00:20&lt;00:00, 1326.51it/s]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[136]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([27006, 85, 81, 81]), torch.Size([27006, 10]), 27006)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[137]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">times_sequential_valid</span><span class="p">,</span> <span class="n">times_sequential_train</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[137]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(DatetimeIndex([&#39;2017-01-09 15:00:00+00:00&#39;, &#39;2017-01-09 18:00:00+00:00&#39;,
                &#39;2017-01-09 21:00:00+00:00&#39;, &#39;2017-01-10 00:00:00+00:00&#39;,
                &#39;2017-01-10 03:00:00+00:00&#39;, &#39;2017-01-10 06:00:00+00:00&#39;,
                &#39;2017-01-20 12:00:00+00:00&#39;, &#39;2017-01-20 15:00:00+00:00&#39;,
                &#39;2017-01-20 18:00:00+00:00&#39;, &#39;2017-01-20 21:00:00+00:00&#39;,
                ...
                &#39;2017-12-30 18:00:00+00:00&#39;, &#39;2017-12-30 21:00:00+00:00&#39;,
                &#39;2017-12-31 00:00:00+00:00&#39;, &#39;2017-12-31 03:00:00+00:00&#39;,
                &#39;2017-12-31 06:00:00+00:00&#39;, &#39;2017-12-31 09:00:00+00:00&#39;,
                &#39;2017-12-31 12:00:00+00:00&#39;, &#39;2017-12-31 15:00:00+00:00&#39;,
                &#39;2017-12-31 18:00:00+00:00&#39;, &#39;2017-12-31 21:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, length=1636, freq=None),
 DatetimeIndex([&#39;2000-06-18 18:00:00+00:00&#39;, &#39;2000-06-18 21:00:00+00:00&#39;,
                &#39;2000-06-19 00:00:00+00:00&#39;, &#39;2000-06-19 03:00:00+00:00&#39;,
                &#39;2000-06-19 06:00:00+00:00&#39;, &#39;2000-06-19 09:00:00+00:00&#39;,
                &#39;2000-06-19 12:00:00+00:00&#39;, &#39;2000-06-19 15:00:00+00:00&#39;,
                &#39;2000-06-19 18:00:00+00:00&#39;, &#39;2000-06-19 21:00:00+00:00&#39;,
                ...
                &#39;2019-12-30 18:00:00+00:00&#39;, &#39;2019-12-30 21:00:00+00:00&#39;,
                &#39;2019-12-31 00:00:00+00:00&#39;, &#39;2019-12-31 03:00:00+00:00&#39;,
                &#39;2019-12-31 06:00:00+00:00&#39;, &#39;2019-12-31 09:00:00+00:00&#39;,
                &#39;2019-12-31 12:00:00+00:00&#39;, &#39;2019-12-31 15:00:00+00:00&#39;,
                &#39;2019-12-31 18:00:00+00:00&#39;, &#39;2019-12-31 21:00:00+00:00&#39;],
               dtype=&#39;datetime64[ns, UTC]&#39;, length=27006, freq=None))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[138]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # torch.save(meteorology_sequential_train,presentation_dir+&quot;meteorology_sequential_train.pkl&quot;)</span>
<span class="c1"># # torch.save(dust_sequential_train,presentation_dir+&quot;dust_sequential_train.pkl&quot;)</span>
<span class="c1"># # torch.save(times_sequential_train,presentation_dir+&quot;times_sequential_train.pkl&quot;)</span>

<span class="c1"># torch.save(meteorology_sequential_valid,presentation_dir+&quot;meteorology_sequential_valid.pkl&quot;)</span>
<span class="c1"># torch.save(dust_sequential_valid,presentation_dir+&quot;dust_sequential_valid.pkl&quot;)</span>
<span class="c1"># torch.save(times_sequential_valid,presentation_dir+&quot;times_sequential_valid.pkl&quot;)</span>

<span class="c1"># torch.save(meteorology_sequential_valid,presentation_dir+&quot;meteorology_sequential_0m12243648_valid.pkl&quot;)</span>
<span class="c1"># torch.save(dust_sequential_valid,presentation_dir+&quot;dust_sequential_0m12243648_valid.pkl&quot;)</span>
<span class="c1"># torch.save(times_sequential_valid,presentation_dir+&quot;times_sequential_0m12243648_valid.pkl&quot;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[139]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">meteorology_sequential_train_2018</span> <span class="o">=</span> <span class="n">meteorology_sequential_train</span><span class="p">[</span><span class="n">times_sequential_train</span><span class="o">.</span><span class="n">year</span><span class="o">==</span><span class="mi">2018</span><span class="p">]</span>
<span class="n">dust_sequential_train_2018</span> <span class="o">=</span> <span class="n">dust_sequential_train</span><span class="p">[</span><span class="n">times_sequential_train</span><span class="o">.</span><span class="n">year</span><span class="o">==</span><span class="mi">2018</span><span class="p">]</span>
<span class="n">times_sequential_train_2018</span> <span class="o">=</span> <span class="n">times_sequential_train</span><span class="p">[</span><span class="n">times_sequential_train</span><span class="o">.</span><span class="n">year</span><span class="o">==</span><span class="mi">2018</span><span class="p">]</span>

<span class="n">meteorology_sequential_train_2018</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dust_sequential_train_2018</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">times_sequential_train_2018</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[139]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1892, 85, 81, 81]), torch.Size([1892, 10]), (1892,))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[140]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># torch.save(meteorology_sequential_train_2018,presentation_dir+&quot;meteorology_sequential_0m12243648_train_2018.pkl&quot;)</span>
<span class="c1"># torch.save(dust_sequential_train_2018,presentation_dir+&quot;dust_sequential_0m12243648_train_2018.pkl&quot;)</span>
<span class="c1"># torch.save(times_sequential_train_2018,presentation_dir+&quot;times_sequential_0m12243648_train_2018.pkl&quot;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[141]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">del</span> <span class="n">meteorology_presentation_train</span>
<span class="k">del</span> <span class="n">dust_presentation_train</span>
<span class="k">del</span> <span class="n">times_presentation_train</span>
<span class="k">del</span> <span class="n">meteorology_presentation_valid</span>
<span class="k">del</span> <span class="n">dust_presentation_valid</span>
<span class="k">del</span> <span class="n">times_presentation_valid</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Presentation set, no augmentation, weighted loss with high deltas weights, sequential 0m12243648, cosine lr scheduler</span>

<span class="n">results_dir_specific</span> <span class="o">=</span> <span class="n">results_dir</span><span class="o">+</span><span class="s2">&quot;sequential_cosine_decay/&quot;</span>

<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">weights_lags</span><span class="o">=</span><span class="p">[</span><span class="mf">10.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="n">weights_delta_lags</span><span class="o">=</span><span class="p">[</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">])</span>


<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;best_model_state.pt&quot;</span>
<span class="n">last_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;last_model_state.pt&quot;</span>

<span class="n">train_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_losses.pkl&quot;</span>
<span class="n">train_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_lags_losses.pkl&quot;</span>
<span class="n">train_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="n">valid_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_losses.pkl&quot;</span>
<span class="n">valid_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_lags_losses.pkl&quot;</span>
<span class="n">valid_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_delta_lags_losses.pkl&quot;</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">meteorology_sequential_train</span><span class="p">,</span>
                                      <span class="n">dust_sequential_train</span><span class="p">,</span>
                                      <span class="n">times_sequential_train</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">meteorology_sequential_valid</span><span class="p">,</span>
                                      <span class="n">dust_sequential_valid</span><span class="p">,</span>
                                      <span class="n">times_sequential_valid</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> 
                              <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> 
                              <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingWarmRestarts</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">350</span>

<span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span><span class="p">,</span>
 <span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses</span><span class="p">,</span>
 <span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                         <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                                         <span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span> <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">save_best_model_dict_to</span><span class="o">=</span><span class="n">best_model_path</span><span class="p">,</span> 
                                                         <span class="n">save_last_model_dict_to</span><span class="o">=</span><span class="n">last_model_path</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                         <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">losses_dir</span><span class="o">=</span><span class="n">results_dir_specific</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">train_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_delta_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span><span class="n">valid_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_delta_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 350   Loss: 2.307e+05   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 2.039e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 350   Loss: 1.946e+05   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.717e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.14929581  28.28333333]
	 [ 55.14886475  51.78333333]
	 [ 55.14912033  49.43333333]
	 [ 55.14925385 124.2       ]
	 [ 55.14958954   4.83333333]]
Train   Epoch: 003 / 350   Loss: 1.91e+05   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 2.043e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 350   Loss: 1.959e+05   Precision: 39.053%   Recall: 81.012%
Valid                   Loss: 2.234e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  79.05390167    6.73333333]
	 [  79.05389404 1024.98333333]
	 [  79.05399323   17.73333333]
	 [  79.0539093    18.63333333]
	 [  79.0539093    15.01666667]]
Train   Epoch: 005 / 350   Loss: 1.835e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.592e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 006 / 350   Loss: 1.827e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.602e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.62726593 28.71666667]
	 [87.62705231 25.8       ]
	 [87.62718964 12.16666667]
	 [87.62704468 64.        ]
	 [87.62741089 39.06666667]]
Train   Epoch: 007 / 350   Loss: 1.862e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.645e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 008 / 350   Loss: 1.739e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.681e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.25059509 32.78333333]
	 [89.25061035 28.33333333]
	 [89.25062561 62.03333333]
	 [89.25060272 36.33333333]
	 [89.25059509 40.41666667]]
Train   Epoch: 009 / 350   Loss: 2.006e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.773e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 010 / 350   Loss: 1.911e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.787e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.83467865 30.8       ]
	 [91.83475494 32.11666667]
	 [91.83473206 28.        ]
	 [91.83473206 19.38333333]
	 [91.8347702  50.88333333]]
Train   Epoch: 011 / 350   Loss: 1.935e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.727e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 012 / 350   Loss: 1.906e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.757e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.35975647 24.33333333]
	 [91.35971832 30.71666667]
	 [91.35975647 43.66666667]
	 [91.35974884 26.45      ]
	 [91.35973358 54.75      ]]
Train   Epoch: 013 / 350   Loss: 1.745e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.687e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 014 / 350   Loss: 1.863e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.679e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.93861389 39.71666667]
	 [89.93865967 43.        ]
	 [89.93890381 26.        ]
	 [89.93861389 53.36666667]
	 [89.93870544 57.45      ]]
Train   Epoch: 015 / 350   Loss: 1.959e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.68e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 016 / 350   Loss: 1.85e+05   Precision: 38.969%   Recall: 100.000%
Valid                   Loss: 2.698e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.2345047  40.03333333]
	 [91.23456573 50.5       ]
	 [91.23440552  9.95      ]
	 [91.23441315 62.61666667]
	 [91.23445129 27.41666667]]
Train   Epoch: 017 / 350   Loss: 1.892e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.837e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 018 / 350   Loss: 1.92e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.759e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.00366974  23.88333333]
	 [ 93.00369263 144.        ]
	 [ 93.00374603  72.01666667]
	 [ 93.00370789  21.        ]
	 [ 93.00366211  31.88333333]]
Train   Epoch: 019 / 350   Loss: 1.886e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.754e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 020 / 350   Loss: 1.958e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.754e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.00289154  55.56666667]
	 [ 91.0029068   28.83333333]
	 [ 91.00286865  40.75      ]
	 [ 91.00288391 142.        ]
	 [ 91.00288391 131.88333333]]
Train   Epoch: 021 / 350   Loss: 2.085e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.774e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 022 / 350   Loss: 1.859e+05   Precision: 38.969%   Recall: 100.000%
Valid                   Loss: 2.764e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.58275604  16.33333333]
	 [ 91.58272552  54.56666667]
	 [ 91.58269501  48.61666667]
	 [ 91.58270264  42.41666667]
	 [ 91.58268738 181.25      ]]
Train   Epoch: 023 / 350   Loss: 1.86e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.712e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 024 / 350   Loss: 1.927e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.72e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.42311096 24.66666667]
	 [90.42303467 35.4       ]
	 [90.42297363 35.78333333]
	 [90.42292023 43.66666667]
	 [90.422966   69.21666667]]
Train   Epoch: 025 / 350   Loss: 1.855e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.727e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 026 / 350   Loss: 1.762e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.696e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.18234253 43.8       ]
	 [90.18238831 58.11666667]
	 [90.18186951 44.61666667]
	 [90.18222046 70.68333333]
	 [90.18231964 24.35      ]]
Train   Epoch: 027 / 350   Loss: 1.832e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.691e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 029 / 350   Loss: 1.896e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.7e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 030 / 350   Loss: 1.963e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.7e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.31858063  34.45      ]
	 [ 90.31879425  28.78333333]
	 [ 90.31926727 459.81666667]
	 [ 90.31856537  36.43333333]
	 [ 90.3192215   79.38333333]]
Train   Epoch: 031 / 350   Loss: 1.985e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.72e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 032 / 350   Loss: 1.944e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.775e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.72822571  69.3       ]
	 [ 91.72806549 102.        ]
	 [ 91.72822571  42.61666667]
	 [ 91.72807312  63.21666667]
	 [ 91.72796631  42.55      ]]
Train   Epoch: 033 / 350   Loss: 1.786e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.723e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 034 / 350   Loss: 1.846e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.693e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.9114151  12.5       ]
	 [89.91143799 32.56666667]
	 [89.91136932 42.1       ]
	 [89.91139984 34.61666667]
	 [89.91143036 77.05      ]]
Train   Epoch: 035 / 350   Loss: 1.99e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.731e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 036 / 350   Loss: 1.885e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.73e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.24137878 36.08333333]
	 [91.2413559  18.18333333]
	 [91.24136353 40.66666667]
	 [91.24136353 29.66666667]
	 [91.24136353 33.53333333]]
Train   Epoch: 037 / 350   Loss: 1.778e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.702e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 038 / 350   Loss: 2.063e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.783e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.52887726 48.5       ]
	 [92.52886963 34.91666667]
	 [92.528862   26.66666667]
	 [92.52887726 29.        ]
	 [92.52884674 51.        ]]
Train   Epoch: 039 / 350   Loss: 2.037e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.738e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 040 / 350   Loss: 1.962e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.751e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.01590729  23.61666667]
	 [ 91.01586914  82.25      ]
	 [ 91.01590729  27.66666667]
	 [ 91.01597595 166.85      ]
	 [ 91.01592255  27.28333333]]
Train   Epoch: 041 / 350   Loss: 1.864e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.741e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 042 / 350   Loss: 1.925e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.699e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.7720108  85.13333333]
	 [90.77304077 30.36666667]
	 [90.77300262 20.95      ]
	 [90.77320099 35.91666667]
	 [90.77270508 52.33333333]]
Train   Epoch: 043 / 350   Loss: 1.868e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.739e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 044 / 350   Loss: 1.993e+05   Precision: 38.973%   Recall: 100.000%
Valid                   Loss: 2.734e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.30465698 36.65      ]
	 [90.30417633 46.3       ]
	 [90.30466461 21.21666667]
	 [90.3041153  30.66666667]
	 [90.30454254 24.1       ]]
Train   Epoch: 045 / 350   Loss: 1.933e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.741e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 046 / 350   Loss: 1.829e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.71e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.45269012 72.68333333]
	 [90.45101929 21.        ]
	 [90.45406342 43.        ]
	 [90.4535675  77.98333333]
	 [90.4517746  38.28333333]]
Train   Epoch: 047 / 350   Loss: 1.958e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.737e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 048 / 350   Loss: 2.026e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.733e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.28317261  34.25      ]
	 [ 91.2824173   51.91666667]
	 [ 91.28266907  43.58333333]
	 [ 91.28645325 118.2       ]
	 [ 91.28369141  17.55      ]]
Train   Epoch: 049 / 350   Loss: 1.868e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.71e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 050 / 350   Loss: 1.778e+05   Precision: 39.062%   Recall: 100.000%
Valid                   Loss: 2.68e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.47477722 224.26666667]
	 [ 90.50637817  88.06666667]
	 [ 90.45450592  27.03333333]
	 [ 90.38561249  20.51666667]
	 [ 90.46031189  76.46666667]]
Train   Epoch: 051 / 350   Loss: 1.858e+05   Precision: 39.071%   Recall: 99.991%
Valid                   Loss: 2.683e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 052 / 350   Loss: 1.954e+05   Precision: 38.969%   Recall: 100.000%
Valid                   Loss: 2.71e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.46912384  22.5       ]
	 [ 90.37747955  64.45      ]
	 [ 90.30438232  33.58333333]
	 [ 90.64820862  25.5       ]
	 [ 90.39331055 133.        ]]
Train   Epoch: 053 / 350   Loss: 1.884e+05   Precision: 39.319%   Recall: 99.194%
Valid                   Loss: 2.504e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 054 / 350   Loss: 1.942e+05   Precision: 45.563%   Recall: 84.112%
Valid                   Loss: 2.295e+04   Precision: 16.860%   Recall: 88.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[66.18204498 43.2       ]
	 [93.15799713 22.95      ]
	 [93.64244843 65.5       ]
	 [93.7564621  24.58333333]
	 [73.84937286 38.71666667]]
Train   Epoch: 055 / 350   Loss: 1.805e+05   Precision: 47.911%   Recall: 83.610%
Valid                   Loss: 2.147e+04   Precision: 18.082%   Recall: 83.673%
Train   Epoch: 056 / 350   Loss: 1.723e+05   Precision: 48.447%   Recall: 84.416%
Valid                   Loss: 2.147e+04   Precision: 18.397%   Recall: 83.163%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.05127716  22.03333333]
	 [ 96.07325745 407.25      ]
	 [ 52.12668228  50.45      ]
	 [ 96.08512878  35.78333333]
	 [ 90.59851074  34.08333333]]
Train   Epoch: 057 / 350   Loss: 1.851e+05   Precision: 49.504%   Recall: 79.989%
Valid                   Loss: 2.18e+04   Precision: 18.031%   Recall: 83.163%
Train   Epoch: 058 / 350   Loss: 1.819e+05   Precision: 49.463%   Recall: 81.174%
Valid                   Loss: 2.195e+04   Precision: 17.447%   Recall: 83.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.33272552 75.16666667]
	 [58.77806473 30.66666667]
	 [58.96159744 22.33333333]
	 [97.30730438 23.75      ]
	 [51.9311409  39.53333333]]
Train   Epoch: 059 / 350   Loss: 1.81e+05   Precision: 49.876%   Recall: 81.695%
Valid                   Loss: 2.221e+04   Precision: 17.414%   Recall: 85.204%
Train   Epoch: 061 / 350   Loss: 1.851e+05   Precision: 46.750%   Recall: 84.463%
Valid                   Loss: 2.144e+04   Precision: 19.574%   Recall: 75.000%
Train   Epoch: 062 / 350   Loss: 1.95e+05   Precision: 41.340%   Recall: 90.596%
Valid                   Loss: 2.714e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.79364014  56.68333333]
	 [ 99.02005768 141.13333333]
	 [ 97.75827026   7.33333333]
	 [ 95.76026154  17.38333333]
	 [ 95.65834045  41.38333333]]
Train   Epoch: 063 / 350   Loss: 1.898e+05   Precision: 42.425%   Recall: 86.065%
Valid                   Loss: 2.872e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 064 / 350   Loss: 1.974e+05   Precision: 39.422%   Recall: 98.749%
Valid                   Loss: 2.472e+04   Precision: 13.761%   Recall: 97.449%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.68454742 58.66666667]
	 [80.09467316 18.78333333]
	 [79.79107666 28.6       ]
	 [91.56467438 11.5       ]
	 [97.8835907   7.25      ]]
Train   Epoch: 065 / 350   Loss: 1.748e+05   Precision: 43.087%   Recall: 89.042%
Valid                   Loss: 2.369e+04   Precision: 18.594%   Recall: 79.592%
Train   Epoch: 066 / 350   Loss: 1.942e+05   Precision: 44.678%   Recall: 82.084%
Valid                   Loss: 3.359e+04   Precision: 11.980%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[114.18751526  24.61666667]
	 [ 89.47698975  32.21666667]
	 [107.06949615  18.43333333]
	 [ 89.45294952  26.25      ]
	 [ 91.22904205  37.33333333]]
Train   Epoch: 067 / 350   Loss: 1.862e+05   Precision: 45.799%   Recall: 82.738%
Valid                   Loss: 2.145e+04   Precision: 18.172%   Recall: 85.204%
Train   Epoch: 068 / 350   Loss: 1.842e+05   Precision: 43.790%   Recall: 88.833%
Valid                   Loss: 2.393e+04   Precision: 15.179%   Recall: 95.408%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.91441345 51.28333333]
	 [95.3338623  65.16666667]
	 [80.71695709 13.45      ]
	 [89.82072449 28.96666667]
	 [80.8469162  23.83333333]]
Train   Epoch: 069 / 350   Loss: 1.738e+05   Precision: 47.033%   Recall: 80.254%
Valid                   Loss: 2.862e+04   Precision: 14.308%   Recall: 91.837%
Train   Epoch: 070 / 350   Loss: 1.764e+05   Precision: 47.351%   Recall: 76.178%
Valid                   Loss: 3.388e+04   Precision: 13.015%   Recall: 91.837%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 52.36315536  45.        ]
	 [ 80.22575378  27.66666667]
	 [121.97179413  22.11666667]
	 [123.69059753  37.05      ]
	 [117.3289032   32.55      ]]
Train   Epoch: 071 / 350   Loss: 1.813e+05   Precision: 47.743%   Recall: 73.590%
Valid                   Loss: 2.764e+04   Precision: 15.753%   Recall: 80.612%
Train   Epoch: 072 / 350   Loss: 1.702e+05   Precision: 47.656%   Recall: 74.576%
Valid                   Loss: 3.269e+04   Precision: 12.772%   Recall: 89.796%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 86.18572998  29.2       ]
	 [ 80.90790558  50.13333333]
	 [ 74.13253784  27.93333333]
	 [117.79131317 142.1       ]
	 [ 78.60562134  33.51666667]]
Train   Epoch: 073 / 350   Loss: 1.949e+05   Precision: 48.403%   Recall: 75.979%
Valid                   Loss: 2.276e+04   Precision: 18.946%   Recall: 67.857%
Train   Epoch: 074 / 350   Loss: 1.781e+05   Precision: 48.856%   Recall: 72.291%
Valid                   Loss: 2.135e+04   Precision: 18.974%   Recall: 62.245%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.93095398  40.66666667]
	 [ 69.13418579  45.66666667]
	 [ 44.14381027  41.88333333]
	 [ 50.37139511  86.        ]
	 [113.51140594 427.11666667]]
Train   Epoch: 075 / 350   Loss: 1.77e+05   Precision: 47.136%   Recall: 72.395%
Valid                   Loss: 5.654e+04   Precision: 13.117%   Recall: 90.816%
Train   Epoch: 076 / 350   Loss: 1.826e+05   Precision: 47.510%   Recall: 70.081%
Valid                   Loss: 2.781e+04   Precision: 18.203%   Recall: 81.633%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 82.86244202  85.73333333]
	 [ 72.78752136  32.45      ]
	 [176.54905701  21.4       ]
	 [ 87.69353485  26.08333333]
	 [ 60.76859283  21.61666667]]
Train   Epoch: 077 / 350   Loss: 1.953e+05   Precision: 42.749%   Recall: 82.766%
Valid                   Loss: 2.781e+04   Precision: 11.980%   Recall: 100.000%
Train   Epoch: 078 / 350   Loss: 1.794e+05   Precision: 42.014%   Recall: 88.549%
Valid                   Loss: 3.634e+04   Precision: 12.084%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.79040527  28.        ]
	 [ 81.1894455   26.78333333]
	 [ 85.08950806  26.35      ]
	 [109.32233429  67.25      ]
	 [119.64337921  22.16666667]]
Train   Epoch: 079 / 350   Loss: 1.725e+05   Precision: 46.846%   Recall: 74.793%
Valid                   Loss: 2.86e+04   Precision: 17.861%   Recall: 80.102%
Train   Epoch: 080 / 350   Loss: 1.672e+05   Precision: 48.267%   Recall: 67.580%
Valid                   Loss: 3.047e+04   Precision: 15.051%   Recall: 83.163%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[193.05714417  19.38333333]
	 [ 57.55093765  60.6       ]
	 [196.41213989  45.21666667]
	 [196.30343628 261.75      ]
	 [ 57.25680923  70.55      ]]
Train   Epoch: 081 / 350   Loss: 1.851e+05   Precision: 47.836%   Recall: 69.893%
Valid                   Loss: 2.603e+04   Precision: 19.236%   Recall: 66.837%
Train   Epoch: 082 / 350   Loss: 1.745e+05   Precision: 48.605%   Recall: 68.063%
Valid                   Loss: 3.644e+04   Precision: 15.846%   Recall: 82.143%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[138.86225891  44.91666667]
	 [ 96.04137421  61.96666667]
	 [ 78.80514526  59.46666667]
	 [ 56.45393753  29.5       ]
	 [ 67.97991943  34.        ]]
Train   Epoch: 083 / 350   Loss: 1.72e+05   Precision: 49.085%   Recall: 64.850%
Valid                   Loss: 2.8e+04   Precision: 19.031%   Recall: 82.143%
Train   Epoch: 084 / 350   Loss: 1.729e+05   Precision: 47.943%   Recall: 67.817%
Valid                   Loss: 3.608e+04   Precision: 16.786%   Recall: 83.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 56.70675278  63.45      ]
	 [ 57.75524139  54.22      ]
	 [ 95.96622467  14.45      ]
	 [ 60.01876831  31.46666667]
	 [219.82449341  21.56666667]]
Train   Epoch: 085 / 350   Loss: 1.788e+05   Precision: 49.347%   Recall: 69.457%
Valid                   Loss: 3.892e+04   Precision: 16.070%   Recall: 84.694%
Train   Epoch: 086 / 350   Loss: 1.724e+05   Precision: 49.269%   Recall: 66.461%
Valid                   Loss: 2.877e+04   Precision: 17.540%   Recall: 83.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[101.74033356  11.88333333]
	 [217.84892273 280.8       ]
	 [ 80.48863983  21.58333333]
	 [ 74.24399567  49.16666667]
	 [ 62.10979462  22.53333333]]
Train   Epoch: 087 / 350   Loss: 1.764e+05   Precision: 49.117%   Recall: 68.434%
Valid                   Loss: 3.762e+04   Precision: 17.285%   Recall: 83.163%
Train   Epoch: 088 / 350   Loss: 1.741e+05   Precision: 49.932%   Recall: 65.665%
Valid                   Loss: 2.579e+04   Precision: 18.359%   Recall: 71.939%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.23508453 25.66666667]
	 [50.85835648 45.66666667]
	 [66.11854553 23.46666667]
	 [62.39760971 36.        ]
	 [56.23809814 31.        ]]
Train   Epoch: 089 / 350   Loss: 1.944e+05   Precision: 49.890%   Recall: 66.897%
Valid                   Loss: 2.45e+04   Precision: 18.670%   Recall: 67.347%
Train   Epoch: 091 / 350   Loss: 1.997e+05   Precision: 49.389%   Recall: 66.622%
Valid                   Loss: 2.259e+04   Precision: 19.795%   Recall: 59.184%
Train   Epoch: 092 / 350   Loss: 1.733e+05   Precision: 50.057%   Recall: 66.575%
Valid                   Loss: 2.191e+04   Precision: 18.720%   Recall: 59.694%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.62481689 47.38333333]
	 [41.10211182 34.83333333]
	 [82.34249878 14.        ]
	 [65.66048431 77.28333333]
	 [86.85186768 36.71666667]]
Train   Epoch: 093 / 350   Loss: 1.642e+05   Precision: 51.298%   Recall: 66.148%
Valid                   Loss: 2.66e+04   Precision: 18.873%   Recall: 78.571%
Train   Epoch: 094 / 350   Loss: 1.826e+05   Precision: 50.177%   Recall: 66.006%
Valid                   Loss: 2.225e+04   Precision: 18.968%   Recall: 63.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 54.52330017  30.83333333]
	 [ 54.5268898   29.91666667]
	 [ 68.25032806  23.3       ]
	 [ 77.51875305 134.08333333]
	 [ 66.16213989  38.66666667]]
Train   Epoch: 095 / 350   Loss: 1.78e+05   Precision: 51.198%   Recall: 64.840%
Valid                   Loss: 2.806e+04   Precision: 18.966%   Recall: 78.571%
Train   Epoch: 096 / 350   Loss: 1.784e+05   Precision: 51.480%   Recall: 65.437%
Valid                   Loss: 3.03e+04   Precision: 17.826%   Recall: 83.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.03790283  37.66666667]
	 [232.93612671  61.25      ]
	 [ 98.8522644   66.45      ]
	 [ 70.90898132  30.41666667]
	 [ 75.44654846  39.33333333]]
Train   Epoch: 097 / 350   Loss: 1.682e+05   Precision: 50.895%   Recall: 67.637%
Valid                   Loss: 2.989e+04   Precision: 18.439%   Recall: 83.163%
Train   Epoch: 098 / 350   Loss: 1.685e+05   Precision: 51.026%   Recall: 64.812%
Valid                   Loss: 2.359e+04   Precision: 19.027%   Recall: 65.816%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.94729614 119.16666667]
	 [ 80.07783508  36.35      ]
	 [134.69396973  24.66666667]
	 [ 52.88336945  24.33333333]
	 [ 46.45084     41.08333333]]
Train   Epoch: 099 / 350   Loss: 1.77e+05   Precision: 51.247%   Recall: 69.523%
Valid                   Loss: 2.239e+04   Precision: 19.623%   Recall: 63.776%
Train   Epoch: 100 / 350   Loss: 1.705e+05   Precision: 52.430%   Recall: 63.826%
Valid                   Loss: 2.696e+04   Precision: 20.339%   Recall: 79.592%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.10453033  73.05      ]
	 [ 58.25795746  61.05      ]
	 [ 96.03294373 109.03333333]
	 [ 71.28862     18.61666667]
	 [ 48.36695099  42.13333333]]
Train   Epoch: 101 / 350   Loss: 1.679e+05   Precision: 52.958%   Recall: 64.499%
Valid                   Loss: 2.259e+04   Precision: 19.088%   Recall: 68.367%
Train   Epoch: 102 / 350   Loss: 1.79e+05   Precision: 51.756%   Recall: 66.234%
Valid                   Loss: 2.205e+04   Precision: 19.657%   Recall: 64.286%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[85.75836182 15.        ]
	 [51.31725693 29.15      ]
	 [69.83948517 62.78333333]
	 [48.27877045 68.75      ]
	 [74.27306366 39.16666667]]
Train   Epoch: 103 / 350   Loss: 1.715e+05   Precision: 52.685%   Recall: 63.703%
Valid                   Loss: 2.23e+04   Precision: 21.612%   Recall: 60.204%
Train   Epoch: 104 / 350   Loss: 1.554e+05   Precision: 52.071%   Recall: 67.256%
Valid                   Loss: 2.4e+04   Precision: 19.399%   Recall: 72.449%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.28659821  40.33333333]
	 [ 63.82561111  22.78333333]
	 [ 75.77722168  13.6       ]
	 [ 53.3589859   33.2       ]
	 [ 47.17650223  37.71666667]]
Train   Epoch: 105 / 350   Loss: 1.69e+05   Precision: 52.083%   Recall: 68.604%
Valid                   Loss:   3e+04   Precision: 18.388%   Recall: 82.653%
Train   Epoch: 106 / 350   Loss: 1.789e+05   Precision: 52.760%   Recall: 67.134%
Valid                   Loss: 2.494e+04   Precision: 19.226%   Recall: 73.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.41835785 35.21666667]
	 [65.74614716 24.63333333]
	 [85.87958527 36.11666667]
	 [78.15882111 35.25      ]
	 [43.65205383 27.66666667]]
Train   Epoch: 107 / 350   Loss: 1.705e+05   Precision: 53.171%   Recall: 66.281%
Valid                   Loss: 2.348e+04   Precision: 19.501%   Recall: 67.857%
Train   Epoch: 108 / 350   Loss: 1.58e+05   Precision: 53.127%   Recall: 66.281%
Valid                   Loss: 2.178e+04   Precision: 20.032%   Recall: 63.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.77513123 22.88333333]
	 [47.32394791 21.41666667]
	 [55.38332748 20.7       ]
	 [69.7401886  28.16666667]
	 [47.1591568  25.71666667]]
Train   Epoch: 109 / 350   Loss: 1.645e+05   Precision: 52.895%   Recall: 67.210%
Valid                   Loss: 2.247e+04   Precision: 21.717%   Recall: 65.816%
Train   Epoch: 110 / 350   Loss: 1.598e+05   Precision: 53.408%   Recall: 67.523%
Valid                   Loss: 2.207e+04   Precision: 21.547%   Recall: 59.694%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.99142838 11.35      ]
	 [93.68624878 42.5       ]
	 [46.7522049  68.33333333]
	 [52.8415184  18.        ]
	 [44.79677963 45.88333333]]
Train   Epoch: 111 / 350   Loss: 1.634e+05   Precision: 52.839%   Recall: 66.603%
Valid                   Loss: 2.386e+04   Precision: 20.863%   Recall: 73.980%
Train   Epoch: 112 / 350   Loss: 1.664e+05   Precision: 54.290%   Recall: 66.404%
Valid                   Loss: 2.183e+04   Precision: 21.661%   Recall: 61.224%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.70981598  57.86666667]
	 [ 58.87358475  28.66666667]
	 [ 82.1619339  126.63333333]
	 [ 42.38430023  44.95      ]
	 [ 63.50859451  67.36666667]]
Train   Epoch: 113 / 350   Loss: 1.625e+05   Precision: 54.059%   Recall: 66.660%
Valid                   Loss: 2.24e+04   Precision: 22.162%   Recall: 62.755%
Train   Epoch: 114 / 350   Loss: 1.548e+05   Precision: 54.700%   Recall: 66.471%
Valid                   Loss: 2.13e+04   Precision: 23.576%   Recall: 61.224%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 41.28156281  45.        ]
	 [ 43.83702469  52.16666667]
	 [ 41.8543396   34.05      ]
	 [ 49.86869812  73.5       ]
	 [153.62973022  73.2       ]]
Train   Epoch: 115 / 350   Loss: 1.607e+05   Precision: 54.356%   Recall: 67.077%
Valid                   Loss: 2.234e+04   Precision: 23.256%   Recall: 66.327%
Train   Epoch: 116 / 350   Loss: 1.701e+05   Precision: 54.405%   Recall: 67.788%
Valid                   Loss: 2.136e+04   Precision: 23.960%   Recall: 61.735%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[120.59906006  28.18333333]
	 [ 58.13515472 126.38333333]
	 [ 42.04230118  60.78333333]
	 [ 62.83550262  55.        ]
	 [ 44.23812485  22.        ]]
Train   Epoch: 117 / 350   Loss: 1.603e+05   Precision: 54.502%   Recall: 69.201%
Valid                   Loss: 2.18e+04   Precision: 23.684%   Recall: 64.286%
Train   Epoch: 118 / 350   Loss: 1.636e+05   Precision: 54.083%   Recall: 67.295%
Valid                   Loss: 2.189e+04   Precision: 24.000%   Recall: 64.286%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.88698959  26.45      ]
	 [ 75.16668701  41.68333333]
	 [ 42.0266037   26.35      ]
	 [305.49127197 921.11666667]
	 [ 74.04399872  26.11666667]]
Train   Epoch: 119 / 350   Loss: 1.55e+05   Precision: 54.552%   Recall: 69.296%
Valid                   Loss: 2.176e+04   Precision: 23.563%   Recall: 62.755%
Train   Epoch: 120 / 350   Loss: 1.647e+05   Precision: 54.210%   Recall: 68.480%
Valid                   Loss: 2.186e+04   Precision: 23.674%   Recall: 63.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.11204147 33.5       ]
	 [75.02506256 46.25      ]
	 [47.52644348 54.75      ]
	 [41.61109924 52.        ]
	 [80.95271301 21.9       ]]
Train   Epoch: 121 / 350   Loss: 1.608e+05   Precision: 53.681%   Recall: 68.424%
Valid                   Loss: 2.184e+04   Precision: 23.574%   Recall: 63.265%
Train   Epoch: 122 / 350   Loss: 1.749e+05   Precision: 53.008%   Recall: 64.821%
Valid                   Loss: 2.688e+04   Precision: 20.914%   Recall: 77.041%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[84.16584015 29.86666667]
	 [53.01050186 18.05      ]
	 [47.84270859 39.16666667]
	 [81.42494202 15.5       ]
	 [81.31467438 51.05      ]]
Train   Epoch: 123 / 350   Loss: 1.631e+05   Precision: 53.298%   Recall: 62.432%
Valid                   Loss: 2.489e+04   Precision: 20.721%   Recall: 70.408%
Train   Epoch: 124 / 350   Loss: 1.701e+05   Precision: 51.758%   Recall: 63.504%
Valid                   Loss: 1.955e+04   Precision: 19.944%   Recall: 36.224%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.08436584 22.33333333]
	 [85.04315186 36.43333333]
	 [43.01131821 55.28333333]
	 [64.46316528 61.61666667]
	 [79.59853363 31.83333333]]
Train   Epoch: 125 / 350   Loss: 1.663e+05   Precision: 52.088%   Recall: 65.039%
Valid                   Loss: 2.966e+04   Precision: 18.294%   Recall: 83.163%
Train   Epoch: 126 / 350   Loss: 1.583e+05   Precision: 51.506%   Recall: 65.333%
Valid                   Loss: 2.033e+04   Precision: 21.096%   Recall: 64.796%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.09008026  52.        ]
	 [108.83274078  37.45      ]
	 [ 47.19102478  40.86666667]
	 [ 50.82268524  39.66666667]
	 [ 59.78757858 166.25      ]]
Train   Epoch: 127 / 350   Loss: 1.695e+05   Precision: 51.063%   Recall: 66.698%
Valid                   Loss: 2.133e+04   Precision: 23.063%   Recall: 65.306%
Train   Epoch: 128 / 350   Loss: 1.658e+05   Precision: 51.782%   Recall: 62.395%
Valid                   Loss: 2.175e+04   Precision: 24.099%   Recall: 64.796%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.6623764   32.78333333]
	 [ 81.18993378 144.5       ]
	 [ 46.3932991   35.55      ]
	 [ 57.85318375  16.7       ]
	 [ 53.04652405  39.08333333]]
Train   Epoch: 129 / 350   Loss: 1.628e+05   Precision: 52.882%   Recall: 62.357%
Valid                   Loss: 2.094e+04   Precision: 18.990%   Recall: 80.612%
Train   Epoch: 130 / 350   Loss: 1.594e+05   Precision: 52.084%   Recall: 65.978%
Valid                   Loss: 1.817e+04   Precision: 29.032%   Recall: 22.959%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.55299377  36.53333333]
	 [ 37.00175858  31.46666667]
	 [ 59.08840561  54.5       ]
	 [ 54.8950882  186.5       ]
	 [ 65.21353149  21.61666667]]
Train   Epoch: 131 / 350   Loss: 1.641e+05   Precision: 52.837%   Recall: 62.233%
Valid                   Loss: 2.407e+04   Precision: 19.152%   Recall: 80.612%
Train   Epoch: 132 / 350   Loss: 1.797e+05   Precision: 52.702%   Recall: 60.916%
Valid                   Loss: 2.461e+04   Precision: 20.323%   Recall: 77.041%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 94.24929047  34.78333333]
	 [ 67.62172699  60.45      ]
	 [ 65.38838196  35.33333333]
	 [199.54013062 108.08333333]
	 [257.43658447  55.95      ]]
Train   Epoch: 133 / 350   Loss: 1.669e+05   Precision: 52.324%   Recall: 64.670%
Valid                   Loss: 2.005e+04   Precision: 22.199%   Recall: 53.571%
Train   Epoch: 134 / 350   Loss: 1.618e+05   Precision: 52.063%   Recall: 65.551%
Valid                   Loss: 1.645e+04   Precision: 37.069%   Recall: 21.939%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 39.80947113 102.48333333]
	 [160.5072937  196.98333333]
	 [ 52.89155579  33.16666667]
	 [ 50.98384094  23.78333333]
	 [149.9362793  138.11666667]]
Train   Epoch: 136 / 350   Loss: 1.711e+05   Precision: 52.974%   Recall: 63.826%
Valid                   Loss: 1.894e+04   Precision: 23.631%   Recall: 41.837%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.22829437 49.88333333]
	 [42.74335861 26.28333333]
	 [58.23648453 21.1       ]
	 [45.06171799 33.25      ]
	 [43.92640686 33.15      ]]
Train   Epoch: 137 / 350   Loss: 1.614e+05   Precision: 53.455%   Recall: 63.058%
Valid                   Loss: 2.131e+04   Precision: 21.440%   Recall: 68.367%
Train   Epoch: 138 / 350   Loss: 1.659e+05   Precision: 54.723%   Recall: 64.148%
Valid                   Loss: 2.427e+04   Precision: 24.537%   Recall: 54.082%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.7955513  50.26666667]
	 [58.00909424 39.21666667]
	 [50.9487114  29.55      ]
	 [57.50741196 63.25      ]
	 [62.87411499 32.        ]]
Train   Epoch: 139 / 350   Loss: 1.72e+05   Precision: 53.532%   Recall: 65.077%
Valid                   Loss: 2.041e+04   Precision: 24.330%   Recall: 55.612%
Train   Epoch: 140 / 350   Loss: 1.573e+05   Precision: 53.265%   Recall: 63.483%
Valid                   Loss: 1.911e+04   Precision: 28.438%   Recall: 46.429%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.37251282 22.33333333]
	 [46.21448898 32.25      ]
	 [53.72008514 27.        ]
	 [39.78330994 46.        ]
	 [54.81748581 76.38333333]]
Train   Epoch: 141 / 350   Loss: 1.732e+05   Precision: 47.292%   Recall: 69.040%
Valid                   Loss: 1.86e+04   Precision: 30.435%   Recall: 50.000%
Train   Epoch: 142 / 350   Loss: 1.575e+05   Precision: 54.269%   Recall: 63.570%
Valid                   Loss: 2.068e+04   Precision: 22.085%   Recall: 63.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.87117767 131.3       ]
	 [259.53713989 186.33333333]
	 [ 50.18655396  28.48333333]
	 [ 59.69219589  71.71666667]
	 [ 46.71927261  33.55      ]]
Train   Epoch: 143 / 350   Loss: 1.696e+05   Precision: 52.809%   Recall: 64.338%
Valid                   Loss: 2.437e+04   Precision: 19.300%   Recall: 81.633%
Train   Epoch: 144 / 350   Loss: 1.759e+05   Precision: 52.591%   Recall: 66.291%
Valid                   Loss: 1.884e+04   Precision: 32.298%   Recall: 53.061%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.6508255  13.31666667]
	 [59.08542252 39.95      ]
	 [68.73791504 41.68333333]
	 [62.48138809 29.75      ]
	 [66.86112976 20.31666667]]
Train   Epoch: 145 / 350   Loss: 1.542e+05   Precision: 53.894%   Recall: 63.627%
Valid                   Loss: 1.837e+04   Precision: 36.530%   Recall: 40.816%
Train   Epoch: 146 / 350   Loss: 1.723e+05   Precision: 52.971%   Recall: 66.499%
Valid                   Loss: 1.949e+04   Precision: 25.743%   Recall: 53.061%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.70106888 44.2       ]
	 [33.94933319 74.66666667]
	 [45.4648819  75.05      ]
	 [63.97785187 35.        ]
	 [41.51828766 30.88333333]]
Train   Epoch: 147 / 350   Loss: 1.616e+05   Precision: 53.919%   Recall: 61.238%
Valid                   Loss: 2.561e+04   Precision: 22.511%   Recall: 75.000%
Train   Epoch: 148 / 350   Loss: 1.648e+05   Precision: 53.019%   Recall: 64.091%
Valid                   Loss: 2.062e+04   Precision: 24.055%   Recall: 71.429%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.74020004 38.        ]
	 [52.20864868 46.78333333]
	 [55.15455246 30.        ]
	 [64.38449097 25.26666667]
	 [64.87030792 29.78333333]]
Train   Epoch: 149 / 350   Loss: 1.594e+05   Precision: 54.332%   Recall: 67.656%
Valid                   Loss: 1.82e+04   Precision: 27.487%   Recall: 53.571%
Train   Epoch: 150 / 350   Loss: 1.692e+05   Precision: 55.081%   Recall: 64.480%
Valid                   Loss: 2.291e+04   Precision: 24.547%   Recall: 62.245%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.95749664  59.06666667]
	 [ 47.68756866  34.63333333]
	 [109.77366638  60.8       ]
	 [ 48.48989487  28.83333333]
	 [ 49.90957642  37.91666667]]
Train   Epoch: 151 / 350   Loss: 1.596e+05   Precision: 56.475%   Recall: 66.224%
Valid                   Loss: 1.77e+04   Precision: 31.399%   Recall: 46.939%
Train   Epoch: 152 / 350   Loss: 1.507e+05   Precision: 56.464%   Recall: 64.158%
Valid                   Loss: 1.872e+04   Precision: 32.620%   Recall: 31.122%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.08916473 25.11666667]
	 [56.70345306 32.05      ]
	 [48.40325165 25.08333333]
	 [50.9549942  25.4       ]
	 [54.66016769 50.03333333]]
Train   Epoch: 153 / 350   Loss: 1.562e+05   Precision: 55.614%   Recall: 65.504%
Valid                   Loss: 4.733e+04   Precision: 15.865%   Recall: 84.184%
Train   Epoch: 154 / 350   Loss: 1.702e+05   Precision: 55.127%   Recall: 66.765%
Valid                   Loss: 2.271e+04   Precision: 25.490%   Recall: 72.959%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.28131104  54.        ]
	 [ 75.87783051 101.75      ]
	 [ 55.50289917  43.55      ]
	 [324.04345703 107.71666667]
	 [ 57.27082062  23.85      ]]
Train   Epoch: 155 / 350   Loss: 1.516e+05   Precision: 56.267%   Recall: 66.850%
Valid                   Loss: 1.691e+04   Precision: 30.035%   Recall: 43.367%
Train   Epoch: 156 / 350   Loss: 1.506e+05   Precision: 56.240%   Recall: 66.594%
Valid                   Loss: 1.776e+04   Precision: 27.273%   Recall: 64.286%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.56835175 54.4       ]
	 [76.28821564 38.13333333]
	 [65.40044403 93.        ]
	 [51.18479156 52.36666667]
	 [58.52391434 69.65      ]]
Train   Epoch: 157 / 350   Loss: 1.634e+05   Precision: 56.295%   Recall: 70.064%
Valid                   Loss: 1.87e+04   Precision: 28.889%   Recall: 59.694%
Train   Epoch: 158 / 350   Loss: 1.668e+05   Precision: 56.421%   Recall: 68.679%
Valid                   Loss: 2.033e+04   Precision: 27.924%   Recall: 59.694%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.73184586  29.38333333]
	 [ 67.87192535  22.33333333]
	 [ 42.44269943  21.2       ]
	 [120.95680237 119.16666667]
	 [ 44.94204712  68.88333333]]
Train   Epoch: 159 / 350   Loss: 1.571e+05   Precision: 55.930%   Recall: 66.964%
Valid                   Loss: 2.322e+04   Precision: 21.972%   Recall: 79.592%
Train   Epoch: 160 / 350   Loss: 1.617e+05   Precision: 57.372%   Recall: 66.878%
Valid                   Loss: 2.147e+04   Precision: 24.704%   Recall: 63.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.62644958 16.83333333]
	 [78.16684723 19.23333333]
	 [88.75299072 40.2       ]
	 [44.52013779 72.33333333]
	 [44.88225555 42.05      ]]
Train   Epoch: 161 / 350   Loss: 1.684e+05   Precision: 56.425%   Recall: 69.305%
Valid                   Loss: 2.103e+04   Precision: 28.067%   Recall: 68.878%
Train   Epoch: 162 / 350   Loss: 1.491e+05   Precision: 57.663%   Recall: 66.983%
Valid                   Loss: 2.056e+04   Precision: 25.379%   Recall: 68.367%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.6504364  22.75      ]
	 [43.46072006 29.23333333]
	 [76.94762421 63.21666667]
	 [45.31238556 85.33333333]
	 [44.43971634 34.66666667]]
Train   Epoch: 163 / 350   Loss: 1.642e+05   Precision: 57.457%   Recall: 69.210%
Valid                   Loss: 1.701e+04   Precision: 29.373%   Recall: 45.408%
Train   Epoch: 164 / 350   Loss: 1.621e+05   Precision: 57.570%   Recall: 69.533%
Valid                   Loss: 1.678e+04   Precision: 30.690%   Recall: 45.408%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.68389893 45.58333333]
	 [39.39570236 13.93333333]
	 [40.5173645  34.66666667]
	 [44.17522049 27.3       ]
	 [61.69682312 12.61666667]]
Train   Epoch: 165 / 350   Loss: 1.629e+05   Precision: 58.963%   Recall: 69.286%
Valid                   Loss: 1.821e+04   Precision: 28.084%   Recall: 54.592%
Train   Epoch: 166 / 350   Loss: 1.643e+05   Precision: 57.415%   Recall: 69.656%
Valid                   Loss: 1.623e+04   Precision: 36.757%   Recall: 34.694%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.96899796 583.05      ]
	 [ 49.20000839  30.28333333]
	 [ 28.05564308  28.53333333]
	 [ 54.50247955  51.11666667]
	 [ 41.74905396  29.71666667]]
Train   Epoch: 167 / 350   Loss: 1.577e+05   Precision: 58.968%   Recall: 68.879%
Valid                   Loss: 2.042e+04   Precision: 31.046%   Recall: 48.469%
Train   Epoch: 168 / 350   Loss: 1.521e+05   Precision: 58.233%   Recall: 67.987%
Valid                   Loss: 2.063e+04   Precision: 29.469%   Recall: 62.245%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.97731018  29.25      ]
	 [ 50.11478043  48.46666667]
	 [ 82.23989105  48.5       ]
	 [ 79.72322845 102.03333333]
	 [ 83.93965912  39.41666667]]
Train   Epoch: 169 / 350   Loss: 1.564e+05   Precision: 57.397%   Recall: 68.926%
Valid                   Loss: 1.871e+04   Precision: 26.434%   Recall: 65.816%
Train   Epoch: 170 / 350   Loss: 1.399e+05   Precision: 58.745%   Recall: 69.125%
Valid                   Loss: 1.804e+04   Precision: 29.639%   Recall: 58.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 54.01809692  44.        ]
	 [ 44.44640732  54.75      ]
	 [ 47.21070099  45.36666667]
	 [104.17351532  98.98333333]
	 [ 50.67816162  10.45      ]]
Train   Epoch: 171 / 350   Loss: 1.532e+05   Precision: 58.099%   Recall: 67.798%
Valid                   Loss: 1.948e+04   Precision: 25.835%   Recall: 75.000%
Train   Epoch: 172 / 350   Loss: 1.58e+05   Precision: 58.088%   Recall: 70.433%
Valid                   Loss: 1.717e+04   Precision: 31.734%   Recall: 43.878%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.54899216 64.        ]
	 [59.44536591 28.66666667]
	 [56.72194672 38.95      ]
	 [73.63368225 81.33333333]
	 [43.44481277 67.6       ]]
Train   Epoch: 174 / 350   Loss: 1.545e+05   Precision: 59.062%   Recall: 69.011%
Valid                   Loss: 1.798e+04   Precision: 31.494%   Recall: 49.490%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 78.82829285  65.5       ]
	 [ 29.6003952   16.3       ]
	 [ 96.39751434  44.45      ]
	 [ 45.48649597  38.4       ]
	 [ 91.9446106  611.45      ]]
Train   Epoch: 175 / 350   Loss: 1.615e+05   Precision: 58.051%   Recall: 70.914%
Valid                   Loss: 1.769e+04   Precision: 31.273%   Recall: 43.878%
Train   Epoch: 176 / 350   Loss: 1.51e+05   Precision: 59.732%   Recall: 68.803%
Valid                   Loss: 1.954e+04   Precision: 28.673%   Recall: 61.735%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.92662048  40.46666667]
	 [602.6229248  287.11666667]
	 [131.32528687  28.        ]
	 [107.89133453  31.5       ]
	 [135.26040649  25.78333333]]
Train   Epoch: 177 / 350   Loss: 1.519e+05   Precision: 58.206%   Recall: 69.798%
Valid                   Loss: 1.918e+04   Precision: 26.616%   Recall: 71.429%
Train   Epoch: 178 / 350   Loss: 1.479e+05   Precision: 58.154%   Recall: 71.391%
Valid                   Loss: 1.861e+04   Precision: 31.385%   Recall: 52.041%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[106.03723907  58.33333333]
	 [ 38.74396515  29.16666667]
	 [ 88.25430298  18.33333333]
	 [ 70.50151062  47.46666667]
	 [ 34.6241684   19.98333333]]
Train   Epoch: 179 / 350   Loss: 1.599e+05   Precision: 60.337%   Recall: 70.329%
Valid                   Loss: 1.807e+04   Precision: 29.412%   Recall: 61.224%
Train   Epoch: 180 / 350   Loss: 1.505e+05   Precision: 59.322%   Recall: 70.793%
Valid                   Loss: 1.912e+04   Precision: 29.139%   Recall: 67.347%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[127.02610779  52.9       ]
	 [ 54.25339508  50.8       ]
	 [ 29.14245224  40.26666667]
	 [ 43.74760437  35.66666667]
	 [ 65.54152679  44.41666667]]
Train   Epoch: 181 / 350   Loss: 1.506e+05   Precision: 58.353%   Recall: 68.803%
Valid                   Loss: 1.653e+04   Precision: 31.628%   Recall: 34.694%
Train   Epoch: 182 / 350   Loss: 1.564e+05   Precision: 59.392%   Recall: 69.476%
Valid                   Loss: 1.681e+04   Precision: 31.268%   Recall: 56.633%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.13920593 18.56666667]
	 [53.13150787 20.81666667]
	 [80.08823395 41.26666667]
	 [27.47410583 13.66666667]
	 [42.61650848 28.03333333]]
Train   Epoch: 183 / 350   Loss: 1.525e+05   Precision: 60.582%   Recall: 71.040%
Valid                   Loss: 1.666e+04   Precision: 31.950%   Recall: 39.286%
Train   Epoch: 184 / 350   Loss: 1.45e+05   Precision: 59.569%   Recall: 70.993%
Valid                   Loss: 1.905e+04   Precision: 29.440%   Recall: 61.735%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.7612915  27.68333333]
	 [37.76465607 59.21666667]
	 [78.98651123 13.1       ]
	 [52.68725967 31.        ]
	 [63.7434082  16.33333333]]
Train   Epoch: 185 / 350   Loss: 1.544e+05   Precision: 60.222%   Recall: 71.542%
Valid                   Loss: 1.679e+04   Precision: 32.993%   Recall: 49.490%
Train   Epoch: 186 / 350   Loss: 1.528e+05   Precision: 60.135%   Recall: 70.054%
Valid                   Loss: 1.95e+04   Precision: 29.909%   Recall: 66.837%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.74227905 43.16666667]
	 [30.2883358  11.25      ]
	 [45.46387863 44.        ]
	 [49.17233276 39.45      ]
	 [53.72679138 49.33333333]]
Train   Epoch: 187 / 350   Loss: 1.494e+05   Precision: 60.395%   Recall: 69.533%
Valid                   Loss: 1.68e+04   Precision: 33.878%   Recall: 42.347%
Train   Epoch: 188 / 350   Loss: 1.494e+05   Precision: 61.817%   Recall: 66.821%
Valid                   Loss: 1.665e+04   Precision: 33.818%   Recall: 47.449%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[129.83151245  79.16666667]
	 [ 52.9983902   44.66666667]
	 [ 92.52648926  46.11666667]
	 [126.8114624   18.86666667]
	 [ 53.38827896  65.11666667]]
Train   Epoch: 189 / 350   Loss: 1.482e+05   Precision: 61.213%   Recall: 71.182%
Valid                   Loss: 1.689e+04   Precision: 32.822%   Recall: 54.592%
Train   Epoch: 190 / 350   Loss: 1.538e+05   Precision: 60.788%   Recall: 70.831%
Valid                   Loss: 1.746e+04   Precision: 31.231%   Recall: 53.061%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.24768066  62.33333333]
	 [355.34750366 280.31666667]
	 [ 58.87567139  31.73333333]
	 [ 46.12236023  68.66666667]
	 [ 30.7620697   18.5       ]]
Train   Epoch: 191 / 350   Loss: 1.553e+05   Precision: 61.112%   Recall: 68.869%
Valid                   Loss: 1.905e+04   Precision: 32.036%   Recall: 54.592%
Train   Epoch: 192 / 350   Loss: 1.469e+05   Precision: 61.338%   Recall: 68.850%
Valid                   Loss: 1.737e+04   Precision: 32.847%   Recall: 45.918%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 58.78380585  16.21666667]
	 [ 66.45565033  51.13333333]
	 [ 32.6588974   19.88333333]
	 [ 14.33447456  19.43333333]
	 [ 78.93375397 101.63333333]]
Train   Epoch: 193 / 350   Loss: 1.49e+05   Precision: 62.398%   Recall: 68.964%
Valid                   Loss: 1.775e+04   Precision: 31.930%   Recall: 46.429%
Train   Epoch: 194 / 350   Loss: 1.491e+05   Precision: 61.840%   Recall: 69.438%
Valid                   Loss: 1.745e+04   Precision: 34.717%   Recall: 46.939%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[84.94001007 29.08333333]
	 [52.08329773 45.        ]
	 [35.64533234 20.5       ]
	 [23.8574276   5.83333333]
	 [48.37202835 44.46666667]]
Train   Epoch: 195 / 350   Loss: 1.511e+05   Precision: 61.599%   Recall: 70.680%
Valid                   Loss: 1.757e+04   Precision: 31.902%   Recall: 53.061%
Train   Epoch: 196 / 350   Loss: 1.464e+05   Precision: 62.198%   Recall: 70.642%
Valid                   Loss: 1.871e+04   Precision: 32.754%   Recall: 57.653%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 79.67111969  41.46666667]
	 [ 36.25794601  39.        ]
	 [ 37.07501984  36.2       ]
	 [113.60287476  32.46666667]
	 [ 66.58385468  84.8       ]]
Train   Epoch: 197 / 350   Loss: 1.544e+05   Precision: 61.581%   Recall: 70.291%
Valid                   Loss: 1.815e+04   Precision: 30.542%   Recall: 63.265%
Train   Epoch: 198 / 350   Loss: 1.504e+05   Precision: 61.633%   Recall: 71.116%
Valid                   Loss: 1.746e+04   Precision: 32.095%   Recall: 61.735%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.7701683   51.11666667]
	 [119.00219727  25.83333333]
	 [ 64.00110626  22.83333333]
	 [ 38.10913086  34.61666667]
	 [ 50.29234314  41.08333333]]
Train   Epoch: 199 / 350   Loss: 1.458e+05   Precision: 61.336%   Recall: 71.476%
Valid                   Loss: 1.77e+04   Precision: 32.000%   Recall: 57.143%
Train   Epoch: 200 / 350   Loss: 1.535e+05   Precision: 61.947%   Recall: 69.552%
Valid                   Loss: 1.894e+04   Precision: 31.159%   Recall: 65.816%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[124.61923218 117.3       ]
	 [130.27111816  65.3       ]
	 [ 48.75032806  44.86666667]
	 [ 73.60227203  32.        ]
	 [ 37.27871704  32.35      ]]
Train   Epoch: 201 / 350   Loss: 1.46e+05   Precision: 62.384%   Recall: 70.244%
Valid                   Loss: 1.711e+04   Precision: 32.670%   Recall: 58.673%
Train   Epoch: 202 / 350   Loss: 1.48e+05   Precision: 62.144%   Recall: 71.068%
Valid                   Loss: 1.793e+04   Precision: 31.609%   Recall: 56.122%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.97134781 21.61666667]
	 [28.37677002 20.26666667]
	 [75.20513153 51.25      ]
	 [56.59607697 36.66666667]
	 [58.93108368 63.91666667]]
Train   Epoch: 203 / 350   Loss: 1.589e+05   Precision: 62.487%   Recall: 71.751%
Valid                   Loss: 1.911e+04   Precision: 31.127%   Recall: 64.796%
Train   Epoch: 204 / 350   Loss: 1.538e+05   Precision: 62.918%   Recall: 70.439%
Valid                   Loss: 1.889e+04   Precision: 31.402%   Recall: 52.551%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.50159836  38.41666667]
	 [ 46.33258057  22.91666667]
	 [ 75.27856445  48.83333333]
	 [123.10196686  93.03333333]
	 [ 67.89608002  51.41666667]]
Train   Epoch: 205 / 350   Loss: 1.541e+05   Precision: 62.500%   Recall: 70.125%
Valid                   Loss: 1.804e+04   Precision: 32.773%   Recall: 59.694%
Train   Epoch: 207 / 350   Loss: 1.439e+05   Precision: 61.746%   Recall: 71.334%
Valid                   Loss: 1.805e+04   Precision: 30.556%   Recall: 56.122%
Train   Epoch: 208 / 350   Loss: 1.552e+05   Precision: 62.601%   Recall: 70.784%
Valid                   Loss: 1.912e+04   Precision: 31.383%   Recall: 60.204%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.68050385 47.15      ]
	 [54.7387886  22.16666667]
	 [40.40772247 50.45      ]
	 [75.02771759 49.65      ]
	 [41.90259171 52.28333333]]
Train   Epoch: 209 / 350   Loss: 1.36e+05   Precision: 62.365%   Recall: 69.292%
Valid                   Loss: 1.763e+04   Precision: 31.921%   Recall: 57.653%
Train   Epoch: 210 / 350   Loss: 1.47e+05   Precision: 62.732%   Recall: 71.741%
Valid                   Loss: 1.822e+04   Precision: 33.244%   Recall: 63.265%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.26164627  34.55      ]
	 [ 28.62386894   9.21666667]
	 [ 46.49239731  38.8       ]
	 [105.70342255  31.5       ]
	 [ 56.06572342  53.55      ]]
Train   Epoch: 211 / 350   Loss: 1.46e+05   Precision: 62.983%   Recall: 70.338%
Valid                   Loss: 1.863e+04   Precision: 32.698%   Recall: 61.224%
Train   Epoch: 212 / 350   Loss: 1.478e+05   Precision: 63.062%   Recall: 70.481%
Valid                   Loss: 1.755e+04   Precision: 32.036%   Recall: 54.592%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.96268463 75.66666667]
	 [37.10427475 23.08333333]
	 [27.79627609 21.63333333]
	 [87.40721893 58.        ]
	 [67.39440918 34.65      ]]
Train   Epoch: 213 / 350   Loss: 1.51e+05   Precision: 62.391%   Recall: 71.741%
Valid                   Loss: 1.993e+04   Precision: 30.848%   Recall: 61.224%
Train   Epoch: 214 / 350   Loss: 1.483e+05   Precision: 62.782%   Recall: 69.774%
Valid                   Loss: 1.922e+04   Precision: 31.075%   Recall: 67.857%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[286.53494263 237.56666667]
	 [ 97.32889557  50.75      ]
	 [ 38.65948486  30.46666667]
	 [ 58.80335236  28.45      ]
	 [ 38.16431808  45.        ]]
Train   Epoch: 215 / 350   Loss: 1.57e+05   Precision: 62.521%   Recall: 71.334%
Valid                   Loss: 1.903e+04   Precision: 29.952%   Recall: 63.265%
Train   Epoch: 216 / 350   Loss: 1.449e+05   Precision: 62.934%   Recall: 70.386%
Valid                   Loss: 1.799e+04   Precision: 31.250%   Recall: 56.122%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.97769928 44.83333333]
	 [61.75165176 27.38333333]
	 [40.87498474 33.5       ]
	 [28.75131798 24.26666667]
	 [83.99041748 39.75      ]]
Train   Epoch: 217 / 350   Loss: 1.477e+05   Precision: 63.505%   Recall: 69.940%
Valid                   Loss: 1.89e+04   Precision: 30.673%   Recall: 62.755%
Train   Epoch: 218 / 350   Loss: 1.396e+05   Precision: 63.904%   Recall: 69.514%
Valid                   Loss: 1.834e+04   Precision: 30.295%   Recall: 57.653%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.1856842  52.55      ]
	 [55.71175385 18.46666667]
	 [78.22657776 50.56666667]
	 [44.05326843 22.        ]
	 [68.79631042 47.23333333]]
Train   Epoch: 219 / 350   Loss: 1.516e+05   Precision: 63.268%   Recall: 70.537%
Valid                   Loss: 1.831e+04   Precision: 31.880%   Recall: 59.694%
Train   Epoch: 220 / 350   Loss: 1.48e+05   Precision: 63.245%   Recall: 70.727%
Valid                   Loss: 1.816e+04   Precision: 32.143%   Recall: 64.286%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.98360062 54.28333333]
	 [56.79790878 28.58333333]
	 [38.98023224 27.56666667]
	 [39.51587677 26.01666667]
	 [46.23413467 31.53333333]]
Train   Epoch: 221 / 350   Loss: 1.455e+05   Precision: 63.075%   Recall: 71.523%
Valid                   Loss: 1.863e+04   Precision: 31.100%   Recall: 66.327%
Train   Epoch: 222 / 350   Loss: 1.458e+05   Precision: 63.400%   Recall: 70.888%
Valid                   Loss: 1.751e+04   Precision: 33.235%   Recall: 57.653%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 41.59958267  28.23333333]
	 [ 87.07696533 109.83333333]
	 [ 88.44574738  39.66666667]
	 [ 71.71339417  46.25      ]
	 [ 42.00689316  27.88333333]]
Train   Epoch: 223 / 350   Loss: 1.453e+05   Precision: 62.340%   Recall: 71.523%
Valid                   Loss: 1.771e+04   Precision: 32.113%   Recall: 58.163%
Train   Epoch: 224 / 350   Loss: 1.378e+05   Precision: 63.170%   Recall: 71.021%
Valid                   Loss: 1.74e+04   Precision: 32.432%   Recall: 55.102%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.43411255 26.41666667]
	 [35.77724838 27.33333333]
	 [43.03029633 26.03333333]
	 [22.47525215 27.2       ]
	 [45.62153244 41.46666667]]
Train   Epoch: 225 / 350   Loss: 1.607e+05   Precision: 63.675%   Recall: 70.338%
Valid                   Loss: 1.852e+04   Precision: 31.746%   Recall: 61.224%
Train   Epoch: 226 / 350   Loss: 1.509e+05   Precision: 63.488%   Recall: 71.192%
Valid                   Loss: 1.853e+04   Precision: 31.892%   Recall: 60.204%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.54550934  18.83333333]
	 [ 34.53027725  42.        ]
	 [110.44652557 104.5       ]
	 [134.76016235  13.98333333]
	 [ 50.50193787  31.33333333]]
Train   Epoch: 227 / 350   Loss: 1.465e+05   Precision: 62.750%   Recall: 70.215%
Valid                   Loss: 1.803e+04   Precision: 30.816%   Recall: 52.041%
Train   Epoch: 228 / 350   Loss: 1.444e+05   Precision: 63.398%   Recall: 70.045%
Valid                   Loss: 1.777e+04   Precision: 32.378%   Recall: 57.653%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.26160049  18.        ]
	 [ 34.09964752  44.71666667]
	 [ 75.58295441  48.61666667]
	 [108.18389893  49.55      ]
	 [ 45.34572601  39.2       ]]
Train   Epoch: 229 / 350   Loss: 1.439e+05   Precision: 63.594%   Recall: 71.220%
Valid                   Loss: 1.821e+04   Precision: 31.694%   Recall: 59.184%
Train   Epoch: 230 / 350   Loss: 1.453e+05   Precision: 63.286%   Recall: 71.504%
Valid                   Loss: 1.831e+04   Precision: 31.215%   Recall: 57.653%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.02973175 68.08333333]
	 [39.2532959  23.05      ]
	 [84.40156555 56.56666667]
	 [48.94633484 22.91666667]
	 [75.37380981 69.5       ]]
Train   Epoch: 231 / 350   Loss: 1.45e+05   Precision: 63.212%   Recall: 69.504%
Valid                   Loss: 1.812e+04   Precision: 31.856%   Recall: 58.673%
Train   Epoch: 232 / 350   Loss: 1.489e+05   Precision: 63.700%   Recall: 71.030%
Valid                   Loss: 1.797e+04   Precision: 31.686%   Recall: 55.612%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 37.71715164  61.16666667]
	 [ 33.74486923  55.71666667]
	 [ 81.40087891  40.        ]
	 [ 34.55667877  35.05      ]
	 [ 91.38031769 140.11666667]]
Train   Epoch: 233 / 350   Loss: 1.455e+05   Precision: 63.949%   Recall: 69.514%
Valid                   Loss: 1.806e+04   Precision: 31.389%   Recall: 57.653%
Train   Epoch: 234 / 350   Loss: 1.471e+05   Precision: 63.049%   Recall: 70.490%
Valid                   Loss: 1.818e+04   Precision: 31.421%   Recall: 58.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.90755081 49.78333333]
	 [38.58405304 29.96666667]
	 [63.94390488 71.41666667]
	 [89.46937561 79.2       ]
	 [48.86656189 24.11666667]]
Train   Epoch: 235 / 350   Loss: 1.439e+05   Precision: 63.516%   Recall: 72.926%
Valid                   Loss: 1.825e+04   Precision: 31.436%   Recall: 59.184%
Train   Epoch: 236 / 350   Loss: 1.416e+05   Precision: 63.664%   Recall: 71.220%
Valid                   Loss: 1.821e+04   Precision: 32.123%   Recall: 58.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.44025803 20.38333333]
	 [45.0536499  65.11666667]
	 [30.96892357 41.78333333]
	 [31.70115662 15.63333333]
	 [31.35670471 45.33333333]]
Train   Epoch: 237 / 350   Loss: 1.41e+05   Precision: 62.556%   Recall: 69.779%
Valid                   Loss: 1.827e+04   Precision: 30.790%   Recall: 57.653%
Train   Epoch: 240 / 350   Loss: 1.435e+05   Precision: 63.544%   Recall: 70.803%
Valid                   Loss: 1.823e+04   Precision: 31.680%   Recall: 58.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.37995529  27.88333333]
	 [ 68.36756897  11.61666667]
	 [ 72.99565125 150.61666667]
	 [ 34.94319534  27.33333333]
	 [ 67.43331909  64.75      ]]
Train   Epoch: 241 / 350   Loss: 1.433e+05   Precision: 63.268%   Recall: 70.291%
Valid                   Loss: 1.823e+04   Precision: 31.680%   Recall: 58.673%
Train   Epoch: 242 / 350   Loss: 1.59e+05   Precision: 62.983%   Recall: 70.661%
Valid                   Loss: 1.823e+04   Precision: 31.680%   Recall: 58.673%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.83007812 39.73333333]
	 [33.37469101 51.5       ]
	 [34.90457916 22.68333333]
	 [59.7202034  33.21666667]
	 [40.2713623  32.66666667]]
Train   Epoch: 243 / 350   Loss: 1.51e+05   Precision: 59.591%   Recall: 70.206%
Valid                   Loss: 1.88e+04   Precision: 30.162%   Recall: 66.327%
Train   Epoch: 244 / 350   Loss: 1.599e+05   Precision: 58.883%   Recall: 66.480%
Valid                   Loss: 1.719e+04   Precision: 32.566%   Recall: 50.510%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.61338806  19.        ]
	 [ 73.84288025 191.33333333]
	 [ 58.9428978   45.65      ]
	 [ 72.15272522  30.78333333]
	 [ 91.90531158  71.83333333]]
Train   Epoch: 245 / 350   Loss: 1.592e+05   Precision: 58.324%   Recall: 69.940%
Valid                   Loss: 2.289e+04   Precision: 26.087%   Recall: 79.592%
Train   Epoch: 246 / 350   Loss: 1.702e+05   Precision: 58.537%   Recall: 68.120%
Valid                   Loss: 1.751e+04   Precision: 31.847%   Recall: 25.510%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.39945984 61.28333333]
	 [49.12648773 37.43333333]
	 [91.14633179 24.36666667]
	 [24.02360725 20.66666667]
	 [42.71126938 35.05      ]]
Train   Epoch: 247 / 350   Loss: 1.521e+05   Precision: 60.214%   Recall: 69.997%
Valid                   Loss: 1.628e+04   Precision: 32.993%   Recall: 49.490%
Train   Epoch: 248 / 350   Loss: 1.539e+05   Precision: 58.527%   Recall: 70.566%
Valid                   Loss: 1.723e+04   Precision: 31.832%   Recall: 54.082%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.83143234 11.        ]
	 [51.28582764 23.33333333]
	 [74.88864136 29.83333333]
	 [69.46554565 96.33333333]
	 [49.57232285 24.        ]]
Train   Epoch: 249 / 350   Loss: 1.459e+05   Precision: 59.743%   Recall: 68.765%
Valid                   Loss: 1.862e+04   Precision: 28.970%   Recall: 68.878%
Train   Epoch: 250 / 350   Loss: 1.609e+05   Precision: 58.576%   Recall: 68.310%
Valid                   Loss: 2.205e+04   Precision: 22.310%   Recall: 86.735%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.02058411  14.21666667]
	 [ 57.24094009  50.        ]
	 [158.82621765  30.5       ]
	 [103.09942627  47.11666667]
	 [ 61.38048553  46.45      ]]
Train   Epoch: 251 / 350   Loss: 1.571e+05   Precision: 57.504%   Recall: 69.590%
Valid                   Loss: 1.791e+04   Precision: 31.594%   Recall: 55.612%
Train   Epoch: 252 / 350   Loss: 1.489e+05   Precision: 59.630%   Recall: 69.030%
Valid                   Loss: 2.055e+04   Precision: 26.476%   Recall: 70.918%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.31409454  57.98333333]
	 [ 52.07398224  32.25      ]
	 [ 72.93869781  43.76666667]
	 [ 44.50963974  50.55      ]
	 [139.70770264  46.46666667]]
Train   Epoch: 253 / 350   Loss: 1.566e+05   Precision: 58.887%   Recall: 69.286%
Valid                   Loss: 1.629e+04   Precision: 33.913%   Recall: 59.694%
Train   Epoch: 254 / 350   Loss: 1.535e+05   Precision: 58.653%   Recall: 68.755%
Valid                   Loss: 1.882e+04   Precision: 26.386%   Recall: 60.714%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.42280197 130.03333333]
	 [ 45.93395615  58.45      ]
	 [ 82.34355164   9.55      ]
	 [ 48.92624664  50.33333333]
	 [ 87.50238037  43.33333333]]
Train   Epoch: 255 / 350   Loss: 1.584e+05   Precision: 57.899%   Recall: 68.651%
Valid                   Loss: 2.491e+04   Precision: 33.929%   Recall: 19.388%
Train   Epoch: 256 / 350   Loss: 1.477e+05   Precision: 58.237%   Recall: 70.509%
Valid                   Loss: 1.531e+04   Precision: 35.526%   Recall: 27.551%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.19179916 46.56666667]
	 [35.65258026 38.15      ]
	 [33.78466415 24.85      ]
	 [32.31152344 34.41666667]
	 [38.24879074 23.46666667]]
Train   Epoch: 257 / 350   Loss: 1.405e+05   Precision: 58.376%   Recall: 68.812%
Valid                   Loss: 2.141e+04   Precision: 29.288%   Recall: 56.633%
Train   Epoch: 258 / 350   Loss: 1.552e+05   Precision: 57.737%   Recall: 68.689%
Valid                   Loss: 1.819e+04   Precision: 29.206%   Recall: 63.776%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 39.89273834  27.        ]
	 [100.11025238  33.2       ]
	 [ 77.03997803  38.73333333]
	 [ 41.11075592  39.96666667]
	 [ 50.84086609  27.05      ]]
Train   Epoch: 259 / 350   Loss: 1.526e+05   Precision: 59.679%   Recall: 69.337%
Valid                   Loss: 1.882e+04   Precision: 29.767%   Recall: 65.306%
Train   Epoch: 260 / 350   Loss: 1.597e+05   Precision: 57.910%   Recall: 68.120%
Valid                   Loss: 1.878e+04   Precision: 29.540%   Recall: 68.878%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.28823471 16.25      ]
	 [78.94281006 23.16666667]
	 [41.64103699 17.1       ]
	 [61.44128799 24.45      ]
	 [44.59190369 23.21666667]]
Train   Epoch: 261 / 350   Loss: 1.535e+05   Precision: 57.845%   Recall: 69.760%
Valid                   Loss: 1.761e+04   Precision: 34.256%   Recall: 50.510%
Train   Epoch: 262 / 350   Loss: 1.58e+05   Precision: 60.175%   Recall: 67.750%
Valid                   Loss: 1.767e+04   Precision: 35.055%   Recall: 48.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.02406311  29.7       ]
	 [ 61.79681015 181.78333333]
	 [ 42.72254944  32.11666667]
	 [ 32.82563019  33.91666667]
	 [ 42.11412048  31.66666667]]
Train   Epoch: 263 / 350   Loss: 1.645e+05   Precision: 59.610%   Recall: 70.177%
Valid                   Loss: 1.814e+04   Precision: 30.627%   Recall: 42.347%
Train   Epoch: 264 / 350   Loss: 1.442e+05   Precision: 60.540%   Recall: 69.751%
Valid                   Loss: 1.902e+04   Precision: 26.011%   Recall: 75.510%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[172.62121582  63.61666667]
	 [333.29534912 128.28333333]
	 [ 54.71541977  21.03333333]
	 [ 98.36589813 112.38333333]
	 [ 46.77713394  42.63333333]]
Train   Epoch: 265 / 350   Loss: 1.474e+05   Precision: 58.317%   Recall: 70.556%
Valid                   Loss: 1.6e+04   Precision: 35.315%   Recall: 51.531%
Train   Epoch: 266 / 350   Loss: 1.463e+05   Precision: 56.770%   Recall: 67.446%
Valid                   Loss: 1.919e+04   Precision: 28.825%   Recall: 66.327%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 39.18250656  26.36666667]
	 [ 48.68670654  55.55      ]
	 [ 42.20551682  39.48333333]
	 [ 47.616642    15.53333333]
	 [ 64.29769135 134.68333333]]
Train   Epoch: 267 / 350   Loss: 1.431e+05   Precision: 58.383%   Recall: 71.865%
Valid                   Loss: 1.653e+04   Precision: 32.177%   Recall: 52.041%
Train   Epoch: 268 / 350   Loss: 1.497e+05   Precision: 58.759%   Recall: 70.841%
Valid                   Loss: 1.687e+04   Precision: 34.672%   Recall: 48.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.18320084 53.91666667]
	 [38.90228271 33.21666667]
	 [71.34967804 73.61666667]
	 [55.91602707 18.08333333]
	 [60.73374557 34.61666667]]
Train   Epoch: 269 / 350   Loss: 1.491e+05   Precision: 56.692%   Recall: 67.617%
Valid                   Loss: 1.861e+04   Precision: 30.789%   Recall: 61.735%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Presentation set, 300 epochs, lr = 0.00001, augmentation, weighted loss</span>

<span class="n">results_dir_specific</span> <span class="o">=</span> <span class="n">results_dir</span><span class="o">+</span><span class="s2">&quot;presentation_set_e300_lr0p00001_augmentation_weighted_loss/&quot;</span>

<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;best_model_state.pt&quot;</span>
<span class="n">last_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;last_model_state.pt&quot;</span>

<span class="n">train_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_losses.pkl&quot;</span>
<span class="n">train_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_lags_losses.pkl&quot;</span>
<span class="n">train_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="n">valid_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_losses.pkl&quot;</span>
<span class="n">valid_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_lags_losses.pkl&quot;</span>
<span class="n">valid_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_delta_lags_losses.pkl&quot;</span>



<span class="n">perlin_augmentation</span> <span class="o">=</span> <span class="n">PerlinAugmentation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_augmentation_meteorology.pkl&quot;</span><span class="p">),</span> 
                                         <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_augmentation_dust.pkl&quot;</span><span class="p">),</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_train.pkl&quot;</span><span class="p">),</span> 
                                      <span class="n">augmentation</span><span class="o">=</span><span class="n">perlin_augmentation</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_valid.pkl&quot;</span><span class="p">))</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                              <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">300</span>

<span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span><span class="p">,</span>
 <span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses</span><span class="p">,</span>
 <span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                         <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                                         <span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span> <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">save_best_model_dict_to</span><span class="o">=</span><span class="n">best_model_path</span><span class="p">,</span> 
                                                         <span class="n">save_last_model_dict_to</span><span class="o">=</span><span class="n">last_model_path</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">train_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_delta_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span><span class="n">valid_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_delta_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 300   Loss: 2.683e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3229   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 300   Loss: 2.793e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3201   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 7.30038261 30.33333333]
	 [ 7.30094242 18.55      ]
	 [ 7.30061293 21.75      ]
	 [ 7.30078554 10.5       ]
	 [ 7.30055714 35.66666667]]
Train   Epoch: 003 / 300   Loss: 2.745e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3170   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 300   Loss: 2.792e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3140   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 8.11379719 62.06666667]
	 [ 8.11375046 35.        ]
	 [ 8.11431503 37.86666667]
	 [ 8.11396027 62.55      ]
	 [ 8.11388874 28.55      ]]
Train   Epoch: 005 / 300   Loss: 2.599e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3106   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 006 / 300   Loss: 2.693e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3074   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 8.647192   28.        ]
	 [ 8.64723969 49.33333333]
	 [ 8.64741898 42.08333333]
	 [ 8.6470747  76.        ]
	 [ 8.6471262  32.55      ]]
Train   Epoch: 007 / 300   Loss: 2.64e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3043   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 300   Loss: 2.83e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3011   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 9.59309864 77.5       ]
	 [ 9.59306908 28.66666667]
	 [ 9.59309292 36.88333333]
	 [ 9.59313965 28.        ]
	 [ 9.59316444 51.5       ]]
Train   Epoch: 009 / 300   Loss: 2.713e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2977   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 300   Loss: 2.821e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2942   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 10.26797485  23.91666667]
	 [ 10.26799774  15.28333333]
	 [ 10.26796913 142.        ]
	 [ 10.26797676  56.21666667]
	 [ 10.26804447  48.98333333]]
Train   Epoch: 011 / 300   Loss: 2.779e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2909   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 300   Loss: 2.712e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2875   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[10.99965477 41.71666667]
	 [11.00008488 55.81666667]
	 [10.99983025 11.        ]
	 [10.99974155 24.45      ]
	 [10.99985313 32.38333333]]
Train   Epoch: 013 / 300   Loss: 2.615e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2842   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 300   Loss: 2.725e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2809   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[11.81573582 52.        ]
	 [11.81596088 15.53333333]
	 [11.81593704 43.66666667]
	 [11.81576443 67.        ]
	 [11.81603622 78.16666667]]
Train   Epoch: 015 / 300   Loss: 2.57e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2778   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 016 / 300   Loss: 2.482e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2746   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[12.6735487  28.63333333]
	 [12.67358017 44.75      ]
	 [12.67356873 16.58333333]
	 [12.67365742 34.51666667]
	 [12.67361546 63.8       ]]
Train   Epoch: 017 / 300   Loss: 2.574e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2713   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 300   Loss: 2.71e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2679   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[13.35904598 98.78333333]
	 [13.35886765 52.88333333]
	 [13.35889339 44.85      ]
	 [13.3587923  52.78333333]
	 [13.3590517  44.        ]]
Train   Epoch: 019 / 300   Loss: 2.484e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2649   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 020 / 300   Loss: 2.496e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2618   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[14.31569958 58.11666667]
	 [14.31533813 33.86666667]
	 [14.3153553  33.08333333]
	 [14.31549168 31.43333333]
	 [14.31554127 19.33333333]]
Train   Epoch: 021 / 300   Loss: 2.551e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2587   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 022 / 300   Loss: 2.672e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2556   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 15.04531479  23.21666667]
	 [ 15.04547596 102.03333333]
	 [ 15.04526234  23.46666667]
	 [ 15.04504776  60.5       ]
	 [ 15.04510975  41.13333333]]
Train   Epoch: 023 / 300   Loss: 2.389e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2527   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 024 / 300   Loss: 2.499e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2497   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 15.9597826  211.5       ]
	 [ 15.95979691  32.66666667]
	 [ 15.95979404  71.78333333]
	 [ 15.95982647  17.58333333]
	 [ 15.95987511  85.        ]]
Train   Epoch: 025 / 300   Loss: 2.647e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2467   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 026 / 300   Loss: 2.585e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2438   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[16.82636452 30.65      ]
	 [16.8265934  50.33333333]
	 [16.82663155 20.36666667]
	 [16.82670212 12.05      ]
	 [16.82673645 24.75      ]]
Train   Epoch: 027 / 300   Loss: 2.577e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2408   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 028 / 300   Loss: 2.504e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2381   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[17.66850662 21.38333333]
	 [17.6688652  50.86666667]
	 [17.66868973 29.11666667]
	 [17.66891289 22.46666667]
	 [17.66850853 61.75      ]]
Train   Epoch: 029 / 300   Loss: 2.587e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2351   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 030 / 300   Loss: 2.531e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2324   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[18.43006897 44.88333333]
	 [18.43022728 33.56666667]
	 [18.43022728 35.33333333]
	 [18.43032646 50.5       ]
	 [18.43047714  8.        ]]
Train   Epoch: 031 / 300   Loss: 2.535e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2296   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 032 / 300   Loss: 2.48e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2270   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[19.30548859 33.18333333]
	 [19.30574417 28.        ]
	 [19.30562401 35.83333333]
	 [19.30558586 42.08333333]
	 [19.30596733 39.95      ]]
Train   Epoch: 033 / 300   Loss: 2.608e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2242   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 034 / 300   Loss: 2.556e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2215   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[20.04618263 25.36666667]
	 [20.04647255 22.38333333]
	 [20.04621124 31.25      ]
	 [20.04648972 12.66666667]
	 [20.04623032 34.41666667]]
Train   Epoch: 035 / 300   Loss: 2.384e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2189   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 036 / 300   Loss: 2.377e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2164   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 20.90864754 160.71666667]
	 [ 20.90829277  32.3       ]
	 [ 20.90857697  38.        ]
	 [ 20.9084549   69.28333333]
	 [ 20.90847206  45.        ]]
Train   Epoch: 037 / 300   Loss: 2.436e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2140   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 038 / 300   Loss: 2.613e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2113   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[21.66765213 52.55      ]
	 [21.66761589 42.78333333]
	 [21.66759109 19.11666667]
	 [21.6680088  34.03333333]
	 [21.66792297 25.83333333]]
Train   Epoch: 039 / 300   Loss: 2.37e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2089   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 040 / 300   Loss: 2.618e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2065   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 22.62896156 102.        ]
	 [ 22.62868118  29.86666667]
	 [ 22.62900925  14.03333333]
	 [ 22.62855339  26.5       ]
	 [ 22.62858772  73.76666667]]
Train   Epoch: 041 / 300   Loss: 2.426e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2042   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 042 / 300   Loss: 2.517e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2017   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[23.45367622 44.55      ]
	 [23.45405388 82.8       ]
	 [23.45370293 31.86666667]
	 [23.45366096 37.88333333]
	 [23.4540329  18.33333333]]
Train   Epoch: 043 / 300   Loss: 2.448e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1994   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 044 / 300   Loss: 2.376e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1972   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[24.2898407  43.75      ]
	 [24.29044724 70.33333333]
	 [24.28994179 28.88333333]
	 [24.29016113 97.55      ]
	 [24.29037857 23.21666667]]
Train   Epoch: 045 / 300   Loss: 2.407e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1949   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 046 / 300   Loss: 2.471e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1927   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[25.07200623 37.41666667]
	 [25.07237816 15.83333333]
	 [25.07203293 43.75      ]
	 [25.07188988 30.21666667]
	 [25.07203293 27.4       ]]
Train   Epoch: 047 / 300   Loss: 2.508e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1905   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 048 / 300   Loss: 2.626e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1885   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[26.02023888 21.1       ]
	 [26.02006149 28.        ]
	 [26.02069283 26.83333333]
	 [26.02007484 36.25      ]
	 [26.02050209 23.4       ]]
Train   Epoch: 049 / 300   Loss: 2.433e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1863   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 050 / 300   Loss: 2.397e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1842   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[26.74003983 23.11666667]
	 [26.73994637 52.16666667]
	 [26.74000359 18.38333333]
	 [26.7399292  53.25      ]
	 [26.74005127 79.86666667]]
Train   Epoch: 051 / 300   Loss: 2.453e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1823   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 052 / 300   Loss: 2.363e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1803   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[27.64123917 47.28333333]
	 [27.64120865 41.55      ]
	 [27.64105797 52.6       ]
	 [27.64136505 29.61666667]
	 [27.64146805 35.        ]]
Train   Epoch: 053 / 300   Loss: 2.426e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1784   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 054 / 300   Loss: 2.4e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1765   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.51833534 29.66666667]
	 [28.51830292 45.66666667]
	 [28.51859856 34.2       ]
	 [28.51848984 30.05      ]
	 [28.51832581 38.7       ]]
Train   Epoch: 055 / 300   Loss: 2.138e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1747   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 056 / 300   Loss: 2.285e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1729   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[29.41157913 17.18333333]
	 [29.41123009 50.16666667]
	 [29.41144562 54.        ]
	 [29.41121864 51.5       ]
	 [29.411129   23.        ]]
Train   Epoch: 057 / 300   Loss: 2.401e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1711   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 058 / 300   Loss: 2.366e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1693   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  30.29404259 1314.28333333]
	 [  30.29382324   45.51666667]
	 [  30.29348564   45.3       ]
	 [  30.29369164   62.78333333]
	 [  30.29346657   36.        ]]
Train   Epoch: 059 / 300   Loss: 2.386e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1676   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 060 / 300   Loss: 2.441e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1659   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[31.11137772 24.03333333]
	 [31.11145782 73.75      ]
	 [31.11096382 28.25      ]
	 [31.11074638 36.48333333]
	 [31.11134148 36.61666667]]
Train   Epoch: 061 / 300   Loss: 2.464e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1643   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 062 / 300   Loss: 2.372e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1627   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[31.97339249 31.05      ]
	 [31.97338104 23.35      ]
	 [31.97280693 38.68333333]
	 [31.97292137 38.66666667]
	 [31.97277451 27.        ]]
Train   Epoch: 063 / 300   Loss: 2.386e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1611   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 064 / 300   Loss: 2.228e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1596   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 32.87246323 133.86666667]
	 [ 32.87223434  50.61666667]
	 [ 32.87154007  45.8       ]
	 [ 32.8717308   36.        ]
	 [ 32.87214661  63.5       ]]
Train   Epoch: 065 / 300   Loss: 2.464e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1581   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 066 / 300   Loss: 2.245e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1566   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.64154053 47.63333333]
	 [33.64204788 41.7       ]
	 [33.6419487  64.21666667]
	 [33.64169693 14.        ]
	 [33.64212036 50.03333333]]
Train   Epoch: 067 / 300   Loss: 2.3e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1553   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 068 / 300   Loss: 2.332e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1539   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.54887009 34.75      ]
	 [34.54891586 33.93333333]
	 [34.54962158 13.66666667]
	 [34.54971695 17.66666667]
	 [34.54903412 32.68333333]]
Train   Epoch: 069 / 300   Loss: 2.386e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1525   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 070 / 300   Loss: 2.339e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1513   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.43390274 15.08333333]
	 [35.43341446 89.41666667]
	 [35.43373871 30.95      ]
	 [35.4340477  43.55      ]
	 [35.43383026 35.        ]]
Train   Epoch: 071 / 300   Loss: 2.364e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1500   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 072 / 300   Loss: 2.306e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1488   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 36.30733109  25.88333333]
	 [ 36.30804062 148.38333333]
	 [ 36.30764008  77.91666667]
	 [ 36.30734253  22.        ]
	 [ 36.3079071   21.83333333]]
Train   Epoch: 073 / 300   Loss: 2.213e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1476   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 074 / 300   Loss: 2.287e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1465   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.17338562 63.1       ]
	 [37.17331696 17.83333333]
	 [37.17310715 57.25      ]
	 [37.17374802 57.43333333]
	 [37.17351151 67.78333333]]
Train   Epoch: 075 / 300   Loss: 2.339e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1453   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 076 / 300   Loss: 2.243e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1444   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.17396545  36.28333333]
	 [ 38.17421341  29.65      ]
	 [ 38.17450333  82.8       ]
	 [ 38.17460251 199.88333333]
	 [ 38.17465591  49.18333333]]
Train   Epoch: 077 / 300   Loss: 2.359e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1433   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 078 / 300   Loss: 2.266e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1423   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.90208054  38.55      ]
	 [ 38.90164185  26.        ]
	 [ 38.90209579  14.66666667]
	 [ 38.90213776  78.81666667]
	 [ 38.9020462  108.2       ]]
Train   Epoch: 079 / 300   Loss: 2.186e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1414   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 080 / 300   Loss: 2.219e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1404   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.71230316 20.28333333]
	 [39.71230316 12.41666667]
	 [39.71261978 42.28333333]
	 [39.71237183 40.55      ]
	 [39.71211624 31.95      ]]
Train   Epoch: 081 / 300   Loss: 2.592e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1397   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 082 / 300   Loss: 2.308e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1387   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.6829567  27.28333333]
	 [40.68295288 77.83333333]
	 [40.68339539 22.5       ]
	 [40.68299866 35.33333333]
	 [40.68283844 33.58333333]]
Train   Epoch: 083 / 300   Loss: 2.292e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1379   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 084 / 300   Loss: 2.177e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1372   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 41.56635666  44.6       ]
	 [ 41.56676102  26.96666667]
	 [ 41.56674576 150.31666667]
	 [ 41.56618881  32.        ]
	 [ 41.56666183  45.03333333]]
Train   Epoch: 085 / 300   Loss: 2.319e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1365   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 086 / 300   Loss: 2.27e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1358   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.39553452 64.5       ]
	 [42.3960228  12.3       ]
	 [42.39614868 67.56666667]
	 [42.39626694 47.08333333]
	 [42.39605713 25.5       ]]
Train   Epoch: 087 / 300   Loss: 2.389e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1352   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 088 / 300   Loss: 2.296e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1346   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.32130051 22.46666667]
	 [43.32131577 66.33333333]
	 [43.32093048 74.83333333]
	 [43.32138443 87.        ]
	 [43.32144928 17.        ]]
Train   Epoch: 089 / 300   Loss: 2.126e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1340   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 090 / 300   Loss: 2.175e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1335   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.13406372 58.45      ]
	 [44.1341362  35.        ]
	 [44.13439941 36.86666667]
	 [44.13430023 96.16666667]
	 [44.13474655 20.46666667]]
Train   Epoch: 091 / 300   Loss: 2.277e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1330   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 092 / 300   Loss: 2.181e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1326   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.04089355 15.4       ]
	 [45.04039383 33.        ]
	 [45.04081726 25.66666667]
	 [45.0399971  30.38333333]
	 [45.0405426  31.5       ]]
Train   Epoch: 093 / 300   Loss: 2.347e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1322   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 094 / 300   Loss: 2.303e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1318   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.977314   16.46666667]
	 [45.97671127 53.91666667]
	 [45.97750473 38.88333333]
	 [45.97667313 42.85      ]
	 [45.97659302  7.78333333]]
Train   Epoch: 095 / 300   Loss: 2.398e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1315   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 096 / 300   Loss: 2.128e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1312   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.89461899 28.33333333]
	 [46.89488602 90.        ]
	 [46.89434433 74.16666667]
	 [46.89496994 31.        ]
	 [46.89514923 15.08333333]]
Train   Epoch: 097 / 300   Loss: 2.184e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1309   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 098 / 300   Loss: 2.243e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1307   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.77220154  1.68333333]
	 [47.77146149 36.21666667]
	 [47.77199554 84.08333333]
	 [47.7721405  58.36666667]
	 [47.77146912 17.        ]]
Train   Epoch: 099 / 300   Loss: 2.13e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1305   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 100 / 300   Loss: 2.152e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1303   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.52429581 53.        ]
	 [48.52492142 39.75      ]
	 [48.52474594 53.23333333]
	 [48.52445984 51.01666667]
	 [48.52476883 38.        ]]
Train   Epoch: 101 / 300   Loss: 2.198e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1302   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 102 / 300   Loss: 2.166e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1302   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.50020599 105.13333333]
	 [ 49.49947739  55.83333333]
	 [ 49.50004959  60.63333333]
	 [ 49.49995041  24.        ]
	 [ 49.49942017  43.66666667]]
Train   Epoch: 103 / 300   Loss: 2.165e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1301   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 104 / 300   Loss: 2.268e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1302   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.33061981  41.78333333]
	 [ 50.33028793  36.08333333]
	 [ 50.33034134  47.9       ]
	 [ 50.33005142 102.9       ]
	 [ 50.32997513 173.66666667]]
Train   Epoch: 105 / 300   Loss: 2.407e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1302   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 106 / 300   Loss: 2.369e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1303   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.28079224 41.96666667]
	 [51.28141022 71.        ]
	 [51.28147125 22.36666667]
	 [51.28131866 22.61666667]
	 [51.28130722 81.86666667]]
Train   Epoch: 107 / 300   Loss: 2.26e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1304   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 108 / 300   Loss: 2.108e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1305   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 52.07248688  49.3       ]
	 [ 52.07311249  37.93333333]
	 [ 52.07262421  21.78333333]
	 [ 52.07289505 256.66666667]
	 [ 52.07244873  23.86666667]]
Train   Epoch: 109 / 300   Loss: 2.121e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1307   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 110 / 300   Loss: 2.192e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1309   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.04274368 28.8       ]
	 [53.04344559 68.16666667]
	 [53.04282379 21.66666667]
	 [53.04324722 17.        ]
	 [53.04353714 36.75      ]]
Train   Epoch: 111 / 300   Loss: 2.176e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1312   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 112 / 300   Loss: 2.167e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1314   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.89532471 84.58333333]
	 [53.89519501 31.3       ]
	 [53.89546204 46.55      ]
	 [53.8950119  34.5       ]
	 [53.89562225 25.78333333]]
Train   Epoch: 113 / 300   Loss: 2.354e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1318   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 114 / 300   Loss: 2.032e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1321   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 54.72094727 269.38333333]
	 [ 54.72042847  21.16666667]
	 [ 54.72032928  46.95      ]
	 [ 54.72082138  37.55      ]
	 [ 54.72091293  50.45      ]]
Train   Epoch: 115 / 300   Loss: 2.196e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1325   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 116 / 300   Loss: 2.292e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1329   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.63433075 26.5       ]
	 [55.63400269 93.81666667]
	 [55.63399506 42.83333333]
	 [55.63414383 25.83333333]
	 [55.63463211 15.08333333]]
Train   Epoch: 117 / 300   Loss: 2.081e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1333   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 118 / 300   Loss: 2.33e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1338   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.47475433 33.21666667]
	 [56.4747467  25.63333333]
	 [56.47500992 31.83333333]
	 [56.47494888 16.21666667]
	 [56.47468185 21.61666667]]
Train   Epoch: 119 / 300   Loss: 2.224e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1343   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 120 / 300   Loss: 2.085e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1348   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[57.24980164 44.46666667]
	 [57.25034332 70.9       ]
	 [57.25038147 37.92      ]
	 [57.25031281 42.83333333]
	 [57.25032043 43.41666667]]
Train   Epoch: 121 / 300   Loss: 2.199e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1354   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 122 / 300   Loss: 2.245e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1360   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 58.2817421   89.41666667]
	 [ 58.28178406  48.78333333]
	 [ 58.28222656 102.        ]
	 [ 58.28178787  18.3       ]
	 [ 58.28192902  12.        ]]
Train   Epoch: 123 / 300   Loss: 2.138e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1366   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 124 / 300   Loss: 2.26e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1372   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[59.17014313 23.13333333]
	 [59.16957092 53.21666667]
	 [59.16981125 46.21666667]
	 [59.17000198 65.3       ]
	 [59.16968918 24.78333333]]
Train   Epoch: 125 / 300   Loss: 2.145e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1379   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 126 / 300   Loss: 2.112e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1386   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 60.05539322  25.88333333]
	 [ 60.05532455 279.33333333]
	 [ 60.05580521  22.        ]
	 [ 60.05538559  59.66666667]
	 [ 60.0555687   55.25      ]]
Train   Epoch: 127 / 300   Loss: 2.306e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1393   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 128 / 300   Loss: 2.103e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1401   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.85609055 51.12      ]
	 [60.85620499 30.16666667]
	 [60.85657501 16.3       ]
	 [60.85615921 70.5       ]
	 [60.85664368 20.55      ]]
Train   Epoch: 129 / 300   Loss: 2.182e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1408   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 130 / 300   Loss: 2.191e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1416   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.69705582 27.16666667]
	 [61.69697189 51.33333333]
	 [61.69667435 32.5       ]
	 [61.6969223  22.16666667]
	 [61.69715881 30.48333333]]
Train   Epoch: 131 / 300   Loss: 2.243e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1425   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 132 / 300   Loss: 2.037e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1433   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.50331879  37.86666667]
	 [ 62.50354767 133.66666667]
	 [ 62.50322723  35.        ]
	 [ 62.50310516  60.08333333]
	 [ 62.50337982  36.83333333]]
Train   Epoch: 133 / 300   Loss: 2.215e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1441   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 134 / 300   Loss: 2.18e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1450   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.36587143 117.63333333]
	 [ 63.36572266  94.65      ]
	 [ 63.3656311   15.48333333]
	 [ 63.36580276  29.66666667]
	 [ 63.36581421  47.66666667]]
Train   Epoch: 135 / 300   Loss: 2.25e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1459   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 136 / 300   Loss: 2.32e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1469   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 64.24873352 151.        ]
	 [ 64.24855804  30.48333333]
	 [ 64.24867249  61.36666667]
	 [ 64.24872589  38.        ]
	 [ 64.248909    25.66666667]]
Train   Epoch: 137 / 300   Loss: 2.371e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1478   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 138 / 300   Loss: 2.236e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1488   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 65.0597229  103.2       ]
	 [ 65.05966949  25.16666667]
	 [ 65.05995941  60.96666667]
	 [ 65.05977631  47.83333333]
	 [ 65.06001282  36.83333333]]
Train   Epoch: 139 / 300   Loss: 2.213e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1497   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 140 / 300   Loss: 2.208e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1507   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[65.9316864   0.55      ]
	 [65.93157196 32.11666667]
	 [65.93180847 99.31666667]
	 [65.93175507 19.58333333]
	 [65.93196869 70.45      ]]
Train   Epoch: 141 / 300   Loss: 2.305e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1517   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 142 / 300   Loss: 2.123e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1527   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[66.75751495 45.21666667]
	 [66.75740051 56.45      ]
	 [66.75744629 52.41666667]
	 [66.75738525 40.21666667]
	 [66.75717163 22.53333333]]
Train   Epoch: 143 / 300   Loss: 2.198e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1537   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 144 / 300   Loss: 2.125e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1547   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[67.55664062 41.11666667]
	 [67.55695343 12.66666667]
	 [67.55695343 15.        ]
	 [67.55673981 73.21666667]
	 [67.55684662 17.21666667]]
Train   Epoch: 145 / 300   Loss: 2.237e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1557   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 146 / 300   Loss: 2.064e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1568   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.31313324  36.        ]
	 [ 68.31299591  43.5       ]
	 [ 68.31322479 126.13333333]
	 [ 68.31307983  28.86666667]
	 [ 68.31299591  12.45      ]]
Train   Epoch: 147 / 300   Loss: 2.009e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1577   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 148 / 300   Loss: 2.248e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1589   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[69.06536102 54.63333333]
	 [69.06523132 32.53333333]
	 [69.06520844 40.83333333]
	 [69.06537628 39.73333333]
	 [69.06528473 44.66666667]]
Train   Epoch: 149 / 300   Loss: 2.09e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1597   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 150 / 300   Loss: 2.078e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1607   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[69.92980957 54.5       ]
	 [69.92989349 33.        ]
	 [69.92996216 20.9       ]
	 [69.92973328 43.11666667]
	 [69.92976379 72.46666667]]
Train   Epoch: 151 / 300   Loss: 2.159e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1619   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 152 / 300   Loss: 2.211e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1629   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[70.72223663 19.16666667]
	 [70.72210693 60.5       ]
	 [70.72215271 24.        ]
	 [70.72221375 30.56666667]
	 [70.72244263  7.91666667]]
Train   Epoch: 153 / 300   Loss: 2.075e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1641   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 154 / 300   Loss: 2.013e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1650   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 71.49227142  27.03333333]
	 [ 71.49214172  29.        ]
	 [ 71.49220276  62.11666667]
	 [ 71.49214172 128.58333333]
	 [ 71.49220276  78.11666667]]
Train   Epoch: 155 / 300   Loss: 2.14e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1662   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 156 / 300   Loss: 1.978e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1673   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[72.11645508 39.05      ]
	 [72.11642456 39.8       ]
	 [72.11610413 17.5       ]
	 [72.11610413 33.06666667]
	 [72.11618042 40.        ]]
Train   Epoch: 157 / 300   Loss: 2.081e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1682   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 158 / 300   Loss: 2.07e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1692   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[72.88676453 14.        ]
	 [72.88677979 33.4       ]
	 [72.88665771 34.83333333]
	 [72.88679504 22.53333333]
	 [72.88678741 14.66666667]]
Train   Epoch: 159 / 300   Loss: 2.149e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1702   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 160 / 300   Loss: 2.277e+04   Precision: 38.293%   Recall: 1.052%
Valid                   Loss:    1713   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[73.64051056 26.03333333]
	 [73.64051819 49.38333333]
	 [73.64038849 26.66666667]
	 [73.64041901 24.71666667]
	 [73.64044189 31.55      ]]
Train   Epoch: 161 / 300   Loss: 2.169e+04   Precision: 39.821%   Recall: 98.171%
Valid                   Loss:    1725   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 162 / 300   Loss: 2.11e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1735   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[74.43301392 38.18333333]
	 [74.43299866 42.83333333]
	 [74.43309784 54.5       ]
	 [74.43301392  9.5       ]
	 [74.4331131  13.53333333]]
Train   Epoch: 163 / 300   Loss: 2.059e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1743   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 164 / 300   Loss: 2.088e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1757   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[75.09126282 37.33333333]
	 [75.09132385 67.43333333]
	 [75.09116364 33.78333333]
	 [75.09130859 59.88333333]
	 [75.09127045 34.05      ]]
Train   Epoch: 165 / 300   Loss: 2.14e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1768   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 166 / 300   Loss: 2.332e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1777   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[75.94482422 32.33333333]
	 [75.94484711 24.06666667]
	 [75.9449234  25.36666667]
	 [75.94483948 59.83333333]
	 [75.94483948 22.16666667]]
Train   Epoch: 167 / 300   Loss: 2.245e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1789   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 168 / 300   Loss: 2.302e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1801   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 76.64761353  25.83333333]
	 [ 76.64756775  83.        ]
	 [ 76.64761353  12.66666667]
	 [ 76.64762878 104.21666667]
	 [ 76.64759064  45.36666667]]
Train   Epoch: 169 / 300   Loss: 2.209e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1809   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 170 / 300   Loss: 2.238e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1821   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[77.42943573 48.21666667]
	 [77.42943573 37.88333333]
	 [77.42947388 12.41666667]
	 [77.42943573 26.83333333]
	 [77.42945099 51.25      ]]
Train   Epoch: 171 / 300   Loss: 2.145e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1831   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 172 / 300   Loss: 2.064e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1842   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.04405975 37.        ]
	 [78.04405975 41.86666667]
	 [78.04409027 26.33333333]
	 [78.04409027 22.45      ]
	 [78.04409027 30.        ]]
Train   Epoch: 173 / 300   Loss: 2.271e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1851   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 174 / 300   Loss: 2.143e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1857   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.77819061 27.5       ]
	 [78.7781601  35.8       ]
	 [78.77818298 54.46666667]
	 [78.77817535 38.25      ]
	 [78.77817535 23.66666667]]
Train   Epoch: 175 / 300   Loss: 2.126e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1870   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 176 / 300   Loss: 2.184e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1881   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[79.39626312 20.63333333]
	 [79.39624023 19.7       ]
	 [79.39625549 58.        ]
	 [79.3962326  26.53333333]
	 [79.3962326  54.58333333]]
Train   Epoch: 177 / 300   Loss: 2.195e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1888   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 178 / 300   Loss: 2.173e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1900   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.00857544  47.91666667]
	 [ 80.00854492  47.9       ]
	 [ 80.00855255  29.55      ]
	 [ 80.00858307  40.38333333]
	 [ 80.00853729 214.36666667]]
Train   Epoch: 179 / 300   Loss: 2.05e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1908   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 180 / 300   Loss: 2.178e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1919   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.58335876 33.83333333]
	 [80.58341217 28.11666667]
	 [80.58338165 34.        ]
	 [80.58338928 32.08333333]
	 [80.58335114 55.36666667]]
Train   Epoch: 181 / 300   Loss: 2.093e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1924   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 182 / 300   Loss: 2.087e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1935   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[81.13663483 32.        ]
	 [81.13661957 32.        ]
	 [81.13661194 29.66666667]
	 [81.13661194 44.75      ]
	 [81.13663483 39.36666667]]
Train   Epoch: 183 / 300   Loss: 2.19e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1938   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 184 / 300   Loss: 2.145e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1950   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[81.76644135 20.11666667]
	 [81.76646423 34.45      ]
	 [81.7664566  58.33333333]
	 [81.76644135 23.7       ]
	 [81.76646423 38.5       ]]
Train   Epoch: 185 / 300   Loss: 2.227e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1962   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 186 / 300   Loss: 2.138e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1965   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 82.34875488  21.91666667]
	 [ 82.34875488  48.81666667]
	 [ 82.34876251 403.66666667]
	 [ 82.34874725  32.63333333]
	 [ 82.34874725  65.28333333]]
Train   Epoch: 187 / 300   Loss: 1.987e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1975   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 188 / 300   Loss: 2.071e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1984   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[82.83965302 51.33333333]
	 [82.83966827 28.23333333]
	 [82.8396225  28.45      ]
	 [82.83959961 50.83333333]
	 [82.83968353 51.16666667]]
Train   Epoch: 189 / 300   Loss: 2.057e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1986   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 190 / 300   Loss: 2.196e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1997   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[83.45661163 44.5       ]
	 [83.45664215 39.18333333]
	 [83.45665741 31.        ]
	 [83.45664978 37.16666667]
	 [83.45663452 65.        ]]
Train   Epoch: 191 / 300   Loss: 2.186e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2008   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 192 / 300   Loss: 2.174e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2013   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[83.97345734 20.5       ]
	 [83.97344208 11.11666667]
	 [83.97342682 58.25      ]
	 [83.97342682  2.15      ]
	 [83.97343445 59.45      ]]
Train   Epoch: 193 / 300   Loss: 2.125e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2019   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 194 / 300   Loss: 1.986e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2026   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[84.4032135  13.16666667]
	 [84.40322876 34.53333333]
	 [84.40322876 51.7       ]
	 [84.40323639 63.4       ]
	 [84.40322113 36.55      ]]
Train   Epoch: 195 / 300   Loss: 2.299e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2035   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 196 / 300   Loss: 1.988e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2040   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 84.95171356  36.55      ]
	 [ 84.95172882  19.61666667]
	 [ 84.95169067 223.        ]
	 [ 84.95169067  42.55      ]
	 [ 84.95171356  27.83333333]]
Train   Epoch: 197 / 300   Loss: 2.1e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2045   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 198 / 300   Loss: 2.072e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2054   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[85.38636017 34.73333333]
	 [85.38638306 24.        ]
	 [85.38639069 89.65      ]
	 [85.38639069 59.71666667]
	 [85.38638306 38.21666667]]
Train   Epoch: 199 / 300   Loss: 2.031e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2057   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 200 / 300   Loss: 1.941e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2061   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 85.7990799   45.53333333]
	 [ 85.79907227  36.45      ]
	 [ 85.79905701 152.56666667]
	 [ 85.79905701  38.21666667]
	 [ 85.79906464  73.25      ]]
Train   Epoch: 201 / 300   Loss: 2.167e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2073   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 202 / 300   Loss: 2.069e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2079   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.21957397 14.41666667]
	 [86.21961975 38.        ]
	 [86.21958923 24.71666667]
	 [86.2195816  52.        ]
	 [86.21954346 30.11666667]]
Train   Epoch: 203 / 300   Loss: 1.987e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2080   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 204 / 300   Loss: 2.054e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2082   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 86.60549927 191.91666667]
	 [ 86.60558319  72.16666667]
	 [ 86.6055603   31.1       ]
	 [ 86.60552979  38.43333333]
	 [ 86.60559082  56.2       ]]
Train   Epoch: 205 / 300   Loss: 2.137e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2094   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 206 / 300   Loss: 2.165e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2100   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 86.99230957  76.        ]
	 [ 86.99230957 109.45      ]
	 [ 86.99237061  22.13333333]
	 [ 86.99235535  37.95      ]
	 [ 86.99239349  38.21666667]]
Train   Epoch: 207 / 300   Loss: 2.177e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2106   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 208 / 300   Loss: 2.171e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2111   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.40901184 59.33333333]
	 [87.4090271  57.05      ]
	 [87.40905762 31.05      ]
	 [87.40903473 30.36666667]
	 [87.4090271  64.43333333]]
Train   Epoch: 209 / 300   Loss: 2.207e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2121   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 210 / 300   Loss: 1.958e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2123   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.7223053  99.8       ]
	 [87.72233582 14.16666667]
	 [87.7223053  10.83333333]
	 [87.72233582 25.48333333]
	 [87.72234344 16.        ]]
Train   Epoch: 211 / 300   Loss: 2.152e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2127   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 212 / 300   Loss: 2.126e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2131   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.07129669 24.55      ]
	 [88.07128906 51.88333333]
	 [88.07131958 43.05      ]
	 [88.07128906 24.5       ]
	 [88.07128143 34.25      ]]
Train   Epoch: 213 / 300   Loss: 2.1e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2135   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 214 / 300   Loss: 2.179e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2144   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.41561127 65.88333333]
	 [88.41566467 48.01666667]
	 [88.4156723  45.75      ]
	 [88.41568756 21.45      ]
	 [88.4156723  23.41666667]]
Train   Epoch: 215 / 300   Loss: 2.153e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2150   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 216 / 300   Loss: 2.083e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2151   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.60144806 28.5       ]
	 [88.60140228 53.58333333]
	 [88.6014328  26.5       ]
	 [88.60142517 62.        ]
	 [88.60137939  4.91666667]]
Train   Epoch: 217 / 300   Loss: 2.173e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2152   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 218 / 300   Loss: 2.456e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2161   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.97442627 378.16666667]
	 [ 88.97444916  56.28333333]
	 [ 88.97442627  32.6       ]
	 [ 88.97451019  32.05      ]
	 [ 88.97444916  14.33333333]]
Train   Epoch: 219 / 300   Loss: 2.296e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2169   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 220 / 300   Loss: 2.163e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2169   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.22770691 66.53333333]
	 [89.22777557 40.78333333]
	 [89.22771454 28.        ]
	 [89.22776031 36.43333333]
	 [89.22770691 67.83333333]]
Train   Epoch: 221 / 300   Loss: 2.156e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2163   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 222 / 300   Loss: 2.218e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2174   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.52774811 28.33333333]
	 [89.52779388 37.61666667]
	 [89.527771   63.4       ]
	 [89.52779388 36.63333333]
	 [89.52778625 55.        ]]
Train   Epoch: 223 / 300   Loss: 2.274e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2184   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 224 / 300   Loss: 2.076e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2181   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.75319672 45.53333333]
	 [89.75307465 30.35      ]
	 [89.75313568 25.68333333]
	 [89.75311279 47.6       ]
	 [89.75315094 82.2       ]]
Train   Epoch: 225 / 300   Loss: 2.142e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2185   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 226 / 300   Loss: 2.153e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2185   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.08412933 33.21666667]
	 [90.08412933 19.66666667]
	 [90.08412933 43.28333333]
	 [90.0841217  37.23333333]
	 [90.08412933 45.75      ]]
Train   Epoch: 227 / 300   Loss: 2.109e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2190   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 228 / 300   Loss: 2.191e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2188   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.28740692  71.95      ]
	 [ 90.28736115  24.        ]
	 [ 90.28736877  90.06666667]
	 [ 90.28740692 128.5       ]
	 [ 90.28743744  49.71666667]]
Train   Epoch: 229 / 300   Loss: 2.082e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2192   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 230 / 300   Loss: 1.947e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2192   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.45917511 222.5       ]
	 [ 90.45912933  31.71666667]
	 [ 90.45896149  39.6       ]
	 [ 90.45906067  43.        ]
	 [ 90.4590683   89.        ]]
Train   Epoch: 231 / 300   Loss: 2.212e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2205   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 232 / 300   Loss: 2.076e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2197   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.64250946 25.75      ]
	 [90.64252472 33.33333333]
	 [90.64246368 22.78333333]
	 [90.64247131 57.26666667]
	 [90.64250946 25.93333333]]
Train   Epoch: 233 / 300   Loss: 2.07e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2204   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 234 / 300   Loss: 2.21e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2207   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.82350159 28.41666667]
	 [90.82349396 21.61666667]
	 [90.82349396 29.96666667]
	 [90.82349396 44.56666667]
	 [90.8235321  10.11666667]]
Train   Epoch: 235 / 300   Loss: 2.214e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2212   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 236 / 300   Loss: 2.387e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2219   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.02001953 44.53333333]
	 [91.02012634 31.08333333]
	 [91.02011108 59.53333333]
	 [91.0201416  54.05      ]
	 [91.0201416  29.41666667]]
Train   Epoch: 237 / 300   Loss: 2.05e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2218   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 238 / 300   Loss: 2.081e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2221   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.19052887 37.28333333]
	 [91.19049072 26.33333333]
	 [91.19052124 22.73333333]
	 [91.19050598 25.78333333]
	 [91.19052124 64.58333333]]
Train   Epoch: 239 / 300   Loss: 2.165e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2228   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 240 / 300   Loss: 2.114e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2226   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.26776123 36.6       ]
	 [91.26776123 28.68333333]
	 [91.26774597 38.13333333]
	 [91.26774597 75.55      ]
	 [91.26774597 19.5       ]]
Train   Epoch: 241 / 300   Loss: 2.176e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2233   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 242 / 300   Loss: 2.096e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2233   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  91.43164825   21.66666667]
	 [  91.43157196 1162.        ]
	 [  91.4316864    32.55      ]
	 [  91.4316864    29.98333333]
	 [  91.43154144   14.5       ]]
Train   Epoch: 243 / 300   Loss: 2.083e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2232   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 244 / 300   Loss: 2.169e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2228   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.56140137 43.18333333]
	 [91.56140137 16.63333333]
	 [91.56139374 18.88333333]
	 [91.56140137 36.45      ]
	 [91.56137848 55.        ]]
Train   Epoch: 245 / 300   Loss: 2.321e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2235   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 246 / 300   Loss: 2.219e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2240   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.75202942 112.96666667]
	 [ 91.75205994  18.28333333]
	 [ 91.7521286   34.        ]
	 [ 91.7520752   24.3       ]
	 [ 91.75210571  43.        ]]
Train   Epoch: 247 / 300   Loss: 2.237e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2248   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 248 / 300   Loss: 2.157e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2245   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.87123871 22.61666667]
	 [91.87123871 31.83333333]
	 [91.87129211 43.33333333]
	 [91.87129211 77.83333333]
	 [91.87122345 74.3       ]]
Train   Epoch: 249 / 300   Loss: 2.113e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2239   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 250 / 300   Loss: 2.204e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2248   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.00739288 37.        ]
	 [92.00746918 50.        ]
	 [92.00743866 17.33333333]
	 [92.00739288 35.3       ]
	 [92.00732422 33.93333333]]
Train   Epoch: 251 / 300   Loss: 2.164e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2249   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 252 / 300   Loss: 2.144e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2250   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.09341431 19.95      ]
	 [92.09346008 29.66666667]
	 [92.09340668 24.55      ]
	 [92.09339905  8.38333333]
	 [92.09345245 42.76666667]]
Train   Epoch: 253 / 300   Loss: 2.213e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2245   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 254 / 300   Loss: 2.349e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2251   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.10494995  39.25      ]
	 [ 92.10493469 190.5       ]
	 [ 92.10501862  50.86666667]
	 [ 92.10498047  58.88333333]
	 [ 92.10502625  30.45      ]]
Train   Epoch: 255 / 300   Loss: 2.02e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2253   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 256 / 300   Loss: 2.138e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2254   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.1965332   49.11666667]
	 [ 92.19652557 175.5       ]
	 [ 92.19663239  28.        ]
	 [ 92.19652557  21.16666667]
	 [ 92.19657135  62.95      ]]
Train   Epoch: 257 / 300   Loss: 2.268e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2262   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 258 / 300   Loss: 2.154e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2258   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.2914505  83.        ]
	 [92.29138947 43.16666667]
	 [92.29133606 17.93333333]
	 [92.29136658 35.5       ]
	 [92.29146576 35.88333333]]
Train   Epoch: 259 / 300   Loss: 2.072e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2253   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 260 / 300   Loss: 2.309e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2262   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.35713959 107.08333333]
	 [ 92.35717773 113.05      ]
	 [ 92.35713959  22.43333333]
	 [ 92.3571701   31.5       ]
	 [ 92.35713959  19.45      ]]
Train   Epoch: 261 / 300   Loss: 2.138e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2262   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 262 / 300   Loss: 2.056e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2260   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.4549942  38.11666667]
	 [92.4549942  41.13333333]
	 [92.45496368 23.58333333]
	 [92.45496368 49.05      ]
	 [92.45496368 32.78333333]]
Train   Epoch: 263 / 300   Loss: 2.098e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2255   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 264 / 300   Loss: 2.177e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2263   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.52901459 100.21666667]
	 [ 92.52906799  25.13333333]
	 [ 92.52902222  27.91666667]
	 [ 92.52903748  26.83333333]
	 [ 92.52907562  63.21666667]]
Train   Epoch: 265 / 300   Loss: 2.118e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2256   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 266 / 300   Loss: 2.144e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2257   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.58447266 34.36666667]
	 [92.58443451 73.83333333]
	 [92.58448029 41.75      ]
	 [92.58448029 28.66666667]
	 [92.58459473 28.78333333]]
Train   Epoch: 267 / 300   Loss: 2.166e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2257   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 268 / 300   Loss: 2.007e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2248   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.49189758 18.4       ]
	 [92.49187469 32.43333333]
	 [92.49189758 26.05      ]
	 [92.49192047 21.5       ]
	 [92.49186707 53.73333333]]
Train   Epoch: 269 / 300   Loss: 2.332e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2261   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 270 / 300   Loss: 2.093e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2265   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.63786316 53.21666667]
	 [92.63784027 24.36666667]
	 [92.63781738 24.25      ]
	 [92.63783264 60.75      ]
	 [92.63780975 31.11666667]]
Train   Epoch: 271 / 300   Loss: 2.282e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2267   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 272 / 300   Loss: 2.152e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2270   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.70239258 45.05      ]
	 [92.70237732 22.5       ]
	 [92.70237732 41.58333333]
	 [92.70238495 57.78333333]
	 [92.70230865 29.53333333]]
Train   Epoch: 273 / 300   Loss: 2.114e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2264   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 274 / 300   Loss: 2.018e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2265   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.79154968 30.41666667]
	 [92.79149628 45.55      ]
	 [92.79160309 18.78333333]
	 [92.79160309 57.21666667]
	 [92.79161072 37.8       ]]
Train   Epoch: 275 / 300   Loss: 2.184e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2263   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 276 / 300   Loss: 2.01e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2253   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.68571472 26.21666667]
	 [92.68553925 26.26666667]
	 [92.68577576 31.66666667]
	 [92.68560791 36.88333333]
	 [92.68595123 17.73333333]]
Train   Epoch: 277 / 300   Loss: 2.235e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2245   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 278 / 300   Loss: 2.119e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2265   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.86664581 16.31666667]
	 [92.86675262 26.16666667]
	 [92.86669922 70.66666667]
	 [92.86668396 52.88333333]
	 [92.86665344 52.5       ]]
Train   Epoch: 279 / 300   Loss: 2.13e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2258   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 280 / 300   Loss: 2.055e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2262   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.91668701 24.66666667]
	 [92.91669464 50.55      ]
	 [92.91671753 28.36666667]
	 [92.91665649 23.13333333]
	 [92.91666412 16.7       ]]
Train   Epoch: 281 / 300   Loss: 2.058e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2259   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 282 / 300   Loss: 2.073e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2252   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.78869629 25.8       ]
	 [92.78911591 25.83333333]
	 [92.78902435 75.58333333]
	 [92.78894806 69.        ]
	 [92.78905487 44.83333333]]
Train   Epoch: 283 / 300   Loss: 2.056e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2245   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 284 / 300   Loss: 2.222e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2260   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.90749359 53.        ]
	 [92.90738678 77.83333333]
	 [92.90742493 46.5       ]
	 [92.90727997 40.83333333]
	 [92.90759277 10.93333333]]
Train   Epoch: 285 / 300   Loss: 2.158e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2252   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 286 / 300   Loss: 2.25e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2260   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.89562988  31.88333333]
	 [ 92.89581299  24.33333333]
	 [ 92.89598846  33.63333333]
	 [ 92.8957901   22.5       ]
	 [ 92.89578247 126.5       ]]
Train   Epoch: 287 / 300   Loss: 2.147e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2265   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 288 / 300   Loss: 2.167e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2275   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.94272614 37.11666667]
	 [92.94271088 23.21666667]
	 [92.94280243 49.        ]
	 [92.94278717 33.        ]
	 [92.94271088 19.45      ]]
Train   Epoch: 289 / 300   Loss: 2.019e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2265   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 290 / 300   Loss: 1.954e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2255   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.6578064   31.5       ]
	 [ 92.65769958  27.        ]
	 [ 92.6578064   76.03333333]
	 [ 92.65774536  17.        ]
	 [ 92.65789795 363.03333333]]
Train   Epoch: 291 / 300   Loss: 2.117e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2259   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 292 / 300   Loss: 2.078e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2256   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.6957016  58.75      ]
	 [92.69589233 18.6       ]
	 [92.69599152 64.38333333]
	 [92.69590759 75.45      ]
	 [92.69572449 55.81666667]]
Train   Epoch: 293 / 300   Loss: 2.119e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2259   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 294 / 300   Loss: 2.271e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2270   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.87785339 25.61666667]
	 [92.87784576 18.75      ]
	 [92.87785339 28.65      ]
	 [92.87788391 20.        ]
	 [92.87793732 51.33333333]]
Train   Epoch: 295 / 300   Loss: 2.066e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2267   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 296 / 300   Loss: 2.13e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2272   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.89160156 17.13333333]
	 [92.89159393 17.95      ]
	 [92.89156342 25.78333333]
	 [92.89156342 42.46666667]
	 [92.8915863  38.66666667]]
Train   Epoch: 297 / 300   Loss: 2.172e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2273   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 298 / 300   Loss: 2.14e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2266   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.94386292 21.2       ]
	 [92.94390869 31.05      ]
	 [92.94384766 46.16666667]
	 [92.94377136 55.25      ]
	 [92.94385529 30.6       ]]
Train   Epoch: 299 / 300   Loss: 2.179e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2277   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 300 / 300   Loss: 2.16e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2272   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.92488098 23.33333333]
	 [92.92490387 33.55      ]
	 [92.92485046 27.45      ]
	 [92.92491913 68.93333333]
	 [92.92494965 41.56666667]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Presentation set, 450 epochs, lr = 0.0001, no augmentation</span>

<span class="n">results_dir_specific</span> <span class="o">=</span> <span class="n">results_dir</span><span class="o">+</span><span class="s2">&quot;presentation_set_e450_lr0p0001_noaugmentation/&quot;</span>


<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;best_model_state.pt&quot;</span>
<span class="n">last_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;last_model_state.pt&quot;</span>

<span class="n">train_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_losses.pkl&quot;</span>
<span class="n">train_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_lags_losses.pkl&quot;</span>
<span class="n">train_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="n">valid_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_losses.pkl&quot;</span>
<span class="n">valid_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_lags_losses.pkl&quot;</span>
<span class="n">valid_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_delta_lags_losses.pkl&quot;</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_train.pkl&quot;</span><span class="p">))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_valid.pkl&quot;</span><span class="p">))</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">450</span>

<span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span><span class="p">,</span>
 <span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses</span><span class="p">,</span>
 <span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                         <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                                         <span class="n">loss_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">save_best_model_dict_to</span><span class="o">=</span><span class="n">best_model_path</span><span class="p">,</span> 
                                                         <span class="n">save_last_model_dict_to</span><span class="o">=</span><span class="n">last_model_path</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">train_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_delta_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span><span class="n">valid_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_delta_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 450   Loss: 4.481e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    5291   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 450   Loss: 4.305e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    4710   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[13.26804066 13.38333333]
	 [13.26757622 66.        ]
	 [13.26765347 25.3       ]
	 [13.26856804 15.        ]
	 [13.26830578 38.31666667]]
Train   Epoch: 003 / 450   Loss: 4.224e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    4164   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 450   Loss: 4.033e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3683   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 21.97319984  18.21666667]
	 [ 21.97380257  18.55      ]
	 [ 21.97281075  69.33333333]
	 [ 21.97291946  46.8       ]
	 [ 21.97188187 214.01666667]]
Train   Epoch: 005 / 450   Loss: 4.261e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    3270   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 006 / 450   Loss: 4.174e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2944   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[30.98970222 24.33333333]
	 [30.98934364 23.16666667]
	 [30.98931503 59.21666667]
	 [30.99030113 16.71666667]
	 [30.98960114 35.83333333]]
Train   Epoch: 007 / 450   Loss: 3.988e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2690   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 450   Loss: 3.718e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2511   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.81838608 38.5       ]
	 [39.8197403  32.8       ]
	 [39.81927109 37.05      ]
	 [39.81863022 31.05      ]
	 [39.81943893 28.25      ]]
Train   Epoch: 009 / 450   Loss: 3.695e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2393   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 450   Loss: 3.745e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2333   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.79001236  60.95      ]
	 [ 47.79045105 271.58333333]
	 [ 47.78941727  38.96666667]
	 [ 47.79001236 167.93333333]
	 [ 47.79144669 227.56666667]]
Train   Epoch: 011 / 450   Loss: 3.603e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2326   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 450   Loss: 4.003e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2360   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.05947876 52.46666667]
	 [56.0591011  35.16666667]
	 [56.05877686 84.91666667]
	 [56.05958176 13.51666667]
	 [56.05914688 39.01666667]]
Train   Epoch: 013 / 450   Loss: 3.554e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2424   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 450   Loss: 3.554e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2514   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.89948273 48.01666667]
	 [62.89941788 31.21666667]
	 [62.89896393 16.33333333]
	 [62.89910889 58.11666667]
	 [62.8984375  27.5       ]]
Train   Epoch: 015 / 450   Loss: 3.731e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2620   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 016 / 450   Loss: 3.542e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2729   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[69.19602966 58.61666667]
	 [69.19503021 48.33333333]
	 [69.19612122 42.56666667]
	 [69.19573212 18.        ]
	 [69.19608307 21.11666667]]
Train   Epoch: 017 / 450   Loss: 3.589e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2848   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 450   Loss: 3.425e+04   Precision: 39.860%   Recall: 43.787%
Valid                   Loss:    2955   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 74.60416412  54.88333333]
	 [ 74.60464478  48.1       ]
	 [ 74.60406494  35.1       ]
	 [ 74.60369873  93.33333333]
	 [ 74.60455322 114.65      ]]
Train   Epoch: 019 / 450   Loss: 3.388e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3057   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 020 / 450   Loss: 3.615e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3147   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[79.06655884 17.81666667]
	 [79.06663513 53.18333333]
	 [79.06615448 69.93333333]
	 [79.06633759 31.        ]
	 [79.0663147  43.        ]]
Train   Epoch: 021 / 450   Loss: 3.498e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3244   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 022 / 450   Loss: 3.841e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3322   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 83.00183105  22.53333333]
	 [ 83.0020752  245.53333333]
	 [ 83.00176239  41.73333333]
	 [ 83.00188446  69.28333333]
	 [ 83.00183868  38.16666667]]
Train   Epoch: 023 / 450   Loss: 3.62e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3413   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 024 / 450   Loss: 3.685e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3476   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.21678925 23.45      ]
	 [86.21721649 42.58333333]
	 [86.21660614 22.33333333]
	 [86.21694946 85.05      ]
	 [86.21702576 30.5       ]]
Train   Epoch: 025 / 450   Loss: 3.227e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3498   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 026 / 450   Loss: 3.431e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3563   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 87.93899536  13.68333333]
	 [ 87.93158722 186.36666667]
	 [ 87.93922424  39.08333333]
	 [ 87.94171906  23.75      ]
	 [ 87.94081879  57.55      ]]
Train   Epoch: 027 / 450   Loss: 3.302e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3547   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 028 / 450   Loss: 3.537e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3577   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.47325134 45.2       ]
	 [89.49974823 51.78333333]
	 [89.48014832 21.88333333]
	 [89.46756744 32.46666667]
	 [89.4914856  57.96666667]]
Train   Epoch: 029 / 450   Loss: 3.202e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3595   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 030 / 450   Loss: 3.813e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3659   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.66252899 60.33333333]
	 [90.75389099 23.08333333]
	 [90.69033813 38.93333333]
	 [90.70783997 86.41666667]
	 [90.69150543 30.5       ]]
Train   Epoch: 031 / 450   Loss: 3.751e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3624   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 032 / 450   Loss: 3.583e+04   Precision: 39.898%   Recall: 99.256%
Valid                   Loss:    3775   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.90449524 67.2       ]
	 [91.94511414 61.83333333]
	 [91.95474243 53.21666667]
	 [91.9347229  31.        ]
	 [91.94497681 58.33333333]]
Train   Epoch: 033 / 450   Loss: 3.653e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3712   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 034 / 450   Loss: 3.461e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3686   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.74560547 25.08333333]
	 [91.94832611 30.2       ]
	 [91.86103821 20.95      ]
	 [91.55345917 38.41666667]
	 [91.62622833 38.        ]]
Train   Epoch: 035 / 450   Loss: 3.345e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3736   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 036 / 450   Loss: 3.607e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3775   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.99542236 19.45      ]
	 [93.03881836 22.5       ]
	 [91.6238327  56.        ]
	 [92.57216644 31.21666667]
	 [91.94773102 24.16666667]]
Train   Epoch: 037 / 450   Loss: 3.464e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3700   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 038 / 450   Loss: 3.617e+04   Precision: 39.991%   Recall: 99.062%
Valid                   Loss:    3646   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.47860718 24.        ]
	 [92.29302216 92.66666667]
	 [92.3600235  32.45      ]
	 [90.66905212 33.        ]
	 [90.63800049 71.        ]]
Train   Epoch: 039 / 450   Loss: 3.671e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    3251   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 040 / 450   Loss: 3.382e+04   Precision: 41.265%   Recall: 94.815%
Valid                   Loss:    3923   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.73636627 29.25      ]
	 [95.42658234 36.9       ]
	 [96.60466766 28.11666667]
	 [95.55757904 40.86666667]
	 [96.31121063 28.78333333]]
Train   Epoch: 041 / 450   Loss: 3.55e+04   Precision: 40.823%   Recall: 97.522%
Valid                   Loss:    3184   Precision: 13.834%   Recall: 94.505%
Train   Epoch: 042 / 450   Loss: 3.275e+04   Precision: 43.652%   Recall: 87.842%
Valid                   Loss:    3682   Precision: 13.357%   Recall: 98.535%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.227211    16.26666667]
	 [100.5154953   26.91666667]
	 [ 92.89828491  18.45      ]
	 [ 99.83731079  63.15      ]
	 [ 87.43422699  26.8       ]]
Train   Epoch: 043 / 450   Loss: 3.354e+04   Precision: 44.557%   Recall: 88.506%
Valid                   Loss:    3304   Precision: 14.876%   Recall: 96.337%
Train   Epoch: 044 / 450   Loss: 3.368e+04   Precision: 45.254%   Recall: 85.002%
Valid                   Loss:    4209   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.26329041  37.83333333]
	 [ 84.31976318  34.33333333]
	 [102.21260834  25.6       ]
	 [101.22537231  19.23333333]
	 [ 91.40538025  37.95      ]]
Train   Epoch: 045 / 450   Loss: 3.379e+04   Precision: 44.367%   Recall: 87.943%
Valid                   Loss:    3009   Precision: 18.122%   Recall: 80.586%
Train   Epoch: 046 / 450   Loss: 3.446e+04   Precision: 46.210%   Recall: 85.264%
Valid                   Loss:    3400   Precision: 16.927%   Recall: 87.179%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[109.16962433  10.13333333]
	 [ 62.15723419  62.16666667]
	 [ 82.61231995  25.41666667]
	 [ 57.61381912  37.81666667]
	 [ 74.9031601   14.33333333]]
Train   Epoch: 047 / 450   Loss: 3.399e+04   Precision: 45.913%   Recall: 83.716%
Valid                   Loss:    3177   Precision: 16.890%   Recall: 87.912%
Train   Epoch: 048 / 450   Loss: 3.487e+04   Precision: 47.195%   Recall: 84.078%
Valid                   Loss:    2827   Precision: 20.457%   Recall: 72.161%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 79.66129303  34.75      ]
	 [111.44473267 861.4       ]
	 [113.23457336  36.56666667]
	 [103.93309784  36.16666667]
	 [ 85.93817902  74.66666667]]
Train   Epoch: 049 / 450   Loss: 3.406e+04   Precision: 48.358%   Recall: 80.206%
Valid                   Loss:    2671   Precision: 20.441%   Recall: 64.469%
Train   Epoch: 050 / 450   Loss: 3.445e+04   Precision: 47.912%   Recall: 79.784%
Valid                   Loss:    3109   Precision: 18.288%   Recall: 86.081%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.9516716   42.63333333]
	 [117.17470551  44.25      ]
	 [ 73.84591675  30.8       ]
	 [ 61.7721405   34.8       ]
	 [ 69.68347168  15.55      ]]
Train   Epoch: 051 / 450   Loss: 3.168e+04   Precision: 47.664%   Recall: 82.336%
Valid                   Loss:    3879   Precision: 15.595%   Recall: 95.971%
Train   Epoch: 052 / 450   Loss: 3.6e+04   Precision: 49.056%   Recall: 78.873%
Valid                   Loss:    2671   Precision: 22.929%   Recall: 69.963%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.85184097  83.        ]
	 [ 51.05573273  36.        ]
	 [120.98834991 132.        ]
	 [ 60.6559639   26.55      ]
	 [ 57.31991196  27.66666667]]
Train   Epoch: 053 / 450   Loss: 3.569e+04   Precision: 47.480%   Recall: 78.257%
Valid                   Loss:    2944   Precision: 20.817%   Recall: 80.220%
Train   Epoch: 054 / 450   Loss: 3.41e+04   Precision: 50.533%   Recall: 76.274%
Valid                   Loss:    2997   Precision: 20.805%   Recall: 77.656%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 74.57706451  62.48333333]
	 [ 58.57530212  74.53333333]
	 [127.73985291 456.7       ]
	 [ 74.44789124  33.11666667]
	 [ 59.13866806  40.86666667]]
Train   Epoch: 055 / 450   Loss: 3.359e+04   Precision: 50.336%   Recall: 74.754%
Valid                   Loss:    4006   Precision: 16.667%   Recall: 94.505%
Train   Epoch: 056 / 450   Loss: 3.428e+04   Precision: 50.480%   Recall: 73.568%
Valid                   Loss:    2881   Precision: 21.702%   Recall: 64.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 56.13827515  30.05      ]
	 [ 63.43202209  50.55      ]
	 [118.87622833  40.41666667]
	 [ 54.03593826  34.71666667]
	 [101.91710663  45.33333333]]
Train   Epoch: 057 / 450   Loss: 3.35e+04   Precision: 51.735%   Recall: 73.012%
Valid                   Loss:    2868   Precision: 21.011%   Recall: 68.498%
Train   Epoch: 058 / 450   Loss: 3.165e+04   Precision: 52.264%   Recall: 71.505%
Valid                   Loss:    2570   Precision: 21.408%   Recall: 54.579%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[75.26197052 56.5       ]
	 [67.10637665 45.4       ]
	 [66.59172058 44.31666667]
	 [58.33213806 93.05      ]
	 [66.70980072 36.        ]]
Train   Epoch: 059 / 450   Loss: 3.383e+04   Precision: 52.273%   Recall: 72.637%
Valid                   Loss:    2650   Precision: 22.411%   Recall: 48.352%
Train   Epoch: 060 / 450   Loss: 3.513e+04   Precision: 52.416%   Recall: 70.641%
Valid                   Loss:    2467   Precision: 24.204%   Recall: 41.758%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.94879532 226.25      ]
	 [ 54.47709274  28.88333333]
	 [ 64.12953186  54.5       ]
	 [ 68.25187683  35.53333333]
	 [ 50.73604584  30.2       ]]
Train   Epoch: 061 / 450   Loss: 3.187e+04   Precision: 50.473%   Recall: 72.483%
Valid                   Loss:    4180   Precision: 13.462%   Recall: 97.436%
Train   Epoch: 062 / 450   Loss: 3.432e+04   Precision: 49.550%   Recall: 75.216%
Valid                   Loss:    2792   Precision: 23.329%   Recall: 70.330%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 56.9053688   47.55      ]
	 [152.23086548  37.46666667]
	 [ 78.6009903   95.98333333]
	 [ 67.19435883  25.46666667]
	 [ 50.32899094  40.        ]]
Train   Epoch: 063 / 450   Loss: 3.491e+04   Precision: 52.039%   Recall: 71.204%
Valid                   Loss:    2580   Precision: 24.126%   Recall: 50.549%
Train   Epoch: 064 / 450   Loss: 3.167e+04   Precision: 53.174%   Recall: 69.462%
Valid                   Loss:    2978   Precision: 22.817%   Recall: 71.795%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.61436462 102.66666667]
	 [ 84.5851593   40.21666667]
	 [ 66.2709198   50.55      ]
	 [159.1177063   42.05      ]
	 [137.23059082  45.91666667]]
Train   Epoch: 065 / 450   Loss: 3.538e+04   Precision: 52.737%   Recall: 70.922%
Valid                   Loss:    3367   Precision: 19.380%   Recall: 91.575%
Train   Epoch: 066 / 450   Loss: 3.362e+04   Precision: 52.856%   Recall: 71.773%
Valid                   Loss:    3691   Precision: 19.040%   Recall: 87.179%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[160.86914062  21.86666667]
	 [ 83.13162231  15.95      ]
	 [ 95.04071808  33.03333333]
	 [126.83802032  10.61666667]
	 [ 60.15929794  20.11666667]]
Train   Epoch: 067 / 450   Loss: 3.511e+04   Precision: 53.301%   Recall: 71.338%
Valid                   Loss:    2665   Precision: 23.462%   Recall: 60.073%
Train   Epoch: 068 / 450   Loss: 3.385e+04   Precision: 53.360%   Recall: 72.450%
Valid                   Loss:    2697   Precision: 23.547%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[72.99511719 45.73333333]
	 [54.05953979 53.25      ]
	 [95.83277893 80.5       ]
	 [54.14288712 25.5       ]
	 [67.43294525 59.23333333]]
Train   Epoch: 069 / 450   Loss: 3.317e+04   Precision: 52.967%   Recall: 68.330%
Valid                   Loss:    2557   Precision: 27.711%   Recall: 42.125%
Train   Epoch: 070 / 450   Loss: 3.401e+04   Precision: 53.381%   Recall: 70.701%
Valid                   Loss:    3387   Precision: 21.594%   Recall: 85.348%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.23116302  62.48333333]
	 [100.79307556 154.        ]
	 [ 52.66309357  34.66666667]
	 [ 54.15577316  27.        ]
	 [ 54.92018509  24.        ]]
Train   Epoch: 071 / 450   Loss: 3.199e+04   Precision: 52.483%   Recall: 68.873%
Valid                   Loss:    3136   Precision: 21.788%   Recall: 71.429%
Train   Epoch: 072 / 450   Loss: 3.317e+04   Precision: 53.909%   Recall: 69.877%
Valid                   Loss:    2707   Precision: 25.449%   Recall: 62.271%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.62078857 38.06666667]
	 [50.73000717 35.25      ]
	 [54.83469009 33.93333333]
	 [75.76995087 63.33333333]
	 [62.9304924  54.        ]]
Train   Epoch: 073 / 450   Loss: 3.311e+04   Precision: 53.914%   Recall: 68.042%
Valid                   Loss:    3996   Precision: 17.896%   Recall: 89.744%
Train   Epoch: 074 / 450   Loss: 3.302e+04   Precision: 53.336%   Recall: 70.681%
Valid                   Loss:    3379   Precision: 23.501%   Recall: 70.330%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[188.91784668 207.96666667]
	 [ 56.47800827  51.        ]
	 [ 61.87931061  20.45      ]
	 [ 71.65244293  40.5       ]
	 [ 53.60577011  24.33333333]]
Train   Epoch: 075 / 450   Loss: 3.278e+04   Precision: 54.730%   Recall: 69.522%
Valid                   Loss:    3454   Precision: 20.282%   Recall: 84.249%
Train   Epoch: 076 / 450   Loss: 3.261e+04   Precision: 53.146%   Recall: 69.938%
Valid                   Loss:    3291   Precision: 23.155%   Recall: 66.667%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 58.56676865  36.21666667]
	 [121.62989044  90.88333333]
	 [198.05415344   5.45      ]
	 [ 75.95072174  12.5       ]
	 [182.16273499  22.83333333]]
Train   Epoch: 077 / 450   Loss: 3.4e+04   Precision: 54.730%   Recall: 70.259%
Valid                   Loss:    2653   Precision: 28.411%   Recall: 55.678%
Train   Epoch: 078 / 450   Loss: 3.417e+04   Precision: 54.736%   Recall: 69.127%
Valid                   Loss:    2913   Precision: 25.000%   Recall: 66.667%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.15324402  36.11666667]
	 [ 50.9520874   11.        ]
	 [ 39.47556686  33.71666667]
	 [ 58.36779022  42.83333333]
	 [160.90348816  78.58333333]]
Train   Epoch: 079 / 450   Loss: 3.577e+04   Precision: 54.717%   Recall: 68.571%
Valid                   Loss:    3099   Precision: 22.886%   Recall: 67.399%
Train   Epoch: 080 / 450   Loss: 3.304e+04   Precision: 54.954%   Recall: 69.033%
Valid                   Loss:    4421   Precision: 17.124%   Recall: 93.773%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[82.24788666 33.43333333]
	 [64.05782318 20.38333333]
	 [56.39468384 22.33333333]
	 [92.79645538 12.66666667]
	 [83.36140442 19.        ]]
Train   Epoch: 081 / 450   Loss: 3.191e+04   Precision: 54.029%   Recall: 69.529%
Valid                   Loss:    2525   Precision: 35.110%   Recall: 41.026%
Train   Epoch: 082 / 450   Loss: 3.214e+04   Precision: 55.124%   Recall: 68.565%
Valid                   Loss:    3436   Precision: 20.982%   Recall: 79.853%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.71432877 35.51666667]
	 [71.61481476 16.83333333]
	 [55.7701416  18.5       ]
	 [69.35826874 27.88333333]
	 [61.12537003 48.51666667]]
Train   Epoch: 083 / 450   Loss: 3.317e+04   Precision: 55.757%   Recall: 68.765%
Valid                   Loss:    2341   Precision: 35.119%   Recall: 43.223%
Train   Epoch: 084 / 450   Loss: 3.162e+04   Precision: 54.910%   Recall: 67.834%
Valid                   Loss:    2907   Precision: 24.278%   Recall: 67.766%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.63311768  34.15      ]
	 [108.21026611  64.33333333]
	 [ 67.72180939  41.33333333]
	 [ 74.47233582  19.51666667]
	 [ 62.30328751  31.28333333]]
Train   Epoch: 085 / 450   Loss: 3.345e+04   Precision: 55.972%   Recall: 69.214%
Valid                   Loss:    2738   Precision: 26.654%   Recall: 53.114%
Train   Epoch: 086 / 450   Loss: 3.164e+04   Precision: 56.460%   Recall: 69.663%
Valid                   Loss:    2677   Precision: 24.719%   Recall: 64.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[81.88635254 36.61666667]
	 [40.39021683 13.96666667]
	 [37.08328247 25.25      ]
	 [57.32838821 53.        ]
	 [60.67684555 40.25      ]]
Train   Epoch: 087 / 450   Loss: 3.174e+04   Precision: 56.680%   Recall: 69.422%
Valid                   Loss:    2492   Precision: 27.051%   Recall: 56.777%
Train   Epoch: 088 / 450   Loss: 3.077e+04   Precision: 55.746%   Recall: 68.297%
Valid                   Loss:    2630   Precision: 27.528%   Recall: 53.846%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.40756607 69.33333333]
	 [48.76375961 12.5       ]
	 [95.06988525 28.86666667]
	 [51.07567596 47.7       ]
	 [90.11403656 28.01666667]]
Train   Epoch: 089 / 450   Loss: 3.274e+04   Precision: 56.967%   Recall: 67.399%
Valid                   Loss:    2780   Precision: 25.156%   Recall: 58.974%
Train   Epoch: 090 / 450   Loss: 3.326e+04   Precision: 55.907%   Recall: 68.946%
Valid                   Loss:    2858   Precision: 26.813%   Recall: 58.242%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.99858093  35.28333333]
	 [220.85783386  33.2       ]
	 [ 44.45623016  29.13333333]
	 [ 82.66429138  69.75      ]
	 [ 40.70107651  35.        ]]
Train   Epoch: 091 / 450   Loss: 3.236e+04   Precision: 56.445%   Recall: 66.234%
Valid                   Loss:    2934   Precision: 24.932%   Recall: 66.667%
Train   Epoch: 092 / 450   Loss: 3.372e+04   Precision: 56.725%   Recall: 67.232%
Valid                   Loss:    3524   Precision: 21.907%   Recall: 70.696%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 64.16164398  41.25      ]
	 [232.18389893  23.16666667]
	 [ 58.63964844  47.93333333]
	 [ 96.13784027 198.1       ]
	 [ 50.59975433  26.86666667]]
Train   Epoch: 093 / 450   Loss: 3.641e+04   Precision: 56.105%   Recall: 66.729%
Valid                   Loss:    2793   Precision: 24.411%   Recall: 64.469%
Train   Epoch: 094 / 450   Loss: 3.363e+04   Precision: 56.766%   Recall: 68.819%
Valid                   Loss:    2845   Precision: 26.814%   Recall: 62.271%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[130.20550537  14.5       ]
	 [ 47.37551117  14.21666667]
	 [ 33.217453    23.08333333]
	 [ 46.31144714  37.28333333]
	 [ 38.77593231  45.33333333]]
Train   Epoch: 095 / 450   Loss: 3.381e+04   Precision: 57.330%   Recall: 67.506%
Valid                   Loss:    2897   Precision: 26.557%   Recall: 59.341%
Train   Epoch: 096 / 450   Loss: 2.817e+04   Precision: 56.294%   Recall: 68.116%
Valid                   Loss:    2486   Precision: 29.723%   Recall: 43.223%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.26402283 54.33333333]
	 [45.03925323 16.38333333]
	 [41.36203003 42.5       ]
	 [66.08071899 29.75      ]
	 [48.30885696 83.86666667]]
Train   Epoch: 097 / 450   Loss: 3.214e+04   Precision: 57.825%   Recall: 67.392%
Valid                   Loss:    3044   Precision: 25.316%   Recall: 65.934%
Train   Epoch: 098 / 450   Loss: 3.238e+04   Precision: 58.362%   Recall: 66.830%
Valid                   Loss:    4011   Precision: 19.142%   Recall: 84.982%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[199.76986694 110.5       ]
	 [ 47.95916748  10.45      ]
	 [ 62.25572586  28.7       ]
	 [ 81.39333344  71.95      ]
	 [ 74.47106934  26.83333333]]
Train   Epoch: 099 / 450   Loss: 3.545e+04   Precision: 55.370%   Recall: 66.093%
Valid                   Loss:    2814   Precision: 28.274%   Recall: 49.817%
Train   Epoch: 100 / 450   Loss: 3.345e+04   Precision: 57.157%   Recall: 68.049%
Valid                   Loss:    2728   Precision: 32.915%   Recall: 38.462%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[73.66412354 28.88333333]
	 [33.44030762 18.53333333]
	 [34.91085434 40.        ]
	 [73.28404236 15.65      ]
	 [66.20982361 37.31666667]]
Train   Epoch: 101 / 450   Loss: 3.27e+04   Precision: 57.754%   Recall: 66.207%
Valid                   Loss:    2422   Precision: 32.161%   Recall: 46.886%
Train   Epoch: 102 / 450   Loss: 3.081e+04   Precision: 56.995%   Recall: 67.674%
Valid                   Loss:    2735   Precision: 26.855%   Recall: 55.678%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 70.53960419  20.11666667]
	 [130.01905823  14.73333333]
	 [ 72.52004242  26.33333333]
	 [193.74481201  60.        ]
	 [ 46.38338089  34.45      ]]
Train   Epoch: 103 / 450   Loss: 3.052e+04   Precision: 58.434%   Recall: 68.404%
Valid                   Loss:    2743   Precision: 28.455%   Recall: 51.282%
Train   Epoch: 104 / 450   Loss: 3.189e+04   Precision: 58.726%   Recall: 66.267%
Valid                   Loss:    3183   Precision: 24.138%   Recall: 69.231%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.45618057  67.33333333]
	 [ 53.11399841  29.38333333]
	 [182.31245422 144.55      ]
	 [160.77252197 515.3       ]
	 [ 86.8181839   35.91666667]]
Train   Epoch: 105 / 450   Loss: 3.333e+04   Precision: 59.094%   Recall: 66.053%
Valid                   Loss:    2529   Precision: 27.565%   Recall: 50.183%
Train   Epoch: 106 / 450   Loss: 3.177e+04   Precision: 58.615%   Recall: 68.270%
Valid                   Loss:    2742   Precision: 28.466%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.92427826  51.78333333]
	 [ 60.63731384  67.66666667]
	 [ 52.7338829   36.95      ]
	 [190.55397034  33.9       ]
	 [ 40.80443954  25.91666667]]
Train   Epoch: 107 / 450   Loss: 3.299e+04   Precision: 58.798%   Recall: 66.140%
Valid                   Loss:    2429   Precision: 34.035%   Recall: 35.531%
Train   Epoch: 108 / 450   Loss: 3.094e+04   Precision: 57.651%   Recall: 68.062%
Valid                   Loss:    2630   Precision: 26.655%   Recall: 56.044%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[224.92610168 193.75      ]
	 [ 41.84284973  18.16666667]
	 [ 69.09310913  46.33333333]
	 [ 52.15983582  64.7       ]
	 [ 64.81550598  22.16666667]]
Train   Epoch: 109 / 450   Loss: 2.912e+04   Precision: 58.875%   Recall: 66.207%
Valid                   Loss:    2444   Precision: 28.355%   Recall: 47.985%
Train   Epoch: 110 / 450   Loss: 3.447e+04   Precision: 58.263%   Recall: 67.044%
Valid                   Loss:    2535   Precision: 29.565%   Recall: 49.817%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.75467682  32.78333333]
	 [ 42.29197693  20.6       ]
	 [ 73.054039    32.2       ]
	 [ 75.01152802  25.63333333]
	 [ 64.21570587 130.03333333]]
Train   Epoch: 111 / 450   Loss: 3.165e+04   Precision: 58.374%   Recall: 66.582%
Valid                   Loss:    2652   Precision: 30.184%   Recall: 47.985%
Train   Epoch: 112 / 450   Loss: 3.126e+04   Precision: 59.384%   Recall: 66.167%
Valid                   Loss:    2838   Precision: 24.084%   Recall: 50.549%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.28622437 43.93333333]
	 [56.54121399 33.71666667]
	 [48.78468323 25.78333333]
	 [39.39161682 22.41666667]
	 [62.66248703 80.05      ]]
Train   Epoch: 113 / 450   Loss: 3.041e+04   Precision: 58.479%   Recall: 66.990%
Valid                   Loss:    3460   Precision: 20.753%   Recall: 78.755%
Train   Epoch: 114 / 450   Loss: 3.115e+04   Precision: 57.976%   Recall: 66.682%
Valid                   Loss:    3089   Precision: 24.601%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.24229431  50.31666667]
	 [ 45.72728348  33.        ]
	 [116.73216248 100.8       ]
	 [ 76.02121735  15.63333333]
	 [245.22419739 174.88333333]]
Train   Epoch: 115 / 450   Loss: 3.175e+04   Precision: 58.995%   Recall: 66.033%
Valid                   Loss:    3356   Precision: 24.597%   Recall: 67.033%
Train   Epoch: 116 / 450   Loss: 2.948e+04   Precision: 58.898%   Recall: 65.423%
Valid                   Loss:    3134   Precision: 25.183%   Recall: 63.004%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[177.28570557  72.01666667]
	 [ 93.93115997  49.86666667]
	 [ 37.23810196  20.58333333]
	 [100.29911041  49.66666667]
	 [ 38.01673126  40.5       ]]
Train   Epoch: 117 / 450   Loss: 2.971e+04   Precision: 59.376%   Recall: 66.408%
Valid                   Loss:    3886   Precision: 21.253%   Recall: 75.824%
Train   Epoch: 118 / 450   Loss: 3.053e+04   Precision: 57.296%   Recall: 66.488%
Valid                   Loss:    2811   Precision: 28.381%   Recall: 54.579%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.90894318  21.75      ]
	 [ 47.34803391  27.88333333]
	 [ 52.4490509   47.3       ]
	 [107.35646057  29.95      ]
	 [104.13072968  24.16666667]]
Train   Epoch: 119 / 450   Loss: 2.877e+04   Precision: 59.017%   Recall: 67.211%
Valid                   Loss:    3265   Precision: 25.331%   Recall: 63.004%
Train   Epoch: 120 / 450   Loss: 3.148e+04   Precision: 59.783%   Recall: 66.026%
Valid                   Loss:    2746   Precision: 25.263%   Recall: 52.747%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[116.9575119   72.        ]
	 [ 46.68082428  36.2       ]
	 [ 70.87669373  26.13333333]
	 [ 43.95112228  19.83333333]
	 [ 54.99929047  32.11666667]]
Train   Epoch: 121 / 450   Loss: 2.981e+04   Precision: 58.878%   Recall: 67.788%
Valid                   Loss:    2515   Precision: 30.652%   Recall: 51.648%
Train   Epoch: 122 / 450   Loss: 3.11e+04   Precision: 58.056%   Recall: 66.401%
Valid                   Loss:    3539   Precision: 26.389%   Recall: 62.637%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 37.35800934  34.55      ]
	 [180.88722229  58.26666667]
	 [ 80.77975464  22.5       ]
	 [ 45.24131012  75.38333333]
	 [122.65860748  55.75      ]]
Train   Epoch: 123 / 450   Loss: 3.172e+04   Precision: 57.696%   Recall: 66.388%
Valid                   Loss:    2888   Precision: 26.667%   Recall: 61.538%
Train   Epoch: 124 / 450   Loss: 3.305e+04   Precision: 59.964%   Recall: 66.917%
Valid                   Loss:    2890   Precision: 22.942%   Recall: 62.271%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.70246124  55.55      ]
	 [ 77.83779907 127.53333333]
	 [ 91.83226776  32.83333333]
	 [ 55.85698318  10.        ]
	 [ 54.62160873  17.66666667]]
Train   Epoch: 125 / 450   Loss: 2.87e+04   Precision: 58.283%   Recall: 65.704%
Valid                   Loss:    3142   Precision: 23.584%   Recall: 65.568%
Train   Epoch: 126 / 450   Loss: 3.149e+04   Precision: 58.604%   Recall: 65.584%
Valid                   Loss:    3648   Precision: 23.261%   Recall: 71.062%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.00598907 112.58333333]
	 [ 87.39583588  50.95      ]
	 [106.12056732  29.88333333]
	 [ 78.47775269  18.06666667]
	 [ 90.7745285   54.46666667]]
Train   Epoch: 127 / 450   Loss: 3.029e+04   Precision: 58.638%   Recall: 66.595%
Valid                   Loss:    2678   Precision: 28.511%   Recall: 49.084%
Train   Epoch: 128 / 450   Loss: 3.049e+04   Precision: 60.371%   Recall: 63.869%
Valid                   Loss:    2452   Precision: 29.679%   Recall: 40.659%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.34150696 132.93333333]
	 [192.53648376  85.55      ]
	 [ 55.00074005  16.21666667]
	 [ 44.52047348  24.16666667]
	 [ 35.0463829   17.        ]]
Train   Epoch: 129 / 450   Loss: 3.049e+04   Precision: 59.455%   Recall: 66.026%
Valid                   Loss:    3306   Precision: 24.638%   Recall: 68.498%
Train   Epoch: 130 / 450   Loss: 3.108e+04   Precision: 59.080%   Recall: 67.533%
Valid                   Loss:    2950   Precision: 29.435%   Recall: 53.480%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.86207962 22.        ]
	 [90.91845703 15.66666667]
	 [35.0746727  37.55      ]
	 [69.71652222 60.        ]
	 [73.13404846 28.68333333]]
Train   Epoch: 131 / 450   Loss: 3.204e+04   Precision: 59.809%   Recall: 66.408%
Valid                   Loss:    3027   Precision: 24.684%   Recall: 64.469%
Train   Epoch: 132 / 450   Loss: 3.051e+04   Precision: 58.699%   Recall: 65.269%
Valid                   Loss:    3851   Precision: 23.973%   Recall: 64.103%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 58.5483284   24.25      ]
	 [ 40.65172195 101.21666667]
	 [104.07765198  19.95      ]
	 [126.98963165  43.66666667]
	 [ 64.64555359  38.55      ]]
Train   Epoch: 133 / 450   Loss: 2.928e+04   Precision: 59.857%   Recall: 66.260%
Valid                   Loss:    2739   Precision: 27.890%   Recall: 55.678%
Train   Epoch: 134 / 450   Loss: 3.241e+04   Precision: 60.196%   Recall: 64.954%
Valid                   Loss:    2764   Precision: 28.519%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.02622604  41.75      ]
	 [ 49.22218323 466.78333333]
	 [ 73.74239349  45.8       ]
	 [131.80444336 185.11666667]
	 [ 51.57865143  34.91666667]]
Train   Epoch: 135 / 450   Loss: 3.058e+04   Precision: 60.780%   Recall: 65.316%
Valid                   Loss:    2814   Precision: 29.759%   Recall: 40.659%
Train   Epoch: 136 / 450   Loss: 2.905e+04   Precision: 59.172%   Recall: 66.421%
Valid                   Loss:    2385   Precision: 31.613%   Recall: 35.897%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.30295563 31.21666667]
	 [45.66656876 52.38333333]
	 [42.06190872 46.        ]
	 [44.41723633 34.05      ]
	 [74.12596893 16.38333333]]
Train   Epoch: 137 / 450   Loss: 3.31e+04   Precision: 58.300%   Recall: 67.379%
Valid                   Loss:    3268   Precision: 25.664%   Recall: 63.736%
Train   Epoch: 138 / 450   Loss: 3.192e+04   Precision: 60.011%   Recall: 63.822%
Valid                   Loss:    2946   Precision: 31.090%   Recall: 35.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[105.40258026  62.66666667]
	 [ 46.62791824  21.66666667]
	 [ 44.62103271  79.        ]
	 [ 93.26531219  44.96666667]
	 [ 36.28659439  77.33333333]]
Train   Epoch: 139 / 450   Loss: 3.091e+04   Precision: 60.942%   Recall: 65.550%
Valid                   Loss:    2886   Precision: 29.038%   Recall: 58.608%
Train   Epoch: 140 / 450   Loss: 2.843e+04   Precision: 60.899%   Recall: 64.713%
Valid                   Loss:    2941   Precision: 27.899%   Recall: 60.806%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.06271362  19.25      ]
	 [101.95051575  45.        ]
	 [ 46.98575211  30.38333333]
	 [118.7339859   58.88333333]
	 [ 42.55995941  29.13333333]]
Train   Epoch: 141 / 450   Loss: 2.984e+04   Precision: 60.665%   Recall: 64.432%
Valid                   Loss:    3536   Precision: 27.592%   Recall: 57.509%
Train   Epoch: 142 / 450   Loss: 2.869e+04   Precision: 60.875%   Recall: 63.554%
Valid                   Loss:    2816   Precision: 29.484%   Recall: 43.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.74860764  36.        ]
	 [ 92.59611511  80.33333333]
	 [ 69.62499237  39.88333333]
	 [ 88.19483948 170.53333333]
	 [ 37.80941772  41.16666667]]
Train   Epoch: 143 / 450   Loss: 2.961e+04   Precision: 60.988%   Recall: 63.762%
Valid                   Loss:    2604   Precision: 34.498%   Recall: 28.938%
Train   Epoch: 144 / 450   Loss:   3e+04   Precision: 59.798%   Recall: 64.492%
Valid                   Loss:    2886   Precision: 28.049%   Recall: 50.549%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 70.29860687  23.85      ]
	 [ 70.6295929   37.81666667]
	 [ 48.47669983  36.        ]
	 [ 61.8326683   33.        ]
	 [ 92.2071991  150.35      ]]
Train   Epoch: 145 / 450   Loss: 2.956e+04   Precision: 62.403%   Recall: 63.306%
Valid                   Loss:    3289   Precision: 26.941%   Recall: 62.271%
Train   Epoch: 146 / 450   Loss: 2.93e+04   Precision: 61.451%   Recall: 65.657%
Valid                   Loss:    2805   Precision: 27.778%   Recall: 43.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.34988022  27.96666667]
	 [ 50.45093536  76.86666667]
	 [ 54.88396072  29.71666667]
	 [ 75.2378006   18.03333333]
	 [ 90.44076538 255.83333333]]
Train   Epoch: 147 / 450   Loss: 2.872e+04   Precision: 61.611%   Recall: 63.943%
Valid                   Loss:    2713   Precision: 32.915%   Recall: 47.985%
Train   Epoch: 148 / 450   Loss: 2.901e+04   Precision: 60.871%   Recall: 63.313%
Valid                   Loss:    2901   Precision: 27.349%   Recall: 47.985%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.89683533  18.55      ]
	 [ 59.88643265  28.        ]
	 [146.73370361  37.        ]
	 [ 56.64484406  33.76666667]
	 [ 54.54850388  49.75      ]]
Train   Epoch: 149 / 450   Loss: 2.921e+04   Precision: 58.759%   Recall: 63.943%
Valid                   Loss:    3102   Precision: 24.588%   Recall: 65.568%
Train   Epoch: 150 / 450   Loss: 2.791e+04   Precision: 61.800%   Recall: 63.092%
Valid                   Loss:    2820   Precision: 29.356%   Recall: 45.055%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[14.90875053 14.78333333]
	 [39.50428009 17.5       ]
	 [61.26823807 61.55      ]
	 [76.1355896  73.        ]
	 [82.03105927 17.43333333]]
Train   Epoch: 151 / 450   Loss: 2.781e+04   Precision: 61.915%   Recall: 63.280%
Valid                   Loss:    3010   Precision: 28.427%   Recall: 51.648%
Train   Epoch: 152 / 450   Loss: 2.985e+04   Precision: 62.337%   Recall: 65.423%
Valid                   Loss:    3048   Precision: 29.379%   Recall: 38.095%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[69.4210968  41.21666667]
	 [21.78744698 17.55      ]
	 [40.86776352 95.5       ]
	 [40.84438705 23.05      ]
	 [81.29795837 30.36666667]]
Train   Epoch: 153 / 450   Loss: 2.904e+04   Precision: 61.558%   Recall: 64.197%
Valid                   Loss:    3066   Precision: 27.757%   Recall: 55.311%
Train   Epoch: 154 / 450   Loss: 2.899e+04   Precision: 61.048%   Recall: 64.773%
Valid                   Loss:    3400   Precision: 28.032%   Recall: 51.648%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[104.93128967  22.45      ]
	 [120.71045685  34.95      ]
	 [ 47.15575027  24.38333333]
	 [ 42.8264122   18.73333333]
	 [110.07978058  32.        ]]
Train   Epoch: 155 / 450   Loss: 2.99e+04   Precision: 61.806%   Recall: 63.789%
Valid                   Loss:    3539   Precision: 25.906%   Recall: 52.381%
Train   Epoch: 156 / 450   Loss: 2.892e+04   Precision: 62.872%   Recall: 63.146%
Valid                   Loss:    4095   Precision: 20.058%   Recall: 75.458%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.78311157  36.5       ]
	 [ 68.23360443  45.55      ]
	 [133.08518982  34.05      ]
	 [ 56.26272583  54.63333333]
	 [ 81.75619507  19.5       ]]
Train   Epoch: 157 / 450   Loss: 3.173e+04   Precision: 59.728%   Recall: 65.805%
Valid                   Loss:    3735   Precision: 25.280%   Recall: 57.875%
Train   Epoch: 158 / 450   Loss: 2.798e+04   Precision: 61.985%   Recall: 64.713%
Valid                   Loss:    2530   Precision: 31.195%   Recall: 39.194%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.91751099 27.33333333]
	 [63.51923752 47.96666667]
	 [39.98007965 38.33333333]
	 [41.50793839 47.38333333]
	 [38.51057816 70.53333333]]
Train   Epoch: 159 / 450   Loss: 2.899e+04   Precision: 61.586%   Recall: 64.090%
Valid                   Loss:    3418   Precision: 26.990%   Recall: 57.143%
Train   Epoch: 160 / 450   Loss: 2.749e+04   Precision: 61.953%   Recall: 63.949%
Valid                   Loss:    3741   Precision: 25.694%   Recall: 67.766%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.69330215  69.16666667]
	 [ 48.61719513  20.        ]
	 [194.94137573 180.53333333]
	 [ 77.07736206  15.41666667]
	 [239.21208191  97.5       ]]
Train   Epoch: 161 / 450   Loss: 2.883e+04   Precision: 62.921%   Recall: 66.247%
Valid                   Loss:    2922   Precision: 26.221%   Recall: 53.114%
Train   Epoch: 162 / 450   Loss: 2.663e+04   Precision: 63.246%   Recall: 64.653%
Valid                   Loss:    3057   Precision: 28.178%   Recall: 48.718%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.78966522 34.28333333]
	 [58.42119598 33.96666667]
	 [44.79285431 44.88333333]
	 [86.48136139 32.11666667]
	 [59.46502686 38.08333333]]
Train   Epoch: 163 / 450   Loss: 2.756e+04   Precision: 63.043%   Recall: 64.170%
Valid                   Loss:    2809   Precision: 30.698%   Recall: 48.352%
Train   Epoch: 164 / 450   Loss: 3.109e+04   Precision: 63.855%   Recall: 63.628%
Valid                   Loss:    2881   Precision: 29.815%   Recall: 58.974%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.877388   29.38333333]
	 [62.58576584 82.08333333]
	 [45.92290115 32.        ]
	 [55.36008453 17.33333333]
	 [74.49490356 70.        ]]
Train   Epoch: 165 / 450   Loss: 2.766e+04   Precision: 63.830%   Recall: 65.122%
Valid                   Loss:    2828   Precision: 31.195%   Recall: 51.648%
Train   Epoch: 166 / 450   Loss: 2.737e+04   Precision: 62.991%   Recall: 63.058%
Valid                   Loss:    2541   Precision: 30.303%   Recall: 43.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[108.3132019   17.41666667]
	 [ 43.54994965  30.58333333]
	 [ 48.49409866  19.        ]
	 [ 67.61779785  48.11666667]
	 [ 92.3263855   46.55      ]]
Train   Epoch: 167 / 450   Loss: 2.545e+04   Precision: 63.661%   Recall: 64.927%
Valid                   Loss:    2694   Precision: 34.203%   Recall: 43.223%
Train   Epoch: 168 / 450   Loss: 2.907e+04   Precision: 62.768%   Recall: 63.259%
Valid                   Loss:    4232   Precision: 20.515%   Recall: 75.824%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.80884552 38.93333333]
	 [80.5605545  35.81666667]
	 [58.55531311 25.66666667]
	 [63.30242538 22.78333333]
	 [50.88909531 15.61666667]]
Train   Epoch: 169 / 450   Loss: 2.786e+04   Precision: 62.737%   Recall: 63.923%
Valid                   Loss:    3218   Precision: 27.838%   Recall: 65.568%
Train   Epoch: 170 / 450   Loss: 2.991e+04   Precision: 63.413%   Recall: 62.891%
Valid                   Loss:    3176   Precision: 27.372%   Recall: 64.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.02615738 38.98333333]
	 [66.80590057 50.05      ]
	 [80.89574432 41.55      ]
	 [54.19900894 26.28333333]
	 [86.41928101 31.45      ]]
Train   Epoch: 171 / 450   Loss: 3.174e+04   Precision: 63.233%   Recall: 65.249%
Valid                   Loss:    3071   Precision: 30.126%   Recall: 52.747%
Train   Epoch: 172 / 450   Loss: 2.84e+04   Precision: 64.132%   Recall: 62.147%
Valid                   Loss:    2701   Precision: 31.771%   Recall: 44.689%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.14582443 48.75      ]
	 [48.3246994  63.        ]
	 [57.8680954  22.78333333]
	 [32.12428284 20.36666667]
	 [42.49599838 46.11666667]]
Train   Epoch: 173 / 450   Loss: 2.694e+04   Precision: 61.617%   Recall: 65.550%
Valid                   Loss:    2784   Precision: 31.395%   Recall: 39.560%
Train   Epoch: 174 / 450   Loss: 2.683e+04   Precision: 63.970%   Recall: 63.293%
Valid                   Loss:    3137   Precision: 31.635%   Recall: 43.223%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[65.86769867 98.45      ]
	 [50.63526535 50.15      ]
	 [60.58261871 55.95      ]
	 [39.96545029 45.91666667]
	 [53.30444336 45.36666667]]
Train   Epoch: 175 / 450   Loss: 2.869e+04   Precision: 63.399%   Recall: 64.023%
Valid                   Loss:    2557   Precision: 34.085%   Recall: 49.817%
Train   Epoch: 176 / 450   Loss: 2.877e+04   Precision: 63.554%   Recall: 62.817%
Valid                   Loss:    3237   Precision: 28.678%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.21478653  34.78333333]
	 [ 56.16227722  28.7       ]
	 [173.008255    60.63333333]
	 [ 49.33334732  33.25      ]
	 [ 92.82628632  17.16666667]]
Train   Epoch: 177 / 450   Loss: 2.695e+04   Precision: 64.342%   Recall: 63.769%
Valid                   Loss:    3622   Precision: 28.442%   Recall: 68.864%
Train   Epoch: 178 / 450   Loss: 2.689e+04   Precision: 59.415%   Recall: 63.427%
Valid                   Loss:    2837   Precision: 28.321%   Recall: 41.392%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[154.20419312  23.95      ]
	 [ 31.1192646   22.        ]
	 [ 43.84682083  27.        ]
	 [ 62.62566757  93.61666667]
	 [ 41.30773544  18.16666667]]
Train   Epoch: 179 / 450   Loss: 2.785e+04   Precision: 63.501%   Recall: 63.058%
Valid                   Loss:    3191   Precision: 27.372%   Recall: 64.469%
Train   Epoch: 180 / 450   Loss: 2.758e+04   Precision: 64.851%   Recall: 63.882%
Valid                   Loss:    2567   Precision: 32.850%   Recall: 49.817%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[84.48593903 73.21666667]
	 [64.58451843 37.56666667]
	 [35.29264069 32.        ]
	 [55.45254135 36.36666667]
	 [25.19793701 36.58333333]]
Train   Epoch: 181 / 450   Loss: 2.72e+04   Precision: 63.540%   Recall: 65.021%
Valid                   Loss:    2854   Precision: 29.508%   Recall: 46.154%
Train   Epoch: 182 / 450   Loss: 2.622e+04   Precision: 63.521%   Recall: 62.811%
Valid                   Loss:    3025   Precision: 33.224%   Recall: 36.996%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.86941147 44.83333333]
	 [38.53482437 16.        ]
	 [57.89509583 58.15      ]
	 [38.09736252 29.16666667]
	 [59.79652786 34.        ]]
Train   Epoch: 183 / 450   Loss: 2.903e+04   Precision: 63.776%   Recall: 62.362%
Valid                   Loss:    2624   Precision: 32.220%   Recall: 49.451%
Train   Epoch: 184 / 450   Loss: 2.794e+04   Precision: 64.908%   Recall: 65.168%
Valid                   Loss:    3176   Precision: 30.000%   Recall: 50.549%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[125.692276    41.16666667]
	 [ 71.43008423  98.8       ]
	 [ 71.05623627  33.03333333]
	 [ 51.22529221  24.78333333]
	 [ 40.25747299  14.        ]]
Train   Epoch: 185 / 450   Loss: 2.654e+04   Precision: 65.041%   Recall: 62.248%
Valid                   Loss:    2976   Precision: 29.926%   Recall: 58.974%
Train   Epoch: 186 / 450   Loss: 2.697e+04   Precision: 62.399%   Recall: 64.673%
Valid                   Loss:    3486   Precision: 33.041%   Recall: 41.392%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.09405518 32.58333333]
	 [29.04051208 31.93333333]
	 [44.36401367 58.9       ]
	 [63.64605331 48.33333333]
	 [82.32931519 55.26666667]]
Train   Epoch: 187 / 450   Loss: 2.582e+04   Precision: 64.240%   Recall: 61.779%
Valid                   Loss:    2864   Precision: 29.857%   Recall: 53.480%
Train   Epoch: 188 / 450   Loss: 2.897e+04   Precision: 53.596%   Recall: 70.226%
Valid                   Loss:    4009   Precision: 22.674%   Recall: 77.656%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.2657814  27.33333333]
	 [95.82089996 12.83333333]
	 [39.09472275 34.08333333]
	 [73.52826691 35.11666667]
	 [54.36877441 46.83333333]]
Train   Epoch: 189 / 450   Loss: 2.655e+04   Precision: 61.315%   Recall: 61.237%
Valid                   Loss:    2424   Precision: 35.401%   Recall: 35.531%
Train   Epoch: 190 / 450   Loss: 2.599e+04   Precision: 65.164%   Recall: 63.728%
Valid                   Loss:    3145   Precision: 33.140%   Recall: 41.758%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.8699379  28.58333333]
	 [72.34052277 20.08333333]
	 [90.41819    41.26666667]
	 [42.8390274  44.46666667]
	 [66.06391907 57.05      ]]
Train   Epoch: 191 / 450   Loss: 2.684e+04   Precision: 65.779%   Recall: 63.373%
Valid                   Loss:    2449   Precision: 32.603%   Recall: 49.084%
Train   Epoch: 192 / 450   Loss: 2.416e+04   Precision: 65.253%   Recall: 65.048%
Valid                   Loss:    2735   Precision: 37.829%   Recall: 42.125%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[71.03826141 38.45      ]
	 [49.33527374 26.5       ]
	 [82.52906036 90.3       ]
	 [51.88550949 26.43333333]
	 [48.64069366 29.9       ]]
Train   Epoch: 193 / 450   Loss: 2.644e+04   Precision: 63.237%   Recall: 62.000%
Valid                   Loss:    3011   Precision: 30.128%   Recall: 51.648%
Train   Epoch: 194 / 450   Loss: 2.69e+04   Precision: 66.006%   Recall: 63.574%
Valid                   Loss:    2611   Precision: 31.955%   Recall: 31.136%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.01134491 57.33333333]
	 [53.90281296 10.25      ]
	 [41.46112442 47.        ]
	 [59.60188293 40.8       ]
	 [43.0772438  50.45      ]]
Train   Epoch: 195 / 450   Loss: 2.351e+04   Precision: 64.436%   Recall: 64.746%
Valid                   Loss:    3747   Precision: 25.692%   Recall: 71.429%
Train   Epoch: 196 / 450   Loss: 2.631e+04   Precision: 65.336%   Recall: 63.896%
Valid                   Loss:    2702   Precision: 32.283%   Recall: 45.055%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[73.94542694 73.86666667]
	 [53.5014534  18.95      ]
	 [32.58291245 27.66666667]
	 [82.39749146 32.96666667]
	 [46.38262177 54.08333333]]
Train   Epoch: 197 / 450   Loss: 2.516e+04   Precision: 64.953%   Recall: 63.547%
Valid                   Loss:    2955   Precision: 32.778%   Recall: 43.223%
Train   Epoch: 198 / 450   Loss: 2.58e+04   Precision: 66.328%   Recall: 63.018%
Valid                   Loss:    2642   Precision: 38.078%   Recall: 39.194%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.7090683  18.63333333]
	 [42.85377884 58.88333333]
	 [47.43950653 36.55      ]
	 [40.59957886 26.03333333]
	 [40.47061539 21.66666667]]
Train   Epoch: 199 / 450   Loss: 2.605e+04   Precision: 65.900%   Recall: 65.215%
Valid                   Loss:    2568   Precision: 36.614%   Recall: 34.066%
Train   Epoch: 200 / 450   Loss: 2.6e+04   Precision: 60.841%   Recall: 63.588%
Valid                   Loss:    2879   Precision: 29.167%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.92140198 61.41666667]
	 [78.31735229 42.78333333]
	 [49.00866699 65.41666667]
	 [34.86743546 31.46666667]
	 [94.11708832 47.7       ]]
Train   Epoch: 201 / 450   Loss: 2.374e+04   Precision: 57.838%   Recall: 63.246%
Valid                   Loss:    3248   Precision: 28.716%   Recall: 62.271%
Train   Epoch: 202 / 450   Loss: 2.645e+04   Precision: 64.839%   Recall: 63.454%
Valid                   Loss:    2671   Precision: 33.083%   Recall: 48.352%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 64.31751251  29.61666667]
	 [ 50.88394547  33.88333333]
	 [109.33450317  24.61666667]
	 [ 42.25038528  31.73333333]
	 [ 65.57518768  53.73333333]]
Train   Epoch: 203 / 450   Loss: 2.499e+04   Precision: 63.598%   Recall: 66.260%
Valid                   Loss:    3123   Precision: 32.416%   Recall: 38.828%
Train   Epoch: 204 / 450   Loss: 2.547e+04   Precision: 65.290%   Recall: 62.596%
Valid                   Loss:    2832   Precision: 31.183%   Recall: 53.114%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[63.48312378 29.66666667]
	 [60.23177338 78.5       ]
	 [63.24160767 44.45      ]
	 [52.00130463 12.38333333]
	 [66.04194641 45.96666667]]
Train   Epoch: 205 / 450   Loss: 2.66e+04   Precision: 66.361%   Recall: 65.463%
Valid                   Loss:    3346   Precision: 29.936%   Recall: 51.648%
Train   Epoch: 206 / 450   Loss: 2.62e+04   Precision: 64.978%   Recall: 64.626%
Valid                   Loss:    3149   Precision: 29.968%   Recall: 34.799%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.37392044 59.28333333]
	 [68.15705872 33.75      ]
	 [57.84449387 32.33333333]
	 [30.71565628 26.75      ]
	 [30.62852097 20.91666667]]
Train   Epoch: 207 / 450   Loss: 2.46e+04   Precision: 64.301%   Recall: 64.391%
Valid                   Loss:    3274   Precision: 33.708%   Recall: 43.956%
Train   Epoch: 208 / 450   Loss: 2.515e+04   Precision: 66.541%   Recall: 64.941%
Valid                   Loss:    2782   Precision: 37.500%   Recall: 42.857%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.77321243 46.16666667]
	 [58.56218338 48.21666667]
	 [78.41573334 50.21666667]
	 [ 9.76966763 24.5       ]
	 [55.74145508 44.5       ]]
Train   Epoch: 209 / 450   Loss: 2.504e+04   Precision: 64.301%   Recall: 64.137%
Valid                   Loss:    2793   Precision: 36.991%   Recall: 43.223%
Train   Epoch: 210 / 450   Loss: 2.34e+04   Precision: 65.508%   Recall: 63.521%
Valid                   Loss:    3018   Precision: 28.395%   Recall: 67.399%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 86.62008667  64.8       ]
	 [115.74664307  20.91666667]
	 [ 89.95584869  20.5       ]
	 [ 50.32091904   7.78333333]
	 [ 90.30257416  17.38333333]]
Train   Epoch: 211 / 450   Loss: 2.589e+04   Precision: 66.032%   Recall: 63.427%
Valid                   Loss:    3045   Precision: 33.071%   Recall: 61.538%
Train   Epoch: 212 / 450   Loss: 2.499e+04   Precision: 65.334%   Recall: 66.341%
Valid                   Loss:    2964   Precision: 34.012%   Recall: 42.857%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.28149414 77.55      ]
	 [42.51880646 52.95      ]
	 [32.7623024  24.        ]
	 [65.76442719 36.33333333]
	 [68.00857544 58.05      ]]
Train   Epoch: 213 / 450   Loss: 2.417e+04   Precision: 66.366%   Recall: 63.681%
Valid                   Loss:    3046   Precision: 37.025%   Recall: 42.857%
Train   Epoch: 214 / 450   Loss: 2.402e+04   Precision: 64.913%   Recall: 64.887%
Valid                   Loss:    3689   Precision: 25.196%   Recall: 58.974%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 69.11734772  37.71666667]
	 [132.18075562  67.41666667]
	 [ 64.35935211  28.        ]
	 [ 66.84770966  50.41666667]
	 [ 55.61002731  15.45      ]]
Train   Epoch: 215 / 450   Loss: 2.433e+04   Precision: 66.704%   Recall: 64.211%
Valid                   Loss:    3328   Precision: 30.105%   Recall: 52.381%
Train   Epoch: 216 / 450   Loss: 2.445e+04   Precision: 65.829%   Recall: 63.708%
Valid                   Loss:    2542   Precision: 34.627%   Recall: 42.491%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 86.55706787 100.16666667]
	 [ 29.07820129  30.5       ]
	 [ 60.14398575  24.63333333]
	 [ 61.58124542  84.45      ]
	 [ 62.1681633   41.33333333]]
Train   Epoch: 217 / 450   Loss: 2.574e+04   Precision: 64.329%   Recall: 64.773%
Valid                   Loss:    3717   Precision: 25.455%   Recall: 71.795%
Train   Epoch: 218 / 450   Loss: 2.562e+04   Precision: 66.143%   Recall: 62.000%
Valid                   Loss:    2785   Precision: 34.318%   Recall: 55.311%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 65.92324829 304.65      ]
	 [ 59.78076553 167.05      ]
	 [ 48.65917206  20.66666667]
	 [ 53.48372269   6.3       ]
	 [ 41.64724731  27.13333333]]
Train   Epoch: 219 / 450   Loss: 2.276e+04   Precision: 66.626%   Recall: 65.189%
Valid                   Loss:    2722   Precision: 35.328%   Recall: 45.421%
Train   Epoch: 220 / 450   Loss: 2.43e+04   Precision: 64.813%   Recall: 66.501%
Valid                   Loss:    2687   Precision: 34.271%   Recall: 49.084%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.26642609 100.03333333]
	 [ 38.72119141  42.13333333]
	 [ 50.81574631  26.81666667]
	 [ 36.33732224  25.98333333]
	 [ 96.60324097  27.38333333]]
Train   Epoch: 221 / 450   Loss: 2.553e+04   Precision: 63.114%   Recall: 61.833%
Valid                   Loss:    2770   Precision: 32.432%   Recall: 35.165%
Train   Epoch: 222 / 450   Loss: 2.691e+04   Precision: 61.094%   Recall: 62.248%
Valid                   Loss:    3029   Precision: 32.629%   Recall: 50.916%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 52.73274231  21.        ]
	 [100.98126984  59.66666667]
	 [ 36.67035675  49.25      ]
	 [ 39.27207184  22.        ]
	 [ 36.51496506  30.        ]]
Train   Epoch: 223 / 450   Loss: 2.441e+04   Precision: 65.996%   Recall: 63.273%
Valid                   Loss:    3159   Precision: 31.144%   Recall: 53.846%
Train   Epoch: 224 / 450   Loss: 2.509e+04   Precision: 67.393%   Recall: 64.391%
Valid                   Loss:    2676   Precision: 36.250%   Recall: 42.491%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[70.1253891  24.21666667]
	 [20.01092529 30.61666667]
	 [ 8.12881184 14.78333333]
	 [30.40917969  7.45      ]
	 [70.11528778 12.16666667]]
Train   Epoch: 225 / 450   Loss: 2.118e+04   Precision: 67.880%   Recall: 63.588%
Valid                   Loss:    3092   Precision: 29.478%   Recall: 47.619%
Train   Epoch: 226 / 450   Loss: 2.372e+04   Precision: 67.034%   Recall: 64.371%
Valid                   Loss:    2555   Precision: 33.512%   Recall: 45.788%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.37232971  40.        ]
	 [ 43.35918045  32.33333333]
	 [134.72140503  27.4       ]
	 [ 12.96268749  15.61666667]
	 [ 46.68176651  72.05      ]]
Train   Epoch: 227 / 450   Loss: 2.32e+04   Precision: 66.905%   Recall: 64.606%
Valid                   Loss:    2695   Precision: 34.571%   Recall: 54.579%
Train   Epoch: 228 / 450   Loss: 2.244e+04   Precision: 67.869%   Recall: 64.050%
Valid                   Loss:    2782   Precision: 34.225%   Recall: 46.886%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.45612717  31.66666667]
	 [ 53.05051041  20.91666667]
	 [ 69.99397278  21.95      ]
	 [ 69.02635193 107.08333333]
	 [ 64.38998413  31.16666667]]
Train   Epoch: 229 / 450   Loss: 2.456e+04   Precision: 66.356%   Recall: 63.983%
Valid                   Loss:    3373   Precision: 34.680%   Recall: 37.729%
Train   Epoch: 230 / 450   Loss: 2.293e+04   Precision: 67.856%   Recall: 64.211%
Valid                   Loss:    3047   Precision: 31.238%   Recall: 60.073%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.55024719 30.08333333]
	 [49.24306107 47.36666667]
	 [50.53673553 28.16666667]
	 [94.73182678 25.83333333]
	 [99.30107117 10.83333333]]
Train   Epoch: 231 / 450   Loss: 2.382e+04   Precision: 67.098%   Recall: 64.191%
Valid                   Loss:    3013   Precision: 32.615%   Recall: 44.322%
Train   Epoch: 232 / 450   Loss: 2.319e+04   Precision: 67.222%   Recall: 63.494%
Valid                   Loss:    2949   Precision: 32.122%   Recall: 61.538%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.84458923 164.55      ]
	 [ 56.31807709  25.7       ]
	 [ 38.92588806  27.16666667]
	 [ 50.15363693  25.33333333]
	 [ 70.41914368  65.11666667]]
Train   Epoch: 233 / 450   Loss: 2.403e+04   Precision: 65.376%   Recall: 65.008%
Valid                   Loss:    3128   Precision: 32.558%   Recall: 46.154%
Train   Epoch: 234 / 450   Loss: 2.268e+04   Precision: 66.992%   Recall: 63.393%
Valid                   Loss:    2631   Precision: 36.842%   Recall: 38.462%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.39847565 36.66666667]
	 [48.21962357 60.88333333]
	 [62.91948318 18.3       ]
	 [94.21847534 44.58333333]
	 [42.22861099 29.75      ]]
Train   Epoch: 235 / 450   Loss: 2.131e+04   Precision: 67.146%   Recall: 65.055%
Valid                   Loss:    3073   Precision: 35.793%   Recall: 35.531%
Train   Epoch: 236 / 450   Loss: 2.292e+04   Precision: 67.716%   Recall: 63.661%
Valid                   Loss:    2496   Precision: 34.426%   Recall: 30.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.51095581 22.        ]
	 [71.2736969  28.        ]
	 [84.60010529 39.93333333]
	 [34.46400452 27.75      ]
	 [31.29558372 23.28333333]]
Train   Epoch: 237 / 450   Loss: 2.258e+04   Precision: 67.918%   Recall: 64.452%
Valid                   Loss:    2628   Precision: 37.892%   Recall: 48.718%
Train   Epoch: 238 / 450   Loss: 2.414e+04   Precision: 67.774%   Recall: 63.407%
Valid                   Loss:    3241   Precision: 25.668%   Recall: 63.370%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[66.74526215 15.33333333]
	 [60.20729065 44.78333333]
	 [71.08269501 21.75      ]
	 [67.00337219 33.21666667]
	 [55.44404984 29.        ]]
Train   Epoch: 239 / 450   Loss: 2.337e+04   Precision: 65.999%   Recall: 66.026%
Valid                   Loss:    3169   Precision: 42.308%   Recall: 32.234%
Train   Epoch: 240 / 450   Loss: 2.315e+04   Precision: 66.863%   Recall: 63.119%
Valid                   Loss:    2518   Precision: 37.218%   Recall: 36.264%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[139.84107971  40.3       ]
	 [ 36.2922287   73.5       ]
	 [ 56.39399719  40.81666667]
	 [ 42.4683075   23.11666667]
	 [ 40.74565506  36.2       ]]
Train   Epoch: 241 / 450   Loss: 2.22e+04   Precision: 68.077%   Recall: 63.393%
Valid                   Loss:    3178   Precision: 36.161%   Recall: 29.670%
Train   Epoch: 242 / 450   Loss: 2.29e+04   Precision: 67.226%   Recall: 63.119%
Valid                   Loss:    2640   Precision: 37.860%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[76.01276398 30.16666667]
	 [47.17316055 49.91666667]
	 [39.69058609 25.66666667]
	 [44.94216537 44.38333333]
	 [50.4422226  74.01666667]]
Train   Epoch: 243 / 450   Loss: 2.316e+04   Precision: 66.981%   Recall: 64.653%
Valid                   Loss:    3826   Precision: 35.561%   Recall: 48.718%
Train   Epoch: 244 / 450   Loss: 2.208e+04   Precision: 68.392%   Recall: 65.758%
Valid                   Loss:    2634   Precision: 34.467%   Recall: 55.678%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.61283493 45.7       ]
	 [50.66032791 16.        ]
	 [51.47969055 42.63333333]
	 [80.45727539 87.11666667]
	 [36.41054916 21.        ]]
Train   Epoch: 245 / 450   Loss: 2.335e+04   Precision: 68.351%   Recall: 66.327%
Valid                   Loss:    3310   Precision: 36.066%   Recall: 40.293%
Train   Epoch: 246 / 450   Loss: 2.469e+04   Precision: 63.929%   Recall: 64.760%
Valid                   Loss:    2871   Precision: 39.689%   Recall: 37.363%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.32883835 20.21666667]
	 [39.99356079 45.58333333]
	 [40.23270035 35.45      ]
	 [43.89984894 18.38333333]
	 [80.71224976 48.5       ]]
Train   Epoch: 247 / 450   Loss: 2.145e+04   Precision: 67.856%   Recall: 62.811%
Valid                   Loss:    2526   Precision: 34.673%   Recall: 50.549%
Train   Epoch: 248 / 450   Loss: 1.977e+04   Precision: 68.869%   Recall: 63.882%
Valid                   Loss:    2930   Precision: 31.830%   Recall: 46.520%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.9505043  18.61666667]
	 [50.15221786 21.33333333]
	 [42.6301384  37.93333333]
	 [23.78862572  9.88333333]
	 [37.42647552 34.16666667]]
Train   Epoch: 249 / 450   Loss: 2.138e+04   Precision: 69.160%   Recall: 67.252%
Valid                   Loss:    2575   Precision: 37.184%   Recall: 37.729%
Train   Epoch: 250 / 450   Loss: 2.16e+04   Precision: 68.616%   Recall: 64.365%
Valid                   Loss:    2942   Precision: 33.121%   Recall: 57.143%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.58747101 26.33333333]
	 [49.45141602 22.33333333]
	 [46.90348816 52.        ]
	 [37.86186218 70.33333333]
	 [62.87683868 34.03333333]]
Train   Epoch: 251 / 450   Loss: 2.279e+04   Precision: 68.104%   Recall: 64.847%
Valid                   Loss:    3102   Precision: 37.915%   Recall: 29.304%
Train   Epoch: 252 / 450   Loss: 2.162e+04   Precision: 67.224%   Recall: 63.608%
Valid                   Loss:    3470   Precision: 27.137%   Recall: 73.260%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.15510941  18.38333333]
	 [120.87718201  21.11666667]
	 [ 70.33023834  43.25      ]
	 [ 41.10610962  35.41666667]
	 [ 72.64850616  27.23333333]]
Train   Epoch: 253 / 450   Loss: 2.2e+04   Precision: 68.053%   Recall: 65.992%
Valid                   Loss:    2694   Precision: 33.420%   Recall: 46.886%
Train   Epoch: 254 / 450   Loss: 2.311e+04   Precision: 68.867%   Recall: 65.403%
Valid                   Loss:    2701   Precision: 37.532%   Recall: 53.480%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[65.46943665 16.66666667]
	 [49.39048386 29.45      ]
	 [77.31600189 34.75      ]
	 [37.76340103 26.78333333]
	 [53.5125618  37.5       ]]
Train   Epoch: 255 / 450   Loss: 2.029e+04   Precision: 69.201%   Recall: 65.108%
Valid                   Loss:    3223   Precision: 40.306%   Recall: 28.938%
Train   Epoch: 256 / 450   Loss: 2.171e+04   Precision: 69.578%   Recall: 64.191%
Valid                   Loss:    2578   Precision: 34.653%   Recall: 38.462%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 39.45899963  11.5       ]
	 [ 24.56780434  22.88333333]
	 [ 68.02753448 258.2       ]
	 [ 36.43808365  30.11666667]
	 [ 45.85719681   7.45      ]]
Train   Epoch: 257 / 450   Loss: 2.237e+04   Precision: 69.529%   Recall: 65.463%
Valid                   Loss:    2913   Precision: 34.118%   Recall: 31.868%
Train   Epoch: 258 / 450   Loss: 2.136e+04   Precision: 69.263%   Recall: 64.271%
Valid                   Loss:    3090   Precision: 28.868%   Recall: 66.300%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.3384552  27.41666667]
	 [32.08952332 17.55      ]
	 [38.92667389 14.41666667]
	 [52.86036301 43.78333333]
	 [72.1145401  42.95      ]]
Train   Epoch: 259 / 450   Loss: 2.39e+04   Precision: 65.742%   Recall: 66.187%
Valid                   Loss:    3223   Precision: 33.259%   Recall: 54.945%
Train   Epoch: 260 / 450   Loss: 2.33e+04   Precision: 66.490%   Recall: 63.105%
Valid                   Loss:    3321   Precision: 38.255%   Recall: 41.758%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[69.74184418 35.58333333]
	 [36.42729187 69.06666667]
	 [46.04200363 35.55      ]
	 [66.94809723 42.28333333]
	 [49.04257584 26.41666667]]
Train   Epoch: 261 / 450   Loss: 2.233e+04   Precision: 66.760%   Recall: 63.675%
Valid                   Loss:    4235   Precision: 31.776%   Recall: 37.363%
Train   Epoch: 262 / 450   Loss: 1.929e+04   Precision: 68.448%   Recall: 66.059%
Valid                   Loss:    2627   Precision: 40.000%   Recall: 35.165%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 37.46678543  66.86666667]
	 [ 80.32455444 128.66666667]
	 [ 88.74442291  21.9       ]
	 [ 74.52626038  34.66666667]
	 [ 17.40889931  12.95      ]]
Train   Epoch: 263 / 450   Loss: 1.868e+04   Precision: 70.261%   Recall: 63.949%
Valid                   Loss:    3128   Precision: 27.744%   Recall: 66.667%
Train   Epoch: 264 / 450   Loss: 1.97e+04   Precision: 70.030%   Recall: 65.128%
Valid                   Loss:    3078   Precision: 36.963%   Recall: 47.253%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 74.29051208  27.16666667]
	 [ 90.77816772  52.78333333]
	 [100.69658661 178.83333333]
	 [ 73.08538818  42.88333333]
	 [ 43.04694748  15.55      ]]
Train   Epoch: 265 / 450   Loss: 1.885e+04   Precision: 70.289%   Recall: 64.907%
Valid                   Loss:    3093   Precision: 26.232%   Recall: 60.440%
Train   Epoch: 266 / 450   Loss: 2.067e+04   Precision: 66.647%   Recall: 66.481%
Valid                   Loss:    2602   Precision: 38.047%   Recall: 41.392%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.8202095  28.11666667]
	 [30.8507309  26.5       ]
	 [36.66286087 33.21666667]
	 [38.12477112 24.83333333]
	 [72.92069244 36.68333333]]
Train   Epoch: 267 / 450   Loss: 1.945e+04   Precision: 70.935%   Recall: 65.229%
Valid                   Loss:    2811   Precision: 34.586%   Recall: 33.700%
Train   Epoch: 268 / 450   Loss: 1.933e+04   Precision: 70.655%   Recall: 65.222%
Valid                   Loss:    3638   Precision: 34.300%   Recall: 52.015%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[145.20924377  41.75      ]
	 [ 59.02597809  73.45      ]
	 [ 36.409832    27.21666667]
	 [ 45.59495544  21.38333333]
	 [ 96.33615112  22.03333333]]
Train   Epoch: 269 / 450   Loss: 2.104e+04   Precision: 68.366%   Recall: 64.666%
Valid                   Loss:    2337   Precision: 34.797%   Recall: 37.729%
Train   Epoch: 270 / 450   Loss: 1.996e+04   Precision: 71.547%   Recall: 66.448%
Valid                   Loss:    2564   Precision: 38.333%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.11437607  25.61666667]
	 [ 46.82232666  43.83333333]
	 [ 40.30800247  35.58333333]
	 [ 47.90719223  47.08333333]
	 [ 78.49632263 107.        ]]
Train   Epoch: 271 / 450   Loss: 1.942e+04   Precision: 71.110%   Recall: 64.351%
Valid                   Loss:    2766   Precision: 36.944%   Recall: 48.718%
Train   Epoch: 272 / 450   Loss: 1.929e+04   Precision: 70.931%   Recall: 64.954%
Valid                   Loss:    2438   Precision: 39.683%   Recall: 27.473%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.55883026 45.25      ]
	 [32.71660233 84.95      ]
	 [51.34957886 72.5       ]
	 [31.2579422  31.91666667]
	 [67.92824554 65.28333333]]
Train   Epoch: 273 / 450   Loss: 1.827e+04   Precision: 71.687%   Recall: 65.838%
Valid                   Loss:    2850   Precision: 38.978%   Recall: 44.689%
Train   Epoch: 274 / 450   Loss: 1.876e+04   Precision: 71.649%   Recall: 66.374%
Valid                   Loss:    2871   Precision: 35.238%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[30.68322182 41.16666667]
	 [43.71034241 34.23333333]
	 [37.92739105 54.        ]
	 [31.09119606 11.5       ]
	 [31.79831696 20.5       ]]
Train   Epoch: 275 / 450   Loss: 1.944e+04   Precision: 71.950%   Recall: 65.463%
Valid                   Loss:    2854   Precision: 39.919%   Recall: 36.264%
Train   Epoch: 276 / 450   Loss: 1.738e+04   Precision: 72.476%   Recall: 66.126%
Valid                   Loss:    2583   Precision: 45.327%   Recall: 35.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[20.56077194 13.33333333]
	 [54.44471359 30.46666667]
	 [40.17618942 21.83333333]
	 [37.14219666 35.55      ]
	 [42.37800598 40.        ]]
Train   Epoch: 277 / 450   Loss: 1.942e+04   Precision: 71.077%   Recall: 66.783%
Valid                   Loss:    2919   Precision: 39.267%   Recall: 27.473%
Train   Epoch: 278 / 450   Loss: 1.888e+04   Precision: 70.345%   Recall: 64.907%
Valid                   Loss:    2350   Precision: 36.078%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 35.40832138  43.66666667]
	 [ 81.23799133  30.06666667]
	 [109.64196777 107.38333333]
	 [ 28.51362038  33.        ]
	 [ 47.44149017  22.88333333]]
Train   Epoch: 279 / 450   Loss: 1.954e+04   Precision: 69.710%   Recall: 66.796%
Valid                   Loss:    3104   Precision: 29.508%   Recall: 32.967%
Train   Epoch: 280 / 450   Loss: 1.926e+04   Precision: 71.317%   Recall: 63.487%
Valid                   Loss:    3332   Precision: 29.044%   Recall: 57.875%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[11.32201004 22.1       ]
	 [39.75516129 19.33333333]
	 [40.64465714 23.36666667]
	 [36.22279739 37.11666667]
	 [44.24308777 28.05      ]]
Train   Epoch: 281 / 450   Loss: 2.009e+04   Precision: 71.932%   Recall: 63.360%
Valid                   Loss:    2308   Precision: 40.000%   Recall: 7.326%
Train   Epoch: 282 / 450   Loss: 3.412e+04   Precision: 43.240%   Recall: 80.823%
Valid                   Loss:    2626   Precision: 32.899%   Recall: 36.996%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.93542099  13.08333333]
	 [ 89.056633    63.2       ]
	 [129.51118469  70.33333333]
	 [ 43.5910759   19.11666667]
	 [ 65.30284119  57.33333333]]
Train   Epoch: 283 / 450   Loss: 3.151e+04   Precision: 46.175%   Recall: 67.834%
Valid                   Loss:    3530   Precision: 23.925%   Recall: 65.201%
Train   Epoch: 284 / 450   Loss: 3.221e+04   Precision: 50.452%   Recall: 62.409%
Valid                   Loss:    3343   Precision: 31.750%   Recall: 46.520%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.76593399  44.78333333]
	 [ 51.83228302  51.6       ]
	 [ 51.56209183  35.86666667]
	 [ 49.82011032  44.        ]
	 [174.16473389  15.41666667]]
Train   Epoch: 285 / 450   Loss: 3.03e+04   Precision: 54.242%   Recall: 70.661%
Valid                   Loss:    4264   Precision: 25.032%   Recall: 71.429%
Train   Epoch: 286 / 450   Loss: 2.235e+04   Precision: 63.233%   Recall: 62.302%
Valid                   Loss:    3155   Precision: 28.680%   Recall: 41.392%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.88459015  26.5       ]
	 [ 37.94651794  43.78333333]
	 [ 33.52313995  51.        ]
	 [133.55960083  60.        ]
	 [ 47.87887573  33.        ]]
Train   Epoch: 287 / 450   Loss: 2.01e+04   Precision: 69.469%   Recall: 63.373%
Valid                   Loss:    3224   Precision: 33.333%   Recall: 36.996%
Train   Epoch: 288 / 450   Loss: 1.932e+04   Precision: 70.079%   Recall: 64.338%
Valid                   Loss:    3163   Precision: 29.680%   Recall: 47.619%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.88070297 31.5       ]
	 [35.02019882 26.46666667]
	 [69.02553558 56.38333333]
	 [45.32373428 21.13333333]
	 [38.85046005 19.25      ]]
Train   Epoch: 289 / 450   Loss: 1.766e+04   Precision: 71.430%   Recall: 65.564%
Valid                   Loss:    3185   Precision: 36.522%   Recall: 30.769%
Train   Epoch: 290 / 450   Loss: 1.785e+04   Precision: 72.305%   Recall: 65.423%
Valid                   Loss:    3134   Precision: 30.435%   Recall: 41.026%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.9027977  23.66666667]
	 [40.97591782 17.66666667]
	 [35.87330246 21.83333333]
	 [43.91627502 49.16666667]
	 [35.4275856  52.33333333]]
Train   Epoch: 291 / 450   Loss: 1.861e+04   Precision: 71.623%   Recall: 64.787%
Valid                   Loss:    3218   Precision: 36.957%   Recall: 24.908%
Train   Epoch: 292 / 450   Loss: 2.059e+04   Precision: 70.480%   Recall: 64.834%
Valid                   Loss:    3194   Precision: 29.835%   Recall: 53.114%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.61278915  39.71666667]
	 [ 39.32287216  16.76666667]
	 [208.1449585  144.        ]
	 [ 59.08401108  31.        ]
	 [ 73.68089294  43.71666667]]
Train   Epoch: 293 / 450   Loss: 1.795e+04   Precision: 70.716%   Recall: 65.590%
Valid                   Loss:    2818   Precision: 35.171%   Recall: 49.084%
Train   Epoch: 294 / 450   Loss: 1.78e+04   Precision: 71.535%   Recall: 64.860%
Valid                   Loss:    2540   Precision: 37.872%   Recall: 32.601%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[71.58921051 44.21666667]
	 [77.66472626 23.        ]
	 [40.21429825 35.2       ]
	 [40.08643341 21.66666667]
	 [60.87198639 25.41666667]]
Train   Epoch: 295 / 450   Loss: 1.939e+04   Precision: 71.889%   Recall: 65.436%
Valid                   Loss:    2564   Precision: 36.437%   Recall: 32.967%
Train   Epoch: 296 / 450   Loss: 1.703e+04   Precision: 71.410%   Recall: 66.220%
Valid                   Loss:    3192   Precision: 28.440%   Recall: 56.777%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.71203995  41.26666667]
	 [ 43.85469055  27.88333333]
	 [ 56.9875412  107.68333333]
	 [ 46.10839462  43.03333333]
	 [ 92.99926758  25.71666667]]
Train   Epoch: 297 / 450   Loss: 1.762e+04   Precision: 72.706%   Recall: 65.323%
Valid                   Loss:    2603   Precision: 39.918%   Recall: 35.531%
Train   Epoch: 298 / 450   Loss: 1.649e+04   Precision: 73.000%   Recall: 66.086%
Valid                   Loss:    2553   Precision: 42.520%   Recall: 19.780%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.79248047 20.61666667]
	 [37.05526733 22.2       ]
	 [33.11372375 28.28333333]
	 [22.0674572  23.45      ]
	 [32.89316559 67.36666667]]
Train   Epoch: 299 / 450   Loss: 1.784e+04   Precision: 73.443%   Recall: 67.131%
Valid                   Loss:    2622   Precision: 39.252%   Recall: 30.769%
Train   Epoch: 300 / 450   Loss: 1.8e+04   Precision: 71.031%   Recall: 65.483%
Valid                   Loss:    2647   Precision: 40.444%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 32.02396774  37.61666667]
	 [ 29.54322433  37.51666667]
	 [ 34.66707611  17.18333333]
	 [ 25.79683495  39.58333333]
	 [110.03577423  79.13333333]]
Train   Epoch: 301 / 450   Loss: 1.812e+04   Precision: 72.589%   Recall: 64.887%
Valid                   Loss:    2544   Precision: 35.519%   Recall: 23.810%
Train   Epoch: 302 / 450   Loss: 1.794e+04   Precision: 73.139%   Recall: 64.894%
Valid                   Loss:    2629   Precision: 31.988%   Recall: 40.659%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.37917328  29.45      ]
	 [ 48.28559113  63.        ]
	 [ 63.55225372  44.08333333]
	 [ 59.18262482 109.5       ]
	 [ 43.06707382  40.5       ]]
Train   Epoch: 303 / 450   Loss: 1.807e+04   Precision: 71.486%   Recall: 64.887%
Valid                   Loss:    3606   Precision: 27.056%   Recall: 37.363%
Train   Epoch: 304 / 450   Loss: 1.936e+04   Precision: 69.040%   Recall: 64.753%
Valid                   Loss:    2500   Precision: 33.333%   Recall: 32.967%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.56639481 19.23333333]
	 [39.03250885 39.05      ]
	 [38.58609772 63.05      ]
	 [50.15639114 19.58333333]
	 [88.50801849 20.1       ]]
Train   Epoch: 305 / 450   Loss: 1.774e+04   Precision: 71.336%   Recall: 66.347%
Valid                   Loss:    2479   Precision: 31.978%   Recall: 43.223%
Train   Epoch: 306 / 450   Loss: 1.695e+04   Precision: 72.525%   Recall: 65.899%
Valid                   Loss:    2808   Precision: 28.713%   Recall: 42.491%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.81552124  50.78333333]
	 [107.1272583   26.36666667]
	 [757.30944824 991.45      ]
	 [ 43.12413025  37.36666667]
	 [ 68.69782257  49.45      ]]
Train   Epoch: 307 / 450   Loss: 1.848e+04   Precision: 72.687%   Recall: 66.455%
Valid                   Loss:    2854   Precision: 31.549%   Recall: 41.026%
Train   Epoch: 308 / 450   Loss: 1.691e+04   Precision: 73.234%   Recall: 65.081%
Valid                   Loss:    3106   Precision: 33.639%   Recall: 40.293%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[84.25124359 37.88333333]
	 [53.43569946 58.08333333]
	 [36.32085037 39.        ]
	 [44.65217972 52.6       ]
	 [44.23001862 27.        ]]
Train   Epoch: 309 / 450   Loss: 1.659e+04   Precision: 72.872%   Recall: 66.307%
Valid                   Loss:    2390   Precision: 41.379%   Recall: 26.374%
Train   Epoch: 310 / 450   Loss: 1.903e+04   Precision: 66.628%   Recall: 64.633%
Valid                   Loss:    2779   Precision: 37.500%   Recall: 20.879%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.30719757  28.75      ]
	 [  2.59122348  23.63333333]
	 [ 97.89790344 114.33333333]
	 [ 41.26135635  38.8       ]
	 [ 35.24142838  22.3       ]]
Train   Epoch: 311 / 450   Loss: 1.867e+04   Precision: 72.103%   Recall: 65.738%
Valid                   Loss:    2675   Precision: 33.040%   Recall: 27.473%
Train   Epoch: 312 / 450   Loss: 1.673e+04   Precision: 73.262%   Recall: 65.430%
Valid                   Loss:    2844   Precision: 37.500%   Recall: 21.978%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.35447693 21.08333333]
	 [23.18421745 44.        ]
	 [26.9540863  12.58333333]
	 [40.10068893 27.16666667]
	 [33.55464172 43.96666667]]
Train   Epoch: 313 / 450   Loss: 1.582e+04   Precision: 73.288%   Recall: 65.791%
Valid                   Loss:    2808   Precision: 38.421%   Recall: 26.740%
Train   Epoch: 314 / 450   Loss: 1.804e+04   Precision: 73.390%   Recall: 65.785%
Valid                   Loss:    2840   Precision: 32.867%   Recall: 34.432%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.39543152  42.2       ]
	 [137.87161255 193.        ]
	 [ 29.82667923  21.51666667]
	 [ 63.44321442  22.66666667]
	 [ 57.62924194  38.45      ]]
Train   Epoch: 315 / 450   Loss: 1.727e+04   Precision: 72.717%   Recall: 65.718%
Valid                   Loss:    2721   Precision: 37.107%   Recall: 21.612%
Train   Epoch: 316 / 450   Loss: 1.65e+04   Precision: 71.534%   Recall: 66.488%
Valid                   Loss:    2751   Precision: 32.331%   Recall: 31.502%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 32.12052155  37.36666667]
	 [ 39.2473526   45.56666667]
	 [ 53.0326767  141.66666667]
	 [ 21.05954552  14.5       ]
	 [ 48.47350311  13.45      ]]
Train   Epoch: 317 / 450   Loss: 1.735e+04   Precision: 72.852%   Recall: 65.771%
Valid                   Loss:    2781   Precision: 40.796%   Recall: 30.037%
Train   Epoch: 318 / 450   Loss: 1.637e+04   Precision: 73.770%   Recall: 66.314%
Valid                   Loss:    2735   Precision: 36.562%   Recall: 42.857%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.00154877 55.95      ]
	 [45.5744133  44.        ]
	 [60.22293472  6.31666667]
	 [31.9474926  10.66666667]
	 [18.89390945 21.75      ]]
Train   Epoch: 319 / 450   Loss: 1.661e+04   Precision: 72.812%   Recall: 67.774%
Valid                   Loss:    3251   Precision: 37.308%   Recall: 35.531%
Train   Epoch: 320 / 450   Loss: 1.658e+04   Precision: 74.160%   Recall: 65.034%
Valid                   Loss:    3159   Precision: 34.431%   Recall: 42.125%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.33282471  37.16666667]
	 [127.0534668   19.5       ]
	 [ 30.51877403  47.83333333]
	 [ 37.83012772  15.16666667]
	 [ 66.46633148   9.        ]]
Train   Epoch: 321 / 450   Loss: 1.731e+04   Precision: 74.608%   Recall: 66.327%
Valid                   Loss:    2751   Precision: 31.680%   Recall: 42.125%
Train   Epoch: 322 / 450   Loss: 1.562e+04   Precision: 72.978%   Recall: 66.736%
Valid                   Loss:    3139   Precision: 32.573%   Recall: 36.630%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.13518524  48.46666667]
	 [ 23.61692619  32.03333333]
	 [203.56221008 201.91666667]
	 [ 37.61840057  38.76666667]
	 [107.78186798  41.05      ]]
Train   Epoch: 323 / 450   Loss: 1.666e+04   Precision: 74.292%   Recall: 65.758%
Valid                   Loss:    2831   Precision: 34.911%   Recall: 43.223%
Train   Epoch: 324 / 450   Loss: 1.561e+04   Precision: 75.099%   Recall: 65.818%
Valid                   Loss:    2522   Precision: 39.922%   Recall: 37.729%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 32.73038101  33.21666667]
	 [ 86.29239655  48.        ]
	 [ 33.44729233  30.11666667]
	 [ 66.4175415   37.93333333]
	 [112.18558502  60.        ]]
Train   Epoch: 325 / 450   Loss: 1.517e+04   Precision: 74.322%   Recall: 67.372%
Valid                   Loss:    2963   Precision: 35.581%   Recall: 34.799%
Train   Epoch: 326 / 450   Loss: 1.646e+04   Precision: 74.010%   Recall: 67.694%
Valid                   Loss:    3323   Precision: 35.928%   Recall: 43.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.41287231  46.45      ]
	 [ 35.52666473  31.5       ]
	 [ 21.4914856   51.1       ]
	 [120.42601776 154.96666667]
	 [ 46.87722397  20.        ]]
Train   Epoch: 327 / 450   Loss: 1.473e+04   Precision: 74.393%   Recall: 66.883%
Valid                   Loss:    2641   Precision: 38.730%   Recall: 44.689%
Train   Epoch: 328 / 450   Loss: 1.507e+04   Precision: 74.373%   Recall: 66.890%
Valid                   Loss:    2506   Precision: 40.110%   Recall: 26.740%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 35.96245193  19.28333333]
	 [ 34.85871887  64.58333333]
	 [ 43.14751816  33.63333333]
	 [ 38.42330551  65.65      ]
	 [143.11224365  89.83333333]]
Train   Epoch: 329 / 450   Loss: 1.651e+04   Precision: 72.746%   Recall: 63.775%
Valid                   Loss:    3097   Precision: 29.098%   Recall: 26.007%
Train   Epoch: 330 / 450   Loss: 1.789e+04   Precision: 71.154%   Recall: 65.430%
Valid                   Loss:    2854   Precision: 34.576%   Recall: 37.363%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.11589432  43.8       ]
	 [ 58.44857788  24.95      ]
	 [119.59282684 199.75      ]
	 [ 56.63557816  55.73333333]
	 [ 39.89991379  24.25      ]]
Train   Epoch: 331 / 450   Loss: 1.559e+04   Precision: 73.509%   Recall: 66.207%
Valid                   Loss:    3474   Precision: 37.868%   Recall: 37.729%
Train   Epoch: 332 / 450   Loss: 1.527e+04   Precision: 73.852%   Recall: 65.041%
Valid                   Loss:    2734   Precision: 33.136%   Recall: 41.026%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.22780991  69.86666667]
	 [ 29.83919144  26.33333333]
	 [ 57.28613663  31.5       ]
	 [240.95878601 344.21666667]
	 [ 56.48812866  42.91666667]]
Train   Epoch: 333 / 450   Loss: 1.681e+04   Precision: 74.983%   Recall: 66.776%
Valid                   Loss:    2720   Precision: 35.231%   Recall: 36.264%
Train   Epoch: 334 / 450   Loss: 1.533e+04   Precision: 75.584%   Recall: 65.443%
Valid                   Loss:    2747   Precision: 38.696%   Recall: 32.601%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.68721771  43.4       ]
	 [ 97.82137299 164.9       ]
	 [ 61.88928986  31.25      ]
	 [ 10.78494453  31.8       ]
	 [ 82.70508575  37.01666667]]
Train   Epoch: 335 / 450   Loss: 1.533e+04   Precision: 74.482%   Recall: 67.861%
Valid                   Loss:    2627   Precision: 41.279%   Recall: 26.007%
Train   Epoch: 336 / 450   Loss: 1.559e+04   Precision: 74.897%   Recall: 67.292%
Valid                   Loss:    3224   Precision: 34.035%   Recall: 35.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[107.98049927 119.16666667]
	 [ 27.19993973  44.63333333]
	 [ 27.68589783  38.05      ]
	 [ 79.07124329  81.38333333]
	 [ 41.03136826  19.11666667]]
Train   Epoch: 337 / 450   Loss: 1.565e+04   Precision: 73.817%   Recall: 65.302%
Valid                   Loss:    2557   Precision: 43.204%   Recall: 32.601%
Train   Epoch: 338 / 450   Loss: 1.44e+04   Precision: 74.389%   Recall: 67.278%
Valid                   Loss:    2618   Precision: 35.193%   Recall: 30.037%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.79162216  26.45      ]
	 [ 39.15073013  41.        ]
	 [ 70.42557526  59.48333333]
	 [100.30594635  71.06666667]
	 [ 34.40920258  48.01666667]]
Train   Epoch: 339 / 450   Loss: 1.67e+04   Precision: 73.608%   Recall: 67.104%
Valid                   Loss:    3426   Precision: 31.329%   Recall: 36.264%
Train   Epoch: 340 / 450   Loss: 1.615e+04   Precision: 73.754%   Recall: 68.216%
Valid                   Loss:    2740   Precision: 43.256%   Recall: 34.066%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.7026062  43.03333333]
	 [19.77157211 29.13333333]
	 [37.98714828 25.16666667]
	 [43.69696045 55.63333333]
	 [41.60735703 12.86666667]]
Train   Epoch: 341 / 450   Loss: 1.558e+04   Precision: 74.138%   Recall: 65.979%
Valid                   Loss:    2979   Precision: 35.361%   Recall: 34.066%
Train   Epoch: 342 / 450   Loss: 1.543e+04   Precision: 74.188%   Recall: 65.611%
Valid                   Loss:    2652   Precision: 41.346%   Recall: 31.502%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.90811539 44.        ]
	 [28.91174316 29.33333333]
	 [38.16415405 37.66666667]
	 [49.04475784 57.78333333]
	 [38.7652359  67.33333333]]
Train   Epoch: 343 / 450   Loss: 1.477e+04   Precision: 74.388%   Recall: 67.781%
Valid                   Loss:    2994   Precision: 36.413%   Recall: 24.542%
Train   Epoch: 344 / 450   Loss: 1.591e+04   Precision: 74.282%   Recall: 65.122%
Valid                   Loss:    2861   Precision: 29.885%   Recall: 47.619%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 76.12864685  40.45      ]
	 [ 31.28482246  30.        ]
	 [203.02507019 309.83333333]
	 [ 44.90618515  24.45      ]
	 [ 11.80154991  22.38333333]]
Train   Epoch: 345 / 450   Loss: 1.47e+04   Precision: 74.915%   Recall: 68.015%
Valid                   Loss:    2900   Precision: 45.238%   Recall: 27.839%
Train   Epoch: 346 / 450   Loss: 1.452e+04   Precision: 73.162%   Recall: 66.046%
Valid                   Loss:    3144   Precision: 34.884%   Recall: 27.473%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.39421082 26.4       ]
	 [31.63495064 20.41666667]
	 [30.39541435 25.        ]
	 [28.12404823 37.25      ]
	 [37.79877472 27.91666667]]
Train   Epoch: 347 / 450   Loss: 1.458e+04   Precision: 74.726%   Recall: 65.771%
Valid                   Loss:    2841   Precision: 35.192%   Recall: 36.996%
Train   Epoch: 348 / 450   Loss: 1.427e+04   Precision: 75.184%   Recall: 67.640%
Valid                   Loss:    2504   Precision: 37.226%   Recall: 37.363%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.66166306 37.45      ]
	 [56.98390961 60.05      ]
	 [51.06209183 44.66666667]
	 [27.83499718 22.        ]
	 [13.43300247 48.76666667]]
Train   Epoch: 349 / 450   Loss: 1.46e+04   Precision: 75.903%   Recall: 66.548%
Valid                   Loss:    2650   Precision: 37.895%   Recall: 39.560%
Train   Epoch: 350 / 450   Loss: 1.429e+04   Precision: 76.292%   Recall: 66.562%
Valid                   Loss:    3130   Precision: 35.439%   Recall: 36.996%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[29.17170143 34.        ]
	 [35.37133408 91.06666667]
	 [ 3.9689765   8.55      ]
	 [33.82889557 32.51666667]
	 [30.87582016 25.11666667]]
Train   Epoch: 351 / 450   Loss: 1.401e+04   Precision: 75.429%   Recall: 67.098%
Valid                   Loss:    2921   Precision: 32.527%   Recall: 44.322%
Train   Epoch: 352 / 450   Loss: 1.637e+04   Precision: 71.257%   Recall: 64.432%
Valid                   Loss:    3129   Precision: 30.175%   Recall: 44.322%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.50866699 74.88333333]
	 [54.36963654 25.        ]
	 [80.79167175 39.95      ]
	 [74.09693146 63.        ]
	 [78.79986572 27.33333333]]
Train   Epoch: 353 / 450   Loss: 1.622e+04   Precision: 70.412%   Recall: 65.182%
Valid                   Loss:    2713   Precision: 40.930%   Recall: 32.234%
Train   Epoch: 354 / 450   Loss: 1.314e+04   Precision: 75.856%   Recall: 65.999%
Valid                   Loss:    2787   Precision: 36.268%   Recall: 37.729%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[128.94950867 277.2       ]
	 [ 21.48752022  15.95      ]
	 [ 40.26073074  18.5       ]
	 [ 38.18799973  46.46666667]
	 [104.05456543 116.55      ]]
Train   Epoch: 355 / 450   Loss: 1.369e+04   Precision: 74.737%   Recall: 66.167%
Valid                   Loss:    2577   Precision: 43.820%   Recall: 28.571%
Train   Epoch: 356 / 450   Loss: 1.408e+04   Precision: 74.455%   Recall: 67.238%
Valid                   Loss:    2495   Precision: 38.140%   Recall: 30.037%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[82.45894623 68.        ]
	 [40.21001816 35.88333333]
	 [34.73138046 30.6       ]
	 [48.96408463 29.08333333]
	 [29.02073669 33.46666667]]
Train   Epoch: 357 / 450   Loss: 1.406e+04   Precision: 75.658%   Recall: 66.582%
Valid                   Loss:    3072   Precision: 31.402%   Recall: 37.729%
Train   Epoch: 358 / 450   Loss: 1.625e+04   Precision: 69.859%   Recall: 65.483%
Valid                   Loss:    3074   Precision: 38.153%   Recall: 34.799%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 84.94755554  53.91666667]
	 [ 51.34034729  33.5       ]
	 [485.7578125  390.        ]
	 [ 90.72645569  49.93333333]
	 [ 40.96633911  16.        ]]
Train   Epoch: 359 / 450   Loss: 1.506e+04   Precision: 74.505%   Recall: 67.084%
Valid                   Loss:    2603   Precision: 34.875%   Recall: 35.897%
Train   Epoch: 360 / 450   Loss: 1.44e+04   Precision: 75.348%   Recall: 65.945%
Valid                   Loss:    2997   Precision: 38.057%   Recall: 34.432%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.94165039 32.01666667]
	 [28.61537361 35.75      ]
	 [33.51786423 23.2       ]
	 [40.12220764 63.56666667]
	 [26.85929489 31.61666667]]
Train   Epoch: 361 / 450   Loss: 1.463e+04   Precision: 75.904%   Recall: 66.341%
Valid                   Loss:    2991   Precision: 32.721%   Recall: 32.601%
Train   Epoch: 362 / 450   Loss: 1.372e+04   Precision: 75.876%   Recall: 67.459%
Valid                   Loss:    2932   Precision: 37.443%   Recall: 30.037%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.46084595 34.7       ]
	 [35.86751938 32.81666667]
	 [26.20246315 34.61666667]
	 [34.18974304 32.45      ]
	 [23.87463951 41.2       ]]
Train   Epoch: 363 / 450   Loss: 1.296e+04   Precision: 75.592%   Recall: 68.189%
Valid                   Loss:    2933   Precision: 35.317%   Recall: 32.601%
Train   Epoch: 364 / 450   Loss: 1.346e+04   Precision: 76.315%   Recall: 67.077%
Valid                   Loss:    2621   Precision: 35.772%   Recall: 32.234%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.549366   29.16666667]
	 [49.21082306 47.66666667]
	 [66.33773804 29.        ]
	 [84.38388824 25.2       ]
	 [29.27786255 35.21666667]]
Train   Epoch: 365 / 450   Loss: 1.307e+04   Precision: 74.536%   Recall: 66.763%
Valid                   Loss:    2643   Precision: 37.037%   Recall: 25.641%
Train   Epoch: 366 / 450   Loss: 1.31e+04   Precision: 75.460%   Recall: 67.600%
Valid                   Loss:    3101   Precision: 36.364%   Recall: 20.513%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.60997009  28.83333333]
	 [ 80.38033295 102.66666667]
	 [ 28.2896347   64.        ]
	 [ 24.34904861  43.        ]
	 [ 42.62628555  19.56666667]]
Train   Epoch: 367 / 450   Loss: 1.304e+04   Precision: 77.015%   Recall: 68.343%
Valid                   Loss:    2639   Precision: 38.565%   Recall: 31.502%
Train   Epoch: 368 / 450   Loss: 1.25e+04   Precision: 76.563%   Recall: 66.917%
Valid                   Loss:    2482   Precision: 35.827%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.68081093  26.66666667]
	 [ 40.07297134  45.23333333]
	 [ 34.27772522  29.66666667]
	 [ 97.56538391 127.7       ]
	 [ 27.1944294   21.61666667]]
Train   Epoch: 369 / 450   Loss: 1.304e+04   Precision: 76.745%   Recall: 67.111%
Valid                   Loss:    2747   Precision: 40.230%   Recall: 25.641%
Train   Epoch: 370 / 450   Loss: 1.216e+04   Precision: 76.827%   Recall: 67.754%
Valid                   Loss:    2593   Precision: 34.513%   Recall: 28.571%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.26544952 16.4       ]
	 [79.60623932 88.05      ]
	 [46.59075928 37.        ]
	 [36.3721199  31.5       ]
	 [40.3870697  39.91666667]]
Train   Epoch: 371 / 450   Loss: 1.357e+04   Precision: 76.531%   Recall: 65.878%
Valid                   Loss:    3752   Precision: 32.065%   Recall: 21.612%
Train   Epoch: 372 / 450   Loss: 1.373e+04   Precision: 75.270%   Recall: 67.198%
Valid                   Loss:    3119   Precision: 43.017%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[160.56802368  31.33333333]
	 [ 22.77733803  18.45      ]
	 [ 63.84970093  37.5       ]
	 [ 29.71606255  29.88333333]
	 [ 96.30129242  94.        ]]
Train   Epoch: 373 / 450   Loss: 1.206e+04   Precision: 76.527%   Recall: 66.716%
Valid                   Loss:    2625   Precision: 40.909%   Recall: 26.374%
Train   Epoch: 374 / 450   Loss: 1.238e+04   Precision: 76.502%   Recall: 68.129%
Valid                   Loss:    2617   Precision: 35.602%   Recall: 24.908%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.60870743 93.33333333]
	 [59.15287781 25.76666667]
	 [29.43825912 28.68333333]
	 [34.71310425 25.55      ]
	 [34.67228699 10.13333333]]
Train   Epoch: 375 / 450   Loss: 1.45e+04   Precision: 73.726%   Recall: 66.367%
Valid                   Loss:    2717   Precision: 37.809%   Recall: 39.194%
Train   Epoch: 376 / 450   Loss: 1.345e+04   Precision: 75.131%   Recall: 65.523%
Valid                   Loss:    2680   Precision: 33.437%   Recall: 39.560%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.73154831 15.95      ]
	 [45.44440079 36.05      ]
	 [78.08391571 78.13333333]
	 [56.81932449 13.11666667]
	 [39.86902618 52.28333333]]
Train   Epoch: 377 / 450   Loss: 1.296e+04   Precision: 76.022%   Recall: 67.513%
Valid                   Loss:    2505   Precision: 40.000%   Recall: 29.304%
Train   Epoch: 378 / 450   Loss: 1.229e+04   Precision: 76.080%   Recall: 67.453%
Valid                   Loss:    2495   Precision: 41.081%   Recall: 27.839%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.16984558 85.16666667]
	 [32.40027618 61.25      ]
	 [53.66991425 97.        ]
	 [30.86789513 60.21666667]
	 [55.29364014 46.66666667]]
Train   Epoch: 379 / 450   Loss: 1.252e+04   Precision: 75.965%   Recall: 67.620%
Valid                   Loss:    3249   Precision: 40.476%   Recall: 24.908%
Train   Epoch: 380 / 450   Loss: 1.247e+04   Precision: 76.397%   Recall: 66.756%
Valid                   Loss:    3482   Precision: 42.609%   Recall: 17.949%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.44065857 38.01666667]
	 [46.34114456 40.75      ]
	 [15.70871353 28.35      ]
	 [34.27794647 41.58333333]
	 [35.93891907 23.9       ]]
Train   Epoch: 381 / 450   Loss: 1.251e+04   Precision: 75.924%   Recall: 66.769%
Valid                   Loss:    2340   Precision: 41.000%   Recall: 30.037%
Train   Epoch: 382 / 450   Loss: 1.269e+04   Precision: 76.815%   Recall: 67.841%
Valid                   Loss:    2994   Precision: 31.608%   Recall: 42.491%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.82114792 38.38333333]
	 [36.49458694 12.6       ]
	 [77.69359589 31.58333333]
	 [42.2226181  59.7       ]
	 [33.98219299 39.55      ]]
Train   Epoch: 383 / 450   Loss: 1.334e+04   Precision: 76.880%   Recall: 67.513%
Valid                   Loss:    3193   Precision: 35.683%   Recall: 29.670%
Train   Epoch: 384 / 450   Loss: 1.34e+04   Precision: 75.872%   Recall: 67.466%
Valid                   Loss:    3204   Precision: 29.505%   Recall: 47.985%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.7741394   30.38333333]
	 [ 73.38398743  46.91666667]
	 [ 32.64428711  16.28333333]
	 [150.69619751 140.75      ]
	 [ 28.56526375  10.83333333]]
Train   Epoch: 385 / 450   Loss: 1.201e+04   Precision: 77.369%   Recall: 67.694%
Valid                   Loss:    2619   Precision: 38.235%   Recall: 28.571%
Train   Epoch: 386 / 450   Loss: 1.274e+04   Precision: 77.449%   Recall: 67.794%
Valid                   Loss:    2650   Precision: 40.385%   Recall: 30.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[14.88641262 27.        ]
	 [37.56482315 61.75      ]
	 [19.8266716  19.55      ]
	 [40.40896988 29.        ]
	 [36.98749542 26.45      ]]
Train   Epoch: 387 / 450   Loss: 1.121e+04   Precision: 76.319%   Recall: 67.225%
Valid                   Loss:    2825   Precision: 42.069%   Recall: 22.344%
Train   Epoch: 388 / 450   Loss: 1.077e+04   Precision: 77.368%   Recall: 67.573%
Valid                   Loss:    3445   Precision: 30.476%   Recall: 46.886%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.13885498 122.75      ]
	 [ 39.51965714  63.16666667]
	 [ 56.37690735  34.41666667]
	 [137.27322388 167.45      ]
	 [ 32.78004456  44.        ]]
Train   Epoch: 389 / 450   Loss: 1.209e+04   Precision: 77.352%   Recall: 67.808%
Valid                   Loss:    2892   Precision: 36.697%   Recall: 29.304%
Train   Epoch: 390 / 450   Loss: 1.208e+04   Precision: 77.062%   Recall: 67.848%
Valid                   Loss:    2393   Precision: 41.899%   Recall: 27.473%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.41010284 17.        ]
	 [69.80519867 85.28333333]
	 [30.82850266 34.66666667]
	 [36.50546646 35.        ]
	 [44.19809341 60.45      ]]
Train   Epoch: 391 / 450   Loss: 1.164e+04   Precision: 77.026%   Recall: 68.699%
Valid                   Loss:    2921   Precision: 37.870%   Recall: 23.443%
Train   Epoch: 392 / 450   Loss: 1.259e+04   Precision: 74.255%   Recall: 66.749%
Valid                   Loss:    2552   Precision: 28.495%   Recall: 38.828%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.81959915 24.95      ]
	 [20.49088287  9.66666667]
	 [44.45428085 14.        ]
	 [45.94570541 63.65      ]
	 [60.35331726 62.71666667]]
Train   Epoch: 393 / 450   Loss: 1.203e+04   Precision: 77.118%   Recall: 67.995%
Valid                   Loss:    2963   Precision: 42.105%   Recall: 23.443%
Train   Epoch: 394 / 450   Loss: 1.18e+04   Precision: 75.545%   Recall: 68.471%
Valid                   Loss:    2618   Precision: 38.854%   Recall: 22.344%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 59.68873215  49.01666667]
	 [ 85.97236633 131.9       ]
	 [ 44.3041153   36.41666667]
	 [ 43.85365677  29.95      ]
	 [ 54.04554367  80.26666667]]
Train   Epoch: 395 / 450   Loss: 1.238e+04   Precision: 76.452%   Recall: 65.349%
Valid                   Loss:    2943   Precision: 35.857%   Recall: 32.967%
Train   Epoch: 396 / 450   Loss: 1.225e+04   Precision: 75.988%   Recall: 67.238%
Valid                   Loss:    2800   Precision: 39.053%   Recall: 24.176%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.38819885  78.23333333]
	 [ 94.21273041  52.3       ]
	 [ 31.16485405  72.95      ]
	 [ 33.77515793  29.        ]
	 [212.92657471  88.2       ]]
Train   Epoch: 397 / 450   Loss: 1.227e+04   Precision: 76.797%   Recall: 67.553%
Valid                   Loss:    3541   Precision: 35.294%   Recall: 17.582%
Train   Epoch: 398 / 450   Loss: 1.195e+04   Precision: 73.790%   Recall: 66.193%
Valid                   Loss:    2932   Precision: 35.030%   Recall: 42.857%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.49925613 18.2       ]
	 [41.00924683 32.43333333]
	 [92.43008423 54.16666667]
	 [72.96179199  4.91666667]
	 [27.00000191 19.38333333]]
Train   Epoch: 399 / 450   Loss: 1.483e+04   Precision: 72.412%   Recall: 67.198%
Valid                   Loss:    3170   Precision: 34.673%   Recall: 25.275%
Train   Epoch: 400 / 450   Loss: 1.147e+04   Precision: 76.479%   Recall: 68.129%
Valid                   Loss:    3423   Precision: 28.611%   Recall: 37.729%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.85762787 110.5       ]
	 [ 53.97027588  28.25      ]
	 [ 89.58190918  34.08333333]
	 [516.49664307 384.96666667]
	 [115.09714508 107.76666667]]
Train   Epoch: 401 / 450   Loss: 1.248e+04   Precision: 75.681%   Recall: 67.185%
Valid                   Loss:    3185   Precision: 36.893%   Recall: 27.839%
Train   Epoch: 402 / 450   Loss: 1.194e+04   Precision: 76.850%   Recall: 67.754%
Valid                   Loss:    3417   Precision: 39.535%   Recall: 24.908%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[30.90413475 37.86666667]
	 [45.1449585  29.5       ]
	 [44.22162628 20.33333333]
	 [32.77147675 29.725     ]
	 [21.46043587 22.16666667]]
Train   Epoch: 403 / 450   Loss: 1.07e+04   Precision: 78.484%   Recall: 68.364%
Valid                   Loss:    3106   Precision: 34.310%   Recall: 30.037%
Train   Epoch: 404 / 450   Loss: 1.062e+04   Precision: 77.975%   Recall: 69.080%
Valid                   Loss:    2968   Precision: 44.654%   Recall: 26.007%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 23.14373779  21.21666667]
	 [ 56.70030975 101.05      ]
	 [ 26.33959579  24.33333333]
	 [ 28.31084633  47.11666667]
	 [ 61.04956055  36.88333333]]
Train   Epoch: 405 / 450   Loss: 1.261e+04   Precision: 74.254%   Recall: 68.330%
Valid                   Loss:    3390   Precision: 38.785%   Recall: 30.403%
Train   Epoch: 406 / 450   Loss: 1.073e+04   Precision: 77.424%   Recall: 67.332%
Valid                   Loss:    2857   Precision: 41.899%   Recall: 27.473%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 30.76540375  16.46666667]
	 [ 29.25564766  49.45      ]
	 [ 44.88188553 111.16666667]
	 [129.0632019  100.53333333]
	 [ 38.88591766  70.63333333]]
Train   Epoch: 407 / 450   Loss: 1.128e+04   Precision: 77.882%   Recall: 68.330%
Valid                   Loss:    3309   Precision: 43.165%   Recall: 21.978%
Train   Epoch: 408 / 450   Loss: 1.158e+04   Precision: 77.365%   Recall: 67.332%
Valid                   Loss:    2866   Precision: 37.563%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[76.31406403 20.91666667]
	 [37.42938614 38.86666667]
	 [24.33721542 51.33333333]
	 [74.56848145 37.        ]
	 [62.37057877 59.        ]]
Train   Epoch: 409 / 450   Loss: 1.223e+04   Precision: 78.479%   Recall: 67.761%
Valid                   Loss:    2662   Precision: 43.503%   Recall: 28.205%
Train   Epoch: 410 / 450   Loss: 1.003e+04   Precision: 78.517%   Recall: 68.377%
Valid                   Loss:    3371   Precision: 37.379%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.34062576 55.        ]
	 [40.47422028 33.98      ]
	 [20.95187187 29.81666667]
	 [33.69261932 60.45      ]
	 [54.89525604 65.51666667]]
Train   Epoch: 411 / 450   Loss: 1.097e+04   Precision: 77.449%   Recall: 68.163%
Valid                   Loss:    2784   Precision: 36.946%   Recall: 27.473%
Train   Epoch: 412 / 450   Loss: 1.16e+04   Precision: 76.527%   Recall: 68.551%
Valid                   Loss:    3350   Precision: 44.366%   Recall: 23.077%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.45837784 97.85      ]
	 [45.16457748 44.55      ]
	 [25.70515823 65.81666667]
	 [30.63786697 21.66666667]
	 [41.54851151 33.33333333]]
Train   Epoch: 413 / 450   Loss: 1.051e+04   Precision: 78.207%   Recall: 69.589%
Valid                   Loss:    3127   Precision: 41.221%   Recall: 19.780%
Train   Epoch: 414 / 450   Loss: 1.177e+04   Precision: 77.175%   Recall: 69.100%
Valid                   Loss:    3289   Precision: 41.294%   Recall: 30.403%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.15998077  19.61666667]
	 [ 70.56279755  23.55      ]
	 [ 92.63898468 131.41666667]
	 [ 27.63079834  23.8       ]
	 [ 43.77767944  24.68333333]]
Train   Epoch: 415 / 450   Loss: 1.054e+04   Precision: 77.832%   Recall: 68.203%
Valid                   Loss:    3235   Precision: 39.831%   Recall: 17.216%
Train   Epoch: 416 / 450   Loss: 1.119e+04   Precision: 78.014%   Recall: 68.786%
Valid                   Loss:    2606   Precision: 41.892%   Recall: 34.066%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  49.84085464   33.11666667]
	 [1089.58398438 1105.46666667]
	 [  75.06759644   62.78333333]
	 [  83.17520142  106.38333333]
	 [  10.46000862   19.25      ]]
Train   Epoch: 417 / 450   Loss: 1.033e+04   Precision: 77.417%   Recall: 67.901%
Valid                   Loss:    3184   Precision: 36.000%   Recall: 23.077%
Train   Epoch: 418 / 450   Loss: 1.189e+04   Precision: 77.237%   Recall: 69.321%
Valid                   Loss:    3737   Precision: 41.538%   Recall: 29.670%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.96242905  24.8       ]
	 [700.9251709  569.46666667]
	 [ 15.62521648  20.5       ]
	 [ 34.69260406  45.08333333]
	 [ 48.76527405  45.83333333]]
Train   Epoch: 419 / 450   Loss: 1.116e+04   Precision: 77.497%   Recall: 67.821%
Valid                   Loss:    2883   Precision: 35.897%   Recall: 30.769%
Train   Epoch: 420 / 450   Loss: 1.005e+04   Precision: 78.475%   Recall: 67.620%
Valid                   Loss:    3291   Precision: 38.251%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 39.74391174  58.66666667]
	 [ 33.29660034  22.56666667]
	 [ 36.28345108  15.6       ]
	 [105.32626343  45.56666667]
	 [ 42.31492233  64.61666667]]
Train   Epoch: 421 / 450   Loss: 1.106e+04   Precision: 78.731%   Recall: 67.988%
Valid                   Loss:    2727   Precision: 41.772%   Recall: 24.176%
Train   Epoch: 422 / 450   Loss: 1.157e+04   Precision: 77.746%   Recall: 68.283%
Valid                   Loss:    3300   Precision: 39.000%   Recall: 28.571%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.72688293  19.15      ]
	 [ 28.09796906  46.7       ]
	 [ 40.31698608 101.78333333]
	 [ 69.83148956 115.56666667]
	 [ 47.99167633  35.11666667]]
Train   Epoch: 423 / 450   Loss: 1.205e+04   Precision: 75.048%   Recall: 67.493%
Valid                   Loss:    2621   Precision: 38.542%   Recall: 40.659%
Train   Epoch: 424 / 450   Loss: 1.115e+04   Precision: 75.356%   Recall: 68.002%
Valid                   Loss:    4085   Precision: 36.250%   Recall: 31.868%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.16053391 48.9       ]
	 [17.96781349 30.96666667]
	 [29.16944695 43.45      ]
	 [39.24407959 39.16666667]
	 [39.87483978 37.13333333]]
Train   Epoch: 425 / 450   Loss: 1.202e+04   Precision: 76.918%   Recall: 66.676%
Valid                   Loss:    2869   Precision: 42.574%   Recall: 31.502%
Train   Epoch: 426 / 450   Loss: 1.027e+04   Precision: 76.780%   Recall: 67.687%
Valid                   Loss:    2515   Precision: 44.375%   Recall: 26.007%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.89929581 21.        ]
	 [24.95480156 31.63333333]
	 [39.92776489 37.        ]
	 [60.95282745 49.        ]
	 [39.00520706 28.75      ]]
Train   Epoch: 427 / 450   Loss:    9946   Precision: 77.212%   Recall: 67.794%
Valid                   Loss:    2815   Precision: 44.444%   Recall: 26.374%
Train   Epoch: 428 / 450   Loss:    9197   Precision: 79.380%   Recall: 68.899%
Valid                   Loss:    4101   Precision: 37.278%   Recall: 23.077%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 94.42067719 131.71666667]
	 [ 36.80929565  31.01666667]
	 [ 30.79166031  51.38333333]
	 [ 70.70982361  44.05      ]
	 [ 31.3455162   37.05      ]]
Train   Epoch: 429 / 450   Loss: 1.099e+04   Precision: 78.975%   Recall: 69.295%
Valid                   Loss:    3745   Precision: 36.216%   Recall: 24.542%
Train   Epoch: 430 / 450   Loss:    9840   Precision: 78.962%   Recall: 68.759%
Valid                   Loss:    2782   Precision: 42.466%   Recall: 22.711%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.64686584 28.58333333]
	 [31.5166378  82.63333333]
	 [42.12505341 21.11666667]
	 [37.40216827 33.78333333]
	 [17.8646698  16.75      ]]
Train   Epoch: 431 / 450   Loss:    9916   Precision: 79.020%   Recall: 68.571%
Valid                   Loss:    2792   Precision: 41.830%   Recall: 23.443%
Train   Epoch: 432 / 450   Loss: 1.003e+04   Precision: 78.817%   Recall: 68.189%
Valid                   Loss:    3068   Precision: 45.139%   Recall: 23.810%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.61988831 38.05      ]
	 [29.92829704 34.        ]
	 [84.93658447 45.11666667]
	 [29.8689785  34.26666667]
	 [26.17223549 62.86666667]]
Train   Epoch: 433 / 450   Loss:    9877   Precision: 79.438%   Recall: 69.275%
Valid                   Loss:    3334   Precision: 35.200%   Recall: 32.234%
Train   Epoch: 434 / 450   Loss:    9474   Precision: 78.417%   Recall: 69.094%
Valid                   Loss:    3007   Precision: 42.424%   Recall: 30.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.43873215 48.35      ]
	 [14.16928101 23.71666667]
	 [26.62633896 32.26666667]
	 [44.52781677 44.11666667]
	 [34.89266586 26.15      ]]
Train   Epoch: 435 / 450   Loss:    9931   Precision: 79.431%   Recall: 69.429%
Valid                   Loss:    3473   Precision: 43.966%   Recall: 18.681%
Train   Epoch: 436 / 450   Loss: 1.055e+04   Precision: 79.054%   Recall: 68.712%
Valid                   Loss:    3068   Precision: 36.800%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[120.16917419 144.55      ]
	 [ 40.41850281  43.88333333]
	 [ 69.54598999  50.83333333]
	 [ 40.29944992  46.36666667]
	 [ 36.76852036  20.25      ]]
Train   Epoch: 437 / 450   Loss: 1.097e+04   Precision: 78.540%   Recall: 67.439%
Valid                   Loss:    3934   Precision: 38.397%   Recall: 33.333%
Train   Epoch: 438 / 450   Loss:    9424   Precision: 77.243%   Recall: 68.571%
Valid                   Loss:    3501   Precision: 33.945%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.62296295 49.        ]
	 [38.28610992 37.66666667]
	 [61.25681305 31.26666667]
	 [36.36968231 27.85      ]
	 [36.1636467  53.63333333]]
Train   Epoch: 439 / 450   Loss:    9683   Precision: 78.587%   Recall: 68.390%
Valid                   Loss:    3415   Precision: 43.262%   Recall: 22.344%
Train   Epoch: 440 / 450   Loss:    9544   Precision: 80.490%   Recall: 69.918%
Valid                   Loss:    3237   Precision: 42.442%   Recall: 26.740%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[6.79572761e-01 2.65000000e+01]
	 [3.81956749e+01 5.76666667e+01]
	 [1.06395154e+03 1.06751667e+03]
	 [3.68002548e+01 2.70000000e+01]
	 [4.26041756e+01 3.85333333e+01]]
Train   Epoch: 441 / 450   Loss: 1.006e+04   Precision: 79.261%   Recall: 69.402%
Valid                   Loss:    3247   Precision: 44.094%   Recall: 20.513%
Train   Epoch: 442 / 450   Loss:    9511   Precision: 78.880%   Recall: 69.348%
Valid                   Loss:    3213   Precision: 39.706%   Recall: 29.670%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.73174286  41.45      ]
	 [ 39.80673218  50.68333333]
	 [104.15570831  88.5       ]
	 [ 40.35834122  31.        ]
	 [ 38.95750046  33.33333333]]
Train   Epoch: 443 / 450   Loss: 1.009e+04   Precision: 79.775%   Recall: 68.745%
Valid                   Loss:    3014   Precision: 38.914%   Recall: 31.502%
Train   Epoch: 444 / 450   Loss:    9752   Precision: 79.129%   Recall: 69.000%
Valid                   Loss:    2784   Precision: 42.925%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 26.47097397  22.25      ]
	 [ 77.16229248  42.25      ]
	 [117.3837204   52.96666667]
	 [ 34.20883179  35.33333333]
	 [ 33.91027832  33.        ]]
Train   Epoch: 445 / 450   Loss: 1.033e+04   Precision: 77.970%   Recall: 68.277%
Valid                   Loss:    2974   Precision: 37.500%   Recall: 26.374%
Train   Epoch: 446 / 450   Loss: 1.098e+04   Precision: 79.044%   Recall: 68.544%
Valid                   Loss:    3659   Precision: 40.698%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.98536301  36.5       ]
	 [ 30.96504021  19.71666667]
	 [101.39696503  45.33333333]
	 [ 55.49155807  25.88333333]
	 [ 28.48259926  53.21666667]]
Train   Epoch: 447 / 450   Loss: 1.164e+04   Precision: 78.195%   Recall: 68.437%
Valid                   Loss:    2884   Precision: 43.396%   Recall: 25.275%
Train   Epoch: 448 / 450   Loss: 1.043e+04   Precision: 78.533%   Recall: 69.422%
Valid                   Loss:    3133   Precision: 44.094%   Recall: 20.513%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.88155365  63.21666667]
	 [  7.19022942  17.        ]
	 [ 21.15594292  37.35      ]
	 [164.91970825 256.66666667]
	 [ 31.76291847  46.        ]]
Train   Epoch: 449 / 450   Loss:    8838   Precision: 79.316%   Recall: 68.042%
Valid                   Loss:    3394   Precision: 38.136%   Recall: 16.484%
Train   Epoch: 450 / 450   Loss:    9633   Precision: 78.891%   Recall: 69.268%
Valid                   Loss:    2958   Precision: 38.686%   Recall: 19.414%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 20.1467495   16.36666667]
	 [ 58.64500427 131.66666667]
	 [ 39.88409805  44.33333333]
	 [ 37.94742203  47.53333333]
	 [ 35.61521912  36.16666667]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Presentation set, 450 epochs, lr = 0.00001, no augmentation</span>

<span class="n">results_dir_specific</span> <span class="o">=</span> <span class="n">results_dir</span><span class="o">+</span><span class="s2">&quot;presentation_set_e450_lr0p00001_noaugmentation/&quot;</span>


<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;best_model_state.pt&quot;</span>
<span class="n">last_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;last_model_state.pt&quot;</span>

<span class="n">train_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_losses.pkl&quot;</span>
<span class="n">train_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_lags_losses.pkl&quot;</span>
<span class="n">train_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="n">valid_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_losses.pkl&quot;</span>
<span class="n">valid_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_lags_losses.pkl&quot;</span>
<span class="n">valid_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_delta_lags_losses.pkl&quot;</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_train.pkl&quot;</span><span class="p">))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_valid.pkl&quot;</span><span class="p">))</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">450</span>

<span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span><span class="p">,</span>
 <span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses</span><span class="p">,</span>
 <span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                         <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                                         <span class="n">loss_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">save_best_model_dict_to</span><span class="o">=</span><span class="n">best_model_path</span><span class="p">,</span> 
                                                         <span class="n">save_last_model_dict_to</span><span class="o">=</span><span class="n">last_model_path</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">train_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_delta_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span><span class="n">valid_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_delta_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Weighted loss for presentation - split 5 (easy to see overfitting)</span>

<span class="kn">from</span> <span class="nn">training.dust_loss</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">results_dir_specific</span> <span class="o">=</span> <span class="n">results_dir</span><span class="o">+</span><span class="s2">&quot;weighted_loss_split5/&quot;</span>


<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;best_model_state.pt&quot;</span>
<span class="n">last_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;last_model_state.pt&quot;</span>

<span class="n">train_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_losses.pkl&quot;</span>
<span class="n">train_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_lags_losses.pkl&quot;</span>
<span class="n">train_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="n">valid_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_losses.pkl&quot;</span>
<span class="n">valid_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_lags_losses.pkl&quot;</span>
<span class="n">valid_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_delta_lags_losses.pkl&quot;</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>

<span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span><span class="p">,</span>
 <span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses</span><span class="p">,</span>
 <span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                         <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                                         <span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span> <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">save_best_model_dict_to</span><span class="o">=</span><span class="n">best_model_path</span><span class="p">,</span> 
                                                         <span class="n">save_last_model_dict_to</span><span class="o">=</span><span class="n">last_model_path</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">train_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_delta_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span><span class="n">valid_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_delta_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 600   Loss: 2.07e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.982e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 600   Loss: 2.022e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.955e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 11.82713032  40.05      ]
	 [ 11.8275671   40.73333333]
	 [ 11.82759285  24.85      ]
	 [ 11.82873249 111.83333333]
	 [ 11.82706928  31.13333333]]
Train   Epoch: 003 / 600   Loss: 2.035e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.927e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 600   Loss: 1.868e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.9e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[17.21795082 60.96666667]
	 [17.2177887  40.03333333]
	 [17.21830368 20.5       ]
	 [17.21704483 92.8       ]
	 [17.21801376 38.68333333]]
Train   Epoch: 005 / 600   Loss: 1.901e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.874e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 006 / 600   Loss: 1.829e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.85e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[22.949646   25.86666667]
	 [22.95074463 32.65      ]
	 [22.94981003 37.        ]
	 [22.94971466 45.58333333]
	 [22.94888115 41.88333333]]
Train   Epoch: 007 / 600   Loss: 1.852e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.828e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 600   Loss: 1.664e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.808e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 28.76524353  51.08333333]
	 [ 28.76355934  37.88333333]
	 [ 28.76438904  22.23333333]
	 [ 28.76378441  41.13333333]
	 [ 28.76371574 101.25      ]]
Train   Epoch: 009 / 600   Loss: 1.75e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.79e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 600   Loss: 1.755e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.774e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.27450943  77.66666667]
	 [ 34.27553558  40.81666667]
	 [ 34.27507401  18.05      ]
	 [ 34.27682114 175.7       ]
	 [ 34.27589417  17.91666667]]
Train   Epoch: 011 / 600   Loss: 1.669e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.76e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 600   Loss: 1.681e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.748e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.23622894 106.8       ]
	 [ 40.23598862  41.28333333]
	 [ 40.23626709  18.25      ]
	 [ 40.23633575  15.08333333]
	 [ 40.23465347  99.51666667]]
Train   Epoch: 013 / 600   Loss: 1.742e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.737e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 600   Loss: 1.831e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.728e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 45.74372482  43.08333333]
	 [ 45.74323654  17.        ]
	 [ 45.74413681  38.16666667]
	 [ 45.74497604  43.28333333]
	 [ 45.74323273 132.36666667]]
Train   Epoch: 015 / 600   Loss: 1.695e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.721e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 016 / 600   Loss: 1.637e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.716e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.26974487  65.75      ]
	 [ 51.27103043 105.76666667]
	 [ 51.27024078  47.78333333]
	 [ 51.27000427  16.3       ]
	 [ 51.26995087 124.25      ]]
Train   Epoch: 017 / 600   Loss: 1.671e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.711e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 600   Loss: 1.738e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.708e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.37742233 71.33333333]
	 [56.37776184 47.36666667]
	 [56.3765831  69.26666667]
	 [56.37863922 28.06666667]
	 [56.37749863 27.16666667]]
Train   Epoch: 019 / 600   Loss: 1.568e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.707e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 020 / 600   Loss: 1.568e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.705e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.86823273 35.86666667]
	 [60.86940384 15.95      ]
	 [60.8667717  38.66666667]
	 [60.86859131 37.31666667]
	 [60.86923599 32.03333333]]
Train   Epoch: 021 / 600   Loss: 1.641e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.705e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 022 / 600   Loss: 1.533e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.706e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[65.31707764 70.06666667]
	 [65.31625366 61.96666667]
	 [65.3171463   9.15      ]
	 [65.31678772 47.88333333]
	 [65.31669617 71.63333333]]
Train   Epoch: 023 / 600   Loss: 1.62e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.707e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 024 / 600   Loss: 1.573e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.709e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[69.21699524 52.76666667]
	 [69.21749115 50.95      ]
	 [69.21774292 68.88333333]
	 [69.21789551 58.73333333]
	 [69.21782684 42.66666667]]
Train   Epoch: 025 / 600   Loss: 1.559e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.71e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 026 / 600   Loss: 1.574e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.712e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[72.52004242 28.08333333]
	 [72.51948547 37.28333333]
	 [72.52076721 44.86666667]
	 [72.52023315 34.05      ]
	 [72.51992798 20.66666667]]
Train   Epoch: 027 / 600   Loss: 1.531e+04   Precision: 39.802%   Recall: 46.261%
Valid                   Loss: 1.715e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 028 / 600   Loss: 1.515e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.717e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[75.86663055 11.41666667]
	 [75.86669159 14.95      ]
	 [75.86730957 21.        ]
	 [75.86617279 51.5       ]
	 [75.86700439  9.58333333]]
Train   Epoch: 029 / 600   Loss: 1.544e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.719e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 030 / 600   Loss: 1.559e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.721e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.18544769 63.48333333]
	 [78.18615723 35.        ]
	 [78.18640137 16.3       ]
	 [78.186203   25.3       ]
	 [78.18703461 34.        ]]
Train   Epoch: 031 / 600   Loss: 1.513e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.723e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 032 / 600   Loss: 1.688e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.725e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.44456482 14.3       ]
	 [80.44367981 72.83333333]
	 [80.44329834 29.83333333]
	 [80.44346619  6.        ]
	 [80.443573   72.33333333]]
Train   Epoch: 033 / 600   Loss: 1.566e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.727e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 034 / 600   Loss: 1.529e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.729e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 82.36966705  31.        ]
	 [ 82.36877441  21.96666667]
	 [ 82.3688736  118.11666667]
	 [ 82.36932373  15.56666667]
	 [ 82.36925507  32.66666667]]
Train   Epoch: 035 / 600   Loss: 1.553e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.73e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 036 / 600   Loss: 1.611e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.732e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 83.93520355  26.95      ]
	 [ 83.93724823  56.35      ]
	 [ 83.93727875  20.5       ]
	 [ 83.93702698 415.26666667]
	 [ 83.93647003  25.78333333]]
Train   Epoch: 037 / 600   Loss: 1.559e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.733e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 038 / 600   Loss: 1.531e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.735e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[85.12888336 40.55      ]
	 [85.12905121 93.35      ]
	 [85.12827301 77.36666667]
	 [85.12971497 42.33333333]
	 [85.12858582 19.36666667]]
Train   Epoch: 039 / 600   Loss: 1.585e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.736e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 040 / 600   Loss: 1.626e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.737e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.34418488 29.8       ]
	 [86.3453064  39.45      ]
	 [86.34532166 26.41666667]
	 [86.3454895  23.16666667]
	 [86.34527588 58.78333333]]
Train   Epoch: 041 / 600   Loss: 1.533e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.737e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 042 / 600   Loss: 1.475e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.737e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.59510803 12.66666667]
	 [86.5381012  49.78333333]
	 [86.5164566  53.88333333]
	 [86.56919098 14.55      ]
	 [86.53800201 16.        ]]
Train   Epoch: 043 / 600   Loss: 1.51e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.738e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 044 / 600   Loss: 1.471e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.739e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.52494049 25.5       ]
	 [87.54411316 42.88333333]
	 [87.57493591 19.75      ]
	 [87.54384613 64.75      ]
	 [87.4944458  34.83333333]]
Train   Epoch: 045 / 600   Loss: 1.552e+04   Precision: 39.934%   Recall: 99.565%
Valid                   Loss: 1.74e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 046 / 600   Loss: 1.59e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.741e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.01596832  33.5       ]
	 [ 88.01283264 138.16666667]
	 [ 88.01473236  37.78333333]
	 [ 88.01772308  38.16666667]
	 [ 88.0134201   26.36666667]]
Train   Epoch: 047 / 600   Loss: 1.559e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.741e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 048 / 600   Loss: 1.636e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.741e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.39295959 38.88333333]
	 [88.39148712 43.2       ]
	 [88.39498138 23.28333333]
	 [88.39028168 51.33333333]
	 [88.39424896 32.        ]]
Train   Epoch: 049 / 600   Loss: 1.626e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.742e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 050 / 600   Loss: 1.598e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.741e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.58875275  29.73333333]
	 [ 88.5758667   30.        ]
	 [ 88.63439941   7.16666667]
	 [ 88.62609863 273.16666667]
	 [ 88.56090546  34.2       ]]
Train   Epoch: 051 / 600   Loss: 1.662e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.742e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 052 / 600   Loss: 1.54e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.74e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.43101501 188.75      ]
	 [ 88.16760254  43.95      ]
	 [ 88.36924744  49.61666667]
	 [ 88.27428436  13.9       ]
	 [ 88.34644318  28.83333333]]
Train   Epoch: 053 / 600   Loss: 1.586e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.741e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 054 / 600   Loss: 1.548e+04   Precision: 40.300%   Recall: 98.030%
Valid                   Loss: 1.746e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.94056702 56.11666667]
	 [89.88072205 18.28333333]
	 [89.95014191 22.5       ]
	 [89.86857605 63.7       ]
	 [89.94550323 86.86666667]]
Train   Epoch: 055 / 600   Loss: 1.599e+04   Precision: 40.709%   Recall: 94.796%
Valid                   Loss: 1.751e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 056 / 600   Loss: 1.537e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.74e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.17208099 40.76666667]
	 [87.86424255 15.75      ]
	 [88.29856873 23.38333333]
	 [88.57714844 23.21666667]
	 [88.57668304 62.71666667]]
Train   Epoch: 057 / 600   Loss: 1.595e+04   Precision: 39.843%   Recall: 99.970%
Valid                   Loss: 1.719e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 058 / 600   Loss: 1.53e+04   Precision: 45.083%   Recall: 77.456%
Valid                   Loss: 1.724e+04   Precision: 14.473%   Recall: 99.165%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.78067017 51.08333333]
	 [85.11865997 33.46666667]
	 [73.58010864 45.        ]
	 [74.00073242 48.91666667]
	 [89.25483704 29.2       ]]
Train   Epoch: 059 / 600   Loss: 1.522e+04   Precision: 45.843%   Recall: 70.766%
Valid                   Loss: 1.733e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 060 / 600   Loss: 1.492e+04   Precision: 48.431%   Recall: 67.078%
Valid                   Loss: 1.716e+04   Precision: 16.176%   Recall: 96.661%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[72.05850983 18.        ]
	 [85.15761566 30.86666667]
	 [78.60668182 38.05      ]
	 [88.95754242 49.25      ]
	 [84.26917267 40.41666667]]
Train   Epoch: 061 / 600   Loss: 1.598e+04   Precision: 49.459%   Recall: 63.278%
Valid                   Loss: 1.706e+04   Precision: 19.240%   Recall: 87.835%
Train   Epoch: 062 / 600   Loss: 1.509e+04   Precision: 50.847%   Recall: 65.794%
Valid                   Loss: 1.687e+04   Precision: 23.889%   Recall: 53.190%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 75.77519226  15.76666667]
	 [ 99.91213226  15.48333333]
	 [ 65.60004425  13.46666667]
	 [104.90067291  20.83333333]
	 [ 63.7088356   43.21666667]]
Train   Epoch: 063 / 600   Loss: 1.525e+04   Precision: 51.355%   Recall: 64.733%
Valid                   Loss: 1.685e+04   Precision: 24.433%   Recall: 63.626%
Train   Epoch: 064 / 600   Loss: 1.517e+04   Precision: 53.143%   Recall: 64.834%
Valid                   Loss: 1.696e+04   Precision: 20.515%   Recall: 82.707%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.84098053 39.4       ]
	 [74.57432556 15.66666667]
	 [74.03948975 76.        ]
	 [64.45641327 37.        ]
	 [65.32189941 24.25      ]]
Train   Epoch: 065 / 600   Loss: 1.543e+04   Precision: 52.363%   Recall: 71.099%
Valid                   Loss: 1.685e+04   Precision: 23.397%   Recall: 73.345%
Train   Epoch: 066 / 600   Loss: 1.569e+04   Precision: 53.058%   Recall: 68.209%
Valid                   Loss: 1.692e+04   Precision: 22.206%   Recall: 81.395%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[117.18065643  31.35      ]
	 [ 58.53829575  16.83333333]
	 [115.42978668 140.        ]
	 [ 58.3870697   60.        ]
	 [ 93.12232208  21.        ]]
Train   Epoch: 067 / 600   Loss: 1.473e+04   Precision: 54.430%   Recall: 70.958%
Valid                   Loss: 1.694e+04   Precision: 21.321%   Recall: 84.675%
Train   Epoch: 068 / 600   Loss: 1.465e+04   Precision: 53.396%   Recall: 68.947%
Valid                   Loss: 1.7e+04   Precision: 21.000%   Recall: 86.643%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.92526245  23.35      ]
	 [ 53.03295517  24.3       ]
	 [122.38610077  16.5       ]
	 [ 92.87376404  24.21666667]
	 [ 65.23992157  29.36666667]]
Train   Epoch: 069 / 600   Loss: 1.524e+04   Precision: 54.804%   Recall: 70.897%
Valid                   Loss: 1.678e+04   Precision: 24.142%   Recall: 70.900%
Train   Epoch: 070 / 600   Loss: 1.468e+04   Precision: 55.658%   Recall: 70.928%
Valid                   Loss: 1.677e+04   Precision: 25.081%   Recall: 69.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.89609528  55.28333333]
	 [128.31135559 242.88333333]
	 [ 48.66237259  58.33333333]
	 [128.37506104  36.35      ]
	 [ 44.44205856  45.83333333]]
Train   Epoch: 071 / 600   Loss: 1.486e+04   Precision: 54.616%   Recall: 71.625%
Valid                   Loss: 1.689e+04   Precision: 23.727%   Recall: 78.056%
Train   Epoch: 072 / 600   Loss: 1.48e+04   Precision: 55.856%   Recall: 71.999%
Valid                   Loss: 1.684e+04   Precision: 24.010%   Recall: 78.831%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.10274887 36.16666667]
	 [65.07253265 40.83333333]
	 [83.66876984 25.83333333]
	 [63.68481445 11.96666667]
	 [45.09601593 42.78333333]]
Train   Epoch: 073 / 600   Loss: 1.58e+04   Precision: 54.408%   Recall: 68.785%
Valid                   Loss: 1.673e+04   Precision: 27.286%   Recall: 65.474%
Train   Epoch: 074 / 600   Loss: 1.469e+04   Precision: 56.117%   Recall: 70.220%
Valid                   Loss: 1.697e+04   Precision: 21.416%   Recall: 86.583%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.5282135  45.46666667]
	 [84.92071533 12.85      ]
	 [87.26280212 37.88333333]
	 [57.97622681 21.        ]
	 [93.45195007 47.38333333]]
Train   Epoch: 075 / 600   Loss: 1.483e+04   Precision: 55.590%   Recall: 70.392%
Valid                   Loss: 1.666e+04   Precision: 27.311%   Recall: 62.016%
Train   Epoch: 076 / 600   Loss: 1.539e+04   Precision: 57.668%   Recall: 64.935%
Valid                   Loss: 1.673e+04   Precision: 24.844%   Recall: 73.584%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.52371979 50.45      ]
	 [57.98522949 28.        ]
	 [91.00547028 47.63333333]
	 [77.71866608 89.5       ]
	 [52.26938248 34.8       ]]
Train   Epoch: 077 / 600   Loss: 1.506e+04   Precision: 56.598%   Recall: 68.179%
Valid                   Loss: 1.68e+04   Precision: 24.811%   Recall: 76.386%
Train   Epoch: 078 / 600   Loss: 1.531e+04   Precision: 57.063%   Recall: 69.846%
Valid                   Loss: 1.685e+04   Precision: 24.780%   Recall: 77.221%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.59303284  12.85      ]
	 [ 73.90144348  24.26666667]
	 [107.4615097   81.58333333]
	 [ 51.29867935  45.5       ]
	 [ 52.95914459  38.5       ]]
Train   Epoch: 079 / 600   Loss: 1.467e+04   Precision: 57.569%   Recall: 68.482%
Valid                   Loss: 1.713e+04   Precision: 21.221%   Recall: 88.312%
Train   Epoch: 080 / 600   Loss: 1.477e+04   Precision: 57.982%   Recall: 68.705%
Valid                   Loss: 1.678e+04   Precision: 23.602%   Recall: 81.813%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 45.73739624  59.55      ]
	 [ 54.9899826   59.73333333]
	 [ 85.79920197  36.21666667]
	 [ 92.56877899 127.21666667]
	 [ 77.13909149  45.        ]]
Train   Epoch: 081 / 600   Loss: 1.423e+04   Precision: 56.452%   Recall: 67.148%
Valid                   Loss: 1.661e+04   Precision: 28.033%   Recall: 57.603%
Train   Epoch: 082 / 600   Loss: 1.338e+04   Precision: 59.286%   Recall: 67.482%
Valid                   Loss: 1.711e+04   Precision: 20.560%   Recall: 88.909%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.61391449  86.7       ]
	 [ 91.00298309  36.        ]
	 [ 93.3716507  332.66666667]
	 [ 69.56880951  26.58333333]
	 [109.35588074  54.33333333]]
Train   Epoch: 083 / 600   Loss: 1.401e+04   Precision: 58.863%   Recall: 69.058%
Valid                   Loss: 1.688e+04   Precision: 23.037%   Recall: 81.336%
Train   Epoch: 084 / 600   Loss: 1.377e+04   Precision: 59.632%   Recall: 65.814%
Valid                   Loss: 1.697e+04   Precision: 22.452%   Recall: 84.854%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 57.76564407  43.5       ]
	 [ 71.16653442  41.21666667]
	 [ 56.54484177  27.11666667]
	 [164.67428589   6.3       ]
	 [ 63.70301437  34.05      ]]
Train   Epoch: 085 / 600   Loss: 1.389e+04   Precision: 56.062%   Recall: 69.291%
Valid                   Loss: 1.671e+04   Precision: 27.270%   Recall: 62.135%
Train   Epoch: 086 / 600   Loss: 1.475e+04   Precision: 58.708%   Recall: 65.774%
Valid                   Loss: 1.719e+04   Precision: 18.967%   Recall: 94.574%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.87795258  27.58333333]
	 [101.66314697  92.13333333]
	 [ 66.01641846  28.28333333]
	 [ 87.23060608  29.83333333]
	 [121.92016602  35.43333333]]
Train   Epoch: 087 / 600   Loss: 1.484e+04   Precision: 57.974%   Recall: 69.058%
Valid                   Loss: 1.67e+04   Precision: 25.166%   Recall: 74.717%
Train   Epoch: 088 / 600   Loss: 1.395e+04   Precision: 56.855%   Recall: 67.340%
Valid                   Loss: 1.695e+04   Precision: 21.640%   Recall: 85.629%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.14157104 32.16666667]
	 [91.89381409 21.98333333]
	 [78.1335144  55.63333333]
	 [88.43574524 32.68333333]
	 [89.7723999  35.71666667]]
Train   Epoch: 089 / 600   Loss: 1.455e+04   Precision: 59.679%   Recall: 65.764%
Valid                   Loss: 1.713e+04   Precision: 22.978%   Recall: 79.606%
Train   Epoch: 090 / 600   Loss: 1.475e+04   Precision: 57.000%   Recall: 69.200%
Valid                   Loss: 1.667e+04   Precision: 27.017%   Recall: 62.910%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.19519424 31.21666667]
	 [38.166008   34.55      ]
	 [51.53961563 20.        ]
	 [61.11457825 39.33333333]
	 [47.32909012 29.33333333]]
Train   Epoch: 091 / 600   Loss: 1.481e+04   Precision: 57.456%   Recall: 66.037%
Valid                   Loss: 1.666e+04   Precision: 25.623%   Recall: 68.038%
Train   Epoch: 092 / 600   Loss: 1.382e+04   Precision: 58.919%   Recall: 67.522%
Valid                   Loss: 1.671e+04   Precision: 26.880%   Recall: 68.634%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.65694427 47.56666667]
	 [81.54036713 41.55      ]
	 [47.41093826 35.55      ]
	 [68.34751129 36.08333333]
	 [66.15123749 24.83333333]]
Train   Epoch: 093 / 600   Loss: 1.437e+04   Precision: 59.742%   Recall: 67.795%
Valid                   Loss: 1.673e+04   Precision: 27.779%   Recall: 61.538%
Train   Epoch: 094 / 600   Loss: 1.476e+04   Precision: 59.936%   Recall: 64.339%
Valid                   Loss: 1.667e+04   Precision: 24.601%   Recall: 70.781%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[126.373909    57.08333333]
	 [ 58.12765503  38.56666667]
	 [ 46.27374649  50.05      ]
	 [ 61.62677002  57.76666667]
	 [ 48.42090225  64.66666667]]
Train   Epoch: 095 / 600   Loss: 1.425e+04   Precision: 59.801%   Recall: 66.037%
Valid                   Loss: 1.665e+04   Precision: 25.873%   Recall: 72.451%
Train   Epoch: 096 / 600   Loss: 1.528e+04   Precision: 60.075%   Recall: 67.785%
Valid                   Loss: 1.663e+04   Precision: 28.650%   Recall: 58.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.02029037 18.2       ]
	 [66.55405426 24.2       ]
	 [62.37602615 15.46666667]
	 [52.77828979 28.58333333]
	 [44.16200256 41.05      ]]
Train   Epoch: 097 / 600   Loss: 1.435e+04   Precision: 61.104%   Recall: 65.532%
Valid                   Loss: 1.698e+04   Precision: 21.916%   Recall: 85.689%
Train   Epoch: 098 / 600   Loss: 1.375e+04   Precision: 57.873%   Recall: 66.966%
Valid                   Loss: 1.7e+04   Precision: 22.722%   Recall: 81.336%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.35902786 22.48333333]
	 [64.7950058  17.78333333]
	 [52.6128273  27.33333333]
	 [49.18666077 44.3       ]
	 [59.81719971 36.7       ]]
Train   Epoch: 099 / 600   Loss: 1.427e+04   Precision: 60.809%   Recall: 66.198%
Valid                   Loss: 1.706e+04   Precision: 21.951%   Recall: 83.721%
Train   Epoch: 100 / 600   Loss: 1.48e+04   Precision: 60.401%   Recall: 66.956%
Valid                   Loss: 1.676e+04   Precision: 24.425%   Recall: 76.029%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 58.82546234  42.        ]
	 [ 93.9114151   22.        ]
	 [130.34173584 190.2       ]
	 [ 54.62757111  34.96666667]
	 [ 39.94075394  22.25      ]]
Train   Epoch: 101 / 600   Loss: 1.398e+04   Precision: 58.490%   Recall: 66.522%
Valid                   Loss: 1.657e+04   Precision: 28.021%   Recall: 58.080%
Train   Epoch: 102 / 600   Loss: 1.471e+04   Precision: 60.428%   Recall: 63.884%
Valid                   Loss: 1.716e+04   Precision: 21.489%   Recall: 84.675%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[142.52017212  84.8       ]
	 [ 72.04083252  39.08333333]
	 [134.28337097  17.28333333]
	 [ 55.51898956  45.33333333]
	 [ 56.39725113  35.5       ]]
Train   Epoch: 103 / 600   Loss: 1.417e+04   Precision: 60.078%   Recall: 65.026%
Valid                   Loss: 1.678e+04   Precision: 24.832%   Recall: 70.423%
Train   Epoch: 104 / 600   Loss: 1.411e+04   Precision: 61.724%   Recall: 65.703%
Valid                   Loss: 1.685e+04   Precision: 23.926%   Recall: 77.042%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  86.60975647   33.23333333]
	 [ 119.47653961   52.6       ]
	 [  51.8391304    34.5       ]
	 [  76.12866974   40.45      ]
	 [ 222.0849762  1207.28333333]]
Train   Epoch: 105 / 600   Loss: 1.36e+04   Precision: 59.311%   Recall: 65.461%
Valid                   Loss: 1.651e+04   Precision: 29.643%   Recall: 52.057%
Train   Epoch: 106 / 600   Loss: 1.486e+04   Precision: 59.669%   Recall: 66.694%
Valid                   Loss: 1.659e+04   Precision: 28.433%   Recall: 54.204%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[144.31016541 108.91666667]
	 [ 60.39200592  22.43333333]
	 [ 50.81520462  16.66666667]
	 [ 47.54816055  47.83333333]
	 [ 39.72701263  69.3       ]]
Train   Epoch: 107 / 600   Loss: 1.326e+04   Precision: 61.926%   Recall: 64.147%
Valid                   Loss: 1.655e+04   Precision: 28.704%   Recall: 59.034%
Train   Epoch: 108 / 600   Loss: 1.286e+04   Precision: 60.155%   Recall: 62.854%
Valid                   Loss: 1.675e+04   Precision: 23.968%   Recall: 74.419%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[226.71772766 264.66666667]
	 [ 56.33998108  60.16666667]
	 [ 55.18648529  53.73333333]
	 [ 60.83456039  22.88333333]
	 [ 49.51996994  23.18333333]]
Train   Epoch: 109 / 600   Loss: 1.396e+04   Precision: 61.858%   Recall: 63.440%
Valid                   Loss: 1.768e+04   Precision: 21.468%   Recall: 84.079%
Train   Epoch: 110 / 600   Loss: 1.414e+04   Precision: 59.148%   Recall: 62.985%
Valid                   Loss: 1.692e+04   Precision: 23.525%   Recall: 76.804%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.36301804  36.4       ]
	 [ 86.13330841  24.43333333]
	 [ 52.05329514   9.95      ]
	 [ 49.96945572  15.88333333]
	 [112.03300476  78.15      ]]
Train   Epoch: 111 / 600   Loss: 1.39e+04   Precision: 60.303%   Recall: 64.794%
Valid                   Loss: 1.685e+04   Precision: 27.109%   Recall: 64.758%
Train   Epoch: 112 / 600   Loss: 1.311e+04   Precision: 61.208%   Recall: 64.703%
Valid                   Loss: 1.662e+04   Precision: 28.297%   Recall: 62.314%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.15509033  18.63333333]
	 [162.69801331 158.2       ]
	 [ 47.82561874  36.48333333]
	 [ 73.72214508  25.55      ]
	 [ 64.58386993  33.96666667]]
Train   Epoch: 113 / 600   Loss: 1.352e+04   Precision: 62.963%   Recall: 63.167%
Valid                   Loss: 1.646e+04   Precision: 31.994%   Recall: 48.897%
Train   Epoch: 114 / 600   Loss: 1.334e+04   Precision: 62.102%   Recall: 61.914%
Valid                   Loss: 1.663e+04   Precision: 30.516%   Recall: 54.264%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.85733795 64.58333333]
	 [96.33159637 46.05      ]
	 [47.23065948 38.38333333]
	 [99.60305786 52.78333333]
	 [36.0909996  33.33333333]]
Train   Epoch: 115 / 600   Loss: 1.397e+04   Precision: 60.713%   Recall: 61.277%
Valid                   Loss: 1.706e+04   Precision: 23.337%   Recall: 77.400%
Train   Epoch: 116 / 600   Loss: 1.392e+04   Precision: 54.702%   Recall: 62.015%
Valid                   Loss: 1.688e+04   Precision: 22.140%   Recall: 79.845%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.73394775 38.01666667]
	 [73.17608643 32.25      ]
	 [92.02455139 29.2       ]
	 [83.18998718 21.35      ]
	 [69.81432343 33.66666667]]
Train   Epoch: 117 / 600   Loss: 1.42e+04   Precision: 62.239%   Recall: 59.276%
Valid                   Loss: 1.682e+04   Precision: 22.896%   Recall: 77.042%
Train   Epoch: 118 / 600   Loss: 1.384e+04   Precision: 60.703%   Recall: 63.015%
Valid                   Loss: 1.646e+04   Precision: 29.738%   Recall: 55.575%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.26034546  27.41666667]
	 [ 76.2882843   76.        ]
	 [ 48.71591187 101.33333333]
	 [ 84.41505432  45.4       ]
	 [ 54.73734665  61.66666667]]
Train   Epoch: 119 / 600   Loss: 1.342e+04   Precision: 60.079%   Recall: 63.248%
Valid                   Loss: 1.65e+04   Precision: 27.575%   Recall: 57.782%
Train   Epoch: 120 / 600   Loss: 1.408e+04   Precision: 60.901%   Recall: 64.470%
Valid                   Loss: 1.649e+04   Precision: 28.991%   Recall: 54.144%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.4617157  56.53333333]
	 [62.9071312  32.88333333]
	 [44.05357742 26.33333333]
	 [86.68856049 50.7       ]
	 [68.93536377 25.98333333]]
Train   Epoch: 121 / 600   Loss: 1.331e+04   Precision: 61.782%   Recall: 60.247%
Valid                   Loss: 1.696e+04   Precision: 24.175%   Recall: 77.340%
Train   Epoch: 122 / 600   Loss: 1.339e+04   Precision: 60.582%   Recall: 61.469%
Valid                   Loss: 1.675e+04   Precision: 25.701%   Recall: 63.387%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[138.35067749  82.88333333]
	 [ 44.39955521  36.95      ]
	 [ 35.38220978  20.3       ]
	 [138.14761353  16.75      ]
	 [265.26663208  46.66666667]]
Train   Epoch: 123 / 600   Loss: 1.325e+04   Precision: 60.133%   Recall: 65.724%
Valid                   Loss: 1.692e+04   Precision: 23.370%   Recall: 77.579%
Train   Epoch: 124 / 600   Loss: 1.308e+04   Precision: 62.219%   Recall: 61.873%
Valid                   Loss: 1.653e+04   Precision: 29.534%   Recall: 50.686%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.31995392 42.55      ]
	 [96.16260529 39.15      ]
	 [51.68456268 30.38333333]
	 [45.80233383 27.21666667]
	 [30.67162895 21.75      ]]
Train   Epoch: 125 / 600   Loss: 1.338e+04   Precision: 62.137%   Recall: 61.540%
Valid                   Loss: 1.664e+04   Precision: 25.856%   Recall: 60.763%
Train   Epoch: 126 / 600   Loss: 1.366e+04   Precision: 62.351%   Recall: 61.116%
Valid                   Loss: 1.66e+04   Precision: 26.798%   Recall: 65.772%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.22526169 63.25      ]
	 [48.97703552 39.55      ]
	 [35.81589127 36.8       ]
	 [60.31676483 43.        ]
	 [50.08482742 10.26666667]]
Train   Epoch: 127 / 600   Loss: 1.319e+04   Precision: 62.643%   Recall: 61.308%
Valid                   Loss: 1.723e+04   Precision: 22.443%   Recall: 74.597%
Train   Epoch: 128 / 600   Loss: 1.258e+04   Precision: 62.091%   Recall: 62.763%
Valid                   Loss: 1.677e+04   Precision: 26.359%   Recall: 68.217%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[143.74038696  68.4       ]
	 [100.20806122  42.68333333]
	 [ 68.71417999  30.6       ]
	 [101.9965744   42.03333333]
	 [ 58.66154861  42.21666667]]
Train   Epoch: 129 / 600   Loss: 1.327e+04   Precision: 61.849%   Recall: 62.005%
Valid                   Loss: 1.669e+04   Precision: 26.050%   Recall: 70.662%
Train   Epoch: 130 / 600   Loss: 1.346e+04   Precision: 62.286%   Recall: 63.601%
Valid                   Loss: 1.653e+04   Precision: 29.268%   Recall: 55.814%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.55199051 59.98333333]
	 [47.02500916 35.78333333]
	 [29.86237907 32.78333333]
	 [42.44969559 30.21666667]
	 [89.82842255 65.        ]]
Train   Epoch: 131 / 600   Loss: 1.246e+04   Precision: 62.707%   Recall: 60.166%
Valid                   Loss: 1.685e+04   Precision: 22.741%   Recall: 79.845%
Train   Epoch: 132 / 600   Loss: 1.363e+04   Precision: 62.753%   Recall: 61.035%
Valid                   Loss: 1.728e+04   Precision: 23.600%   Recall: 73.643%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 86.57189178  91.08333333]
	 [130.43000793 136.        ]
	 [ 74.50640106  62.21666667]
	 [ 63.15455246  30.95      ]
	 [ 50.23800659  23.16666667]]
Train   Epoch: 133 / 600   Loss: 1.322e+04   Precision: 62.533%   Recall: 60.024%
Valid                   Loss: 1.686e+04   Precision: 24.421%   Recall: 74.180%
Train   Epoch: 134 / 600   Loss: 1.387e+04   Precision: 62.459%   Recall: 63.147%
Valid                   Loss: 1.667e+04   Precision: 28.278%   Recall: 60.823%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.32363129  17.        ]
	 [ 39.82893753  41.23333333]
	 [133.32104492  24.25      ]
	 [ 46.26640701  21.78333333]
	 [ 66.23725128  53.55      ]]
Train   Epoch: 135 / 600   Loss: 1.23e+04   Precision: 63.523%   Recall: 60.994%
Valid                   Loss: 1.69e+04   Precision: 24.776%   Recall: 74.299%
Train   Epoch: 136 / 600   Loss: 1.375e+04   Precision: 63.315%   Recall: 58.791%
Valid                   Loss: 1.716e+04   Precision: 25.010%   Recall: 75.194%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 56.91343689  27.55      ]
	 [ 58.79961395  17.16666667]
	 [ 79.18708038  20.16666667]
	 [ 79.42285919 120.08333333]
	 [ 71.82710266  14.38333333]]
Train   Epoch: 137 / 600   Loss: 1.365e+04   Precision: 61.162%   Recall: 63.480%
Valid                   Loss: 1.657e+04   Precision: 29.429%   Recall: 54.085%
Train   Epoch: 138 / 600   Loss: 1.278e+04   Precision: 62.995%   Recall: 60.792%
Valid                   Loss: 1.717e+04   Precision: 24.456%   Recall: 71.676%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.74723434  59.21666667]
	 [ 64.68930817  40.76666667]
	 [120.86920166  53.86666667]
	 [ 59.42001343  17.55      ]
	 [ 97.35488129  57.1       ]]
Train   Epoch: 139 / 600   Loss: 1.361e+04   Precision: 63.995%   Recall: 61.388%
Valid                   Loss: 1.693e+04   Precision: 26.822%   Recall: 61.896%
Train   Epoch: 140 / 600   Loss: 1.378e+04   Precision: 62.919%   Recall: 62.257%
Valid                   Loss: 1.667e+04   Precision: 28.417%   Recall: 58.020%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.91937256 55.        ]
	 [43.82619476 29.83333333]
	 [41.86254883 31.46666667]
	 [82.90873718 25.51666667]
	 [60.5074234  11.41666667]]
Train   Epoch: 141 / 600   Loss: 1.257e+04   Precision: 63.273%   Recall: 61.661%
Valid                   Loss: 1.662e+04   Precision: 28.124%   Recall: 58.378%
Train   Epoch: 142 / 600   Loss: 1.318e+04   Precision: 64.659%   Recall: 61.510%
Valid                   Loss: 1.68e+04   Precision: 28.878%   Recall: 57.841%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.35168457 37.41666667]
	 [48.35539246 27.21666667]
	 [97.23410797 66.86666667]
	 [41.62452316 38.        ]
	 [56.66917419 49.85      ]]
Train   Epoch: 143 / 600   Loss: 1.304e+04   Precision: 63.893%   Recall: 61.833%
Valid                   Loss: 1.648e+04   Precision: 29.872%   Recall: 48.539%
Train   Epoch: 144 / 600   Loss: 1.228e+04   Precision: 63.421%   Recall: 62.389%
Valid                   Loss: 1.667e+04   Precision: 29.241%   Recall: 56.947%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 45.39333344  42.11666667]
	 [113.64467621  44.25      ]
	 [ 52.39464951  17.5       ]
	 [ 69.73107147  66.3       ]
	 [ 64.55717468  32.76666667]]
Train   Epoch: 145 / 600   Loss: 1.301e+04   Precision: 63.899%   Recall: 59.024%
Valid                   Loss: 1.648e+04   Precision: 31.291%   Recall: 54.204%
Train   Epoch: 146 / 600   Loss: 1.296e+04   Precision: 62.876%   Recall: 62.520%
Valid                   Loss: 1.672e+04   Precision: 27.502%   Recall: 67.501%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 33.59764099  24.66666667]
	 [100.02857208  52.26666667]
	 [ 56.23829269  40.16666667]
	 [ 61.02906799  60.        ]
	 [ 81.30427551  68.21666667]]
Train   Epoch: 147 / 600   Loss: 1.274e+04   Precision: 63.967%   Recall: 58.892%
Valid                   Loss: 1.649e+04   Precision: 30.486%   Recall: 54.264%
Train   Epoch: 148 / 600   Loss: 1.32e+04   Precision: 61.316%   Recall: 62.338%
Valid                   Loss: 1.66e+04   Precision: 26.314%   Recall: 59.988%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.49681473 43.25      ]
	 [49.8894043  13.05      ]
	 [70.85891724 27.56666667]
	 [51.52345276 46.33333333]
	 [64.24388123 50.75      ]]
Train   Epoch: 149 / 600   Loss: 1.205e+04   Precision: 64.255%   Recall: 59.600%
Valid                   Loss: 1.693e+04   Precision: 26.986%   Recall: 63.804%
Train   Epoch: 150 / 600   Loss: 1.35e+04   Precision: 63.718%   Recall: 62.005%
Valid                   Loss: 1.656e+04   Precision: 29.319%   Recall: 56.470%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.4835968  38.3       ]
	 [71.60286713 46.11666667]
	 [44.18527985 35.5       ]
	 [49.04708862 20.71666667]
	 [44.58707428 21.96666667]]
Train   Epoch: 151 / 600   Loss: 1.259e+04   Precision: 63.532%   Recall: 62.672%
Valid                   Loss: 1.662e+04   Precision: 28.311%   Recall: 60.167%
Train   Epoch: 152 / 600   Loss: 1.352e+04   Precision: 65.133%   Recall: 59.216%
Valid                   Loss: 1.663e+04   Precision: 28.169%   Recall: 62.016%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.11598969  71.71666667]
	 [ 76.9885025   49.96666667]
	 [104.00506592  17.45      ]
	 [ 52.0705452   56.11666667]
	 [ 61.03559875  23.06666667]]
Train   Epoch: 153 / 600   Loss: 1.356e+04   Precision: 64.623%   Recall: 61.671%
Valid                   Loss: 1.742e+04   Precision: 24.387%   Recall: 74.120%
Train   Epoch: 154 / 600   Loss: 1.353e+04   Precision: 63.622%   Recall: 60.671%
Valid                   Loss: 1.708e+04   Precision: 25.847%   Recall: 69.171%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.21734238 16.41666667]
	 [52.68049622 29.26666667]
	 [64.05332184 27.66666667]
	 [48.52554703 52.45      ]
	 [44.36626816 67.28333333]]
Train   Epoch: 155 / 600   Loss: 1.259e+04   Precision: 64.025%   Recall: 60.014%
Valid                   Loss: 1.651e+04   Precision: 30.290%   Recall: 51.640%
Train   Epoch: 156 / 600   Loss: 1.329e+04   Precision: 64.337%   Recall: 61.489%
Valid                   Loss: 1.725e+04   Precision: 22.541%   Recall: 83.781%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.01316833  40.05      ]
	 [117.88690948  34.96666667]
	 [ 84.26926422  33.83333333]
	 [197.35145569 145.66666667]
	 [ 59.82951355  65.13333333]]
Train   Epoch: 157 / 600   Loss: 1.224e+04   Precision: 63.031%   Recall: 60.267%
Valid                   Loss: 1.661e+04   Precision: 29.457%   Recall: 52.415%
Train   Epoch: 158 / 600   Loss: 1.209e+04   Precision: 65.598%   Recall: 58.498%
Valid                   Loss: 1.685e+04   Precision: 23.812%   Recall: 75.611%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.90599442 27.        ]
	 [56.76953125 60.26666667]
	 [68.44878387 33.01666667]
	 [57.82986069 38.16666667]
	 [54.38252258 41.33333333]]
Train   Epoch: 159 / 600   Loss: 1.32e+04   Precision: 63.498%   Recall: 61.419%
Valid                   Loss: 1.67e+04   Precision: 27.341%   Recall: 58.855%
Train   Epoch: 160 / 600   Loss: 1.373e+04   Precision: 63.285%   Recall: 61.085%
Valid                   Loss: 1.684e+04   Precision: 23.552%   Recall: 74.419%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 82.36000824  24.75      ]
	 [109.73042297  29.7       ]
	 [ 59.89887619  25.6       ]
	 [ 61.78514481  49.95      ]
	 [ 65.19876099  18.01666667]]
Train   Epoch: 161 / 600   Loss: 1.193e+04   Precision: 63.588%   Recall: 60.812%
Valid                   Loss: 1.642e+04   Precision: 27.908%   Recall: 63.387%
Train   Epoch: 162 / 600   Loss: 1.172e+04   Precision: 64.633%   Recall: 59.408%
Valid                   Loss: 1.65e+04   Precision: 31.747%   Recall: 45.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.09159851 45.58333333]
	 [46.41160202 19.8       ]
	 [80.28295898 71.75      ]
	 [52.44854736 45.75      ]
	 [40.21018982 51.        ]]
Train   Epoch: 163 / 600   Loss: 1.32e+04   Precision: 63.804%   Recall: 60.509%
Valid                   Loss: 1.669e+04   Precision: 31.211%   Recall: 47.943%
Train   Epoch: 164 / 600   Loss: 1.184e+04   Precision: 65.103%   Recall: 61.267%
Valid                   Loss: 1.658e+04   Precision: 27.591%   Recall: 64.937%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.27005768  63.2       ]
	 [ 46.75328827  25.        ]
	 [204.5055542   69.26666667]
	 [ 36.72972488  25.        ]
	 [ 52.43516541  64.95      ]]
Train   Epoch: 165 / 600   Loss: 1.197e+04   Precision: 65.941%   Recall: 60.338%
Valid                   Loss: 1.641e+04   Precision: 30.310%   Recall: 51.819%
Train   Epoch: 166 / 600   Loss: 1.294e+04   Precision: 65.040%   Recall: 61.267%
Valid                   Loss: 1.688e+04   Precision: 26.975%   Recall: 60.465%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.9209404  92.25      ]
	 [44.57870865 56.9       ]
	 [49.39310455 29.66666667]
	 [74.76921844 27.56666667]
	 [47.98717499 65.21666667]]
Train   Epoch: 167 / 600   Loss: 1.215e+04   Precision: 65.331%   Recall: 60.459%
Valid                   Loss: 1.658e+04   Precision: 29.020%   Recall: 57.364%
Train   Epoch: 168 / 600   Loss: 1.197e+04   Precision: 65.897%   Recall: 60.236%
Valid                   Loss: 1.69e+04   Precision: 27.225%   Recall: 67.859%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.92220306  70.88333333]
	 [ 33.35968399  49.66666667]
	 [ 84.70802307  52.96666667]
	 [ 92.23631287 104.21666667]
	 [ 51.52591324  39.78333333]]
Train   Epoch: 169 / 600   Loss: 1.155e+04   Precision: 66.807%   Recall: 59.145%
Valid                   Loss: 1.684e+04   Precision: 25.726%   Recall: 69.708%
Train   Epoch: 170 / 600   Loss: 1.266e+04   Precision: 63.724%   Recall: 58.791%
Valid                   Loss: 1.634e+04   Precision: 27.722%   Recall: 47.525%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.74237823 39.33333333]
	 [61.28200912 75.        ]
	 [41.65649414 19.        ]
	 [69.32322693 42.5       ]
	 [45.58018494 23.        ]]
Train   Epoch: 171 / 600   Loss: 1.239e+04   Precision: 65.580%   Recall: 56.487%
Valid                   Loss: 1.656e+04   Precision: 27.056%   Recall: 65.534%
Train   Epoch: 172 / 600   Loss: 1.304e+04   Precision: 64.848%   Recall: 62.581%
Valid                   Loss: 1.643e+04   Precision: 30.497%   Recall: 57.066%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.13344955 36.41666667]
	 [39.17647171 55.68333333]
	 [46.07798767 48.21666667]
	 [48.21229553 60.41666667]
	 [85.46078491 67.91666667]]
Train   Epoch: 173 / 600   Loss: 1.198e+04   Precision: 65.017%   Recall: 59.590%
Valid                   Loss: 1.669e+04   Precision: 30.751%   Recall: 46.869%
Train   Epoch: 174 / 600   Loss: 1.114e+04   Precision: 66.431%   Recall: 60.792%
Valid                   Loss: 1.661e+04   Precision: 29.111%   Recall: 57.961%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.08596802  63.5       ]
	 [ 35.15761566   5.55      ]
	 [ 39.42444611  22.21666667]
	 [357.36361694  31.66666667]
	 [ 80.05046082  32.23333333]]
Train   Epoch: 175 / 600   Loss: 1.157e+04   Precision: 67.588%   Recall: 59.782%
Valid                   Loss: 1.699e+04   Precision: 28.622%   Recall: 57.484%
Train   Epoch: 176 / 600   Loss: 1.272e+04   Precision: 64.841%   Recall: 59.802%
Valid                   Loss: 1.641e+04   Precision: 30.138%   Recall: 61.121%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.98873901  22.        ]
	 [ 47.52532196  11.21666667]
	 [ 41.05104446  35.2       ]
	 [414.85317993 881.1       ]
	 [105.31111908 110.28333333]]
Train   Epoch: 177 / 600   Loss: 1.205e+04   Precision: 65.496%   Recall: 59.559%
Valid                   Loss: 1.622e+04   Precision: 31.769%   Recall: 47.227%
Train   Epoch: 178 / 600   Loss: 1.246e+04   Precision: 65.291%   Recall: 59.630%
Valid                   Loss: 1.681e+04   Precision: 27.959%   Recall: 64.937%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.87514496  38.75      ]
	 [ 74.37179565 125.4       ]
	 [ 68.65610504  66.41666667]
	 [ 98.83575439  79.9       ]
	 [ 48.66679001  35.61666667]]
Train   Epoch: 179 / 600   Loss: 1.241e+04   Precision: 65.808%   Recall: 58.114%
Valid                   Loss: 1.707e+04   Precision: 26.172%   Recall: 73.226%
Train   Epoch: 180 / 600   Loss: 1.177e+04   Precision: 66.034%   Recall: 60.802%
Valid                   Loss: 1.697e+04   Precision: 29.719%   Recall: 63.089%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.0454216   24.38333333]
	 [ 84.09716034  81.5       ]
	 [ 76.13339233  26.45      ]
	 [121.78057861  73.75      ]
	 [ 43.19681168  33.1       ]]
Train   Epoch: 181 / 600   Loss: 1.233e+04   Precision: 64.336%   Recall: 60.812%
Valid                   Loss: 1.659e+04   Precision: 28.629%   Recall: 59.273%
Train   Epoch: 182 / 600   Loss: 1.228e+04   Precision: 68.647%   Recall: 57.902%
Valid                   Loss: 1.647e+04   Precision: 28.924%   Recall: 61.419%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.58920288 14.2       ]
	 [62.05745316 19.33333333]
	 [59.68720245 17.1       ]
	 [48.38650513 30.21666667]
	 [68.72886658 24.65      ]]
Train   Epoch: 183 / 600   Loss: 1.14e+04   Precision: 66.428%   Recall: 60.105%
Valid                   Loss: 1.722e+04   Precision: 27.232%   Recall: 64.937%
Train   Epoch: 184 / 600   Loss: 1.188e+04   Precision: 63.308%   Recall: 58.286%
Valid                   Loss: 1.656e+04   Precision: 31.809%   Recall: 45.617%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.98248291  29.66666667]
	 [ 60.51571274 138.56666667]
	 [ 45.26343155  49.2       ]
	 [ 57.69306564  26.05      ]
	 [ 55.44736481  46.46666667]]
Train   Epoch: 185 / 600   Loss: 1.181e+04   Precision: 66.854%   Recall: 57.619%
Valid                   Loss: 1.623e+04   Precision: 33.972%   Recall: 40.191%
Train   Epoch: 186 / 600   Loss: 1.171e+04   Precision: 65.341%   Recall: 61.611%
Valid                   Loss: 1.658e+04   Precision: 27.448%   Recall: 67.024%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.29914093 31.21666667]
	 [47.40756226 61.66666667]
	 [69.81446075 95.68333333]
	 [86.27693176 67.33333333]
	 [41.67219543 24.95      ]]
Train   Epoch: 187 / 600   Loss: 1.145e+04   Precision: 67.141%   Recall: 59.155%
Valid                   Loss: 1.659e+04   Precision: 26.812%   Recall: 64.401%
Train   Epoch: 188 / 600   Loss: 1.208e+04   Precision: 66.993%   Recall: 58.741%
Valid                   Loss: 1.642e+04   Precision: 28.159%   Recall: 61.002%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[71.65388489 39.85      ]
	 [68.2483139  42.05      ]
	 [52.49564743 35.83333333]
	 [46.14637756 33.7       ]
	 [85.36791992 34.25      ]]
Train   Epoch: 189 / 600   Loss: 1.17e+04   Precision: 66.388%   Recall: 61.095%
Valid                   Loss: 1.657e+04   Precision: 27.036%   Recall: 58.795%
Train   Epoch: 190 / 600   Loss: 1.214e+04   Precision: 66.246%   Recall: 58.327%
Valid                   Loss: 1.629e+04   Precision: 30.301%   Recall: 51.640%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 74.26073456  20.91666667]
	 [ 89.44713593  11.11666667]
	 [ 45.47516632  34.13333333]
	 [ 48.24579239 120.08333333]
	 [ 51.20713043  31.26666667]]
Train   Epoch: 191 / 600   Loss: 1.23e+04   Precision: 65.405%   Recall: 60.964%
Valid                   Loss: 1.691e+04   Precision: 26.262%   Recall: 67.024%
Train   Epoch: 192 / 600   Loss: 1.093e+04   Precision: 68.061%   Recall: 59.024%
Valid                   Loss: 1.72e+04   Precision: 25.844%   Recall: 68.456%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 84.36097717  65.8       ]
	 [ 76.95302582  51.21666667]
	 [130.51461792  41.16666667]
	 [110.43981934 140.6       ]
	 [ 43.83222961  32.13333333]]
Train   Epoch: 193 / 600   Loss: 1.172e+04   Precision: 64.183%   Recall: 56.407%
Valid                   Loss: 1.67e+04   Precision: 26.488%   Recall: 70.602%
Train   Epoch: 194 / 600   Loss: 1.178e+04   Precision: 67.380%   Recall: 59.216%
Valid                   Loss: 1.689e+04   Precision: 26.759%   Recall: 66.905%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.00862885  82.06666667]
	 [ 60.68560028  25.68333333]
	 [ 53.78458405  31.58333333]
	 [ 58.99658203  49.63333333]
	 [104.48456573 113.85      ]]
Train   Epoch: 195 / 600   Loss: 1.131e+04   Precision: 67.741%   Recall: 59.034%
Valid                   Loss: 1.655e+04   Precision: 28.012%   Recall: 62.671%
Train   Epoch: 196 / 600   Loss: 1.193e+04   Precision: 67.192%   Recall: 59.064%
Valid                   Loss: 1.667e+04   Precision: 26.663%   Recall: 65.951%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 75.369133    13.6       ]
	 [ 44.89863968  20.85      ]
	 [ 54.73249054  27.56666667]
	 [ 57.30462646  29.23333333]
	 [100.07447052  17.        ]]
Train   Epoch: 197 / 600   Loss: 1.195e+04   Precision: 67.696%   Recall: 58.700%
Valid                   Loss: 1.698e+04   Precision: 26.881%   Recall: 55.397%
Train   Epoch: 198 / 600   Loss: 1.11e+04   Precision: 67.470%   Recall: 59.691%
Valid                   Loss: 1.661e+04   Precision: 28.316%   Recall: 59.571%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.08068085 20.75      ]
	 [35.59902191 27.85      ]
	 [89.78794861 88.        ]
	 [34.87310028 39.83333333]
	 [38.26321793 42.15      ]]
Train   Epoch: 199 / 600   Loss: 1.209e+04   Precision: 65.231%   Recall: 59.549%
Valid                   Loss: 1.641e+04   Precision: 29.691%   Recall: 45.200%
Train   Epoch: 200 / 600   Loss: 1.151e+04   Precision: 68.024%   Recall: 57.053%
Valid                   Loss: 1.657e+04   Precision: 28.801%   Recall: 52.415%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.97464371 38.16666667]
	 [62.84454346 55.61666667]
	 [79.01437378 38.78333333]
	 [64.71656036 20.65      ]
	 [51.02286911 34.78333333]]
Train   Epoch: 201 / 600   Loss: 1.071e+04   Precision: 68.369%   Recall: 59.539%
Valid                   Loss: 1.66e+04   Precision: 29.469%   Recall: 53.965%
Train   Epoch: 202 / 600   Loss: 1.085e+04   Precision: 67.389%   Recall: 58.741%
Valid                   Loss: 1.646e+04   Precision: 28.768%   Recall: 59.869%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 76.06867981  99.96666667]
	 [ 65.4442749   11.41666667]
	 [ 49.31289673  29.88333333]
	 [194.22909546  58.11666667]
	 [ 53.03850937  26.66666667]]
Train   Epoch: 203 / 600   Loss: 1.114e+04   Precision: 68.871%   Recall: 58.731%
Valid                   Loss: 1.648e+04   Precision: 30.238%   Recall: 48.539%
Train   Epoch: 204 / 600   Loss: 1.106e+04   Precision: 67.847%   Recall: 60.236%
Valid                   Loss: 1.682e+04   Precision: 26.259%   Recall: 66.249%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.72237015  32.11666667]
	 [ 51.74999237  24.41666667]
	 [ 54.09237671  40.16666667]
	 [246.77073669  37.56666667]
	 [ 52.48551941  50.1       ]]
Train   Epoch: 205 / 600   Loss: 1.077e+04   Precision: 69.338%   Recall: 58.933%
Valid                   Loss: 1.672e+04   Precision: 28.416%   Recall: 62.135%
Train   Epoch: 206 / 600   Loss: 1.073e+04   Precision: 68.418%   Recall: 59.216%
Valid                   Loss: 1.653e+04   Precision: 28.609%   Recall: 58.140%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.03128433 29.53333333]
	 [33.27850342 61.63333333]
	 [47.53785324 25.98333333]
	 [60.94757843 19.        ]
	 [49.09843445 52.        ]]
Train   Epoch: 207 / 600   Loss: 1.114e+04   Precision: 69.090%   Recall: 58.296%
Valid                   Loss: 1.626e+04   Precision: 31.910%   Recall: 48.122%
Train   Epoch: 208 / 600   Loss: 1.215e+04   Precision: 68.509%   Recall: 59.489%
Valid                   Loss: 1.676e+04   Precision: 30.166%   Recall: 47.704%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.47054291  14.48333333]
	 [ 48.50694275  34.78333333]
	 [ 86.04831696  20.5       ]
	 [ 64.41942596 120.98333333]
	 [ 70.66448975  62.51666667]]
Train   Epoch: 209 / 600   Loss: 1.131e+04   Precision: 69.943%   Recall: 58.599%
Valid                   Loss: 1.663e+04   Precision: 30.113%   Recall: 55.575%
Train   Epoch: 210 / 600   Loss: 1.033e+04   Precision: 69.751%   Recall: 59.186%
Valid                   Loss: 1.659e+04   Precision: 30.481%   Recall: 55.874%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.45865631 74.88333333]
	 [66.3616333  34.55      ]
	 [83.80500793 64.95      ]
	 [53.61294556 42.05      ]
	 [58.79929352 32.9       ]]
Train   Epoch: 211 / 600   Loss: 1.057e+04   Precision: 69.328%   Recall: 59.317%
Valid                   Loss: 1.702e+04   Precision: 28.507%   Recall: 60.346%
Train   Epoch: 212 / 600   Loss: 1.049e+04   Precision: 68.479%   Recall: 58.943%
Valid                   Loss: 1.65e+04   Precision: 30.393%   Recall: 50.328%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.43934631  32.25      ]
	 [ 38.91505051  12.13333333]
	 [ 73.81719971  31.11666667]
	 [105.82791138  50.83333333]
	 [ 47.64718628  24.9       ]]
Train   Epoch: 213 / 600   Loss: 1.074e+04   Precision: 69.206%   Recall: 59.024%
Valid                   Loss: 1.689e+04   Precision: 28.272%   Recall: 59.511%
Train   Epoch: 214 / 600   Loss: 1.1e+04   Precision: 68.841%   Recall: 60.257%
Valid                   Loss: 1.647e+04   Precision: 31.090%   Recall: 45.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.27336884  38.65      ]
	 [107.72271729  14.33333333]
	 [ 29.48891258  46.        ]
	 [ 16.4145565   26.21666667]
	 [ 39.4491539   37.48333333]]
Train   Epoch: 215 / 600   Loss: 1.087e+04   Precision: 69.480%   Recall: 59.721%
Valid                   Loss: 1.653e+04   Precision: 30.302%   Recall: 55.635%
Train   Epoch: 216 / 600   Loss: 1.1e+04   Precision: 69.780%   Recall: 57.609%
Valid                   Loss: 1.734e+04   Precision: 28.859%   Recall: 59.750%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.60255432 54.66666667]
	 [69.00007629 43.85      ]
	 [31.95514297 17.78333333]
	 [56.47888947 52.55      ]
	 [62.5610199  19.03333333]]
Train   Epoch: 217 / 600   Loss: 1.126e+04   Precision: 68.680%   Recall: 60.893%
Valid                   Loss: 1.673e+04   Precision: 31.431%   Recall: 44.663%
Train   Epoch: 218 / 600   Loss: 1.082e+04   Precision: 70.982%   Recall: 58.458%
Valid                   Loss: 1.647e+04   Precision: 28.204%   Recall: 64.818%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[67.58686066 40.38333333]
	 [48.04857254 54.8       ]
	 [38.5414772  25.08333333]
	 [79.00077057 23.25      ]
	 [39.60956573 33.08333333]]
Train   Epoch: 219 / 600   Loss: 1.113e+04   Precision: 70.180%   Recall: 59.954%
Valid                   Loss: 1.676e+04   Precision: 29.714%   Recall: 56.470%
Train   Epoch: 220 / 600   Loss: 1.126e+04   Precision: 69.616%   Recall: 58.438%
Valid                   Loss: 1.669e+04   Precision: 30.135%   Recall: 58.438%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[213.96243286  30.        ]
	 [ 33.07859421  32.5       ]
	 [ 51.33057785  16.95      ]
	 [ 42.22591782  48.        ]
	 [ 40.98781586  42.66666667]]
Train   Epoch: 221 / 600   Loss: 1.044e+04   Precision: 70.964%   Recall: 58.185%
Valid                   Loss: 1.659e+04   Precision: 29.830%   Recall: 48.241%
Train   Epoch: 222 / 600   Loss: 1.11e+04   Precision: 66.889%   Recall: 60.691%
Valid                   Loss: 1.651e+04   Precision: 31.789%   Recall: 55.218%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.72402954 25.68333333]
	 [47.11751175 32.11666667]
	 [62.93144226 35.33333333]
	 [16.33641243 16.75      ]
	 [77.59938812 57.78333333]]
Train   Epoch: 223 / 600   Loss: 1.054e+04   Precision: 68.781%   Recall: 59.489%
Valid                   Loss: 1.677e+04   Precision: 31.600%   Recall: 51.819%
Train   Epoch: 224 / 600   Loss:    9771   Precision: 70.030%   Recall: 58.347%
Valid                   Loss: 1.632e+04   Precision: 34.855%   Recall: 40.966%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[21.95135117 32.45      ]
	 [47.23029709 21.51666667]
	 [50.28124237 53.55      ]
	 [63.84776306 41.01666667]
	 [43.35903168 72.5       ]]
Train   Epoch: 225 / 600   Loss: 1.048e+04   Precision: 68.926%   Recall: 57.245%
Valid                   Loss: 1.775e+04   Precision: 22.173%   Recall: 74.478%
Train   Epoch: 226 / 600   Loss: 1.084e+04   Precision: 69.259%   Recall: 58.579%
Valid                   Loss: 1.653e+04   Precision: 30.276%   Recall: 57.662%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.88576889  50.46666667]
	 [ 47.6475029   69.93333333]
	 [124.0848465  123.11666667]
	 [ 65.33518219  55.75      ]
	 [ 53.18420029  42.68333333]]
Train   Epoch: 227 / 600   Loss: 1.001e+04   Precision: 70.036%   Recall: 59.731%
Valid                   Loss: 1.679e+04   Precision: 28.181%   Recall: 59.034%
Train   Epoch: 228 / 600   Loss: 1.02e+04   Precision: 69.749%   Recall: 60.974%
Valid                   Loss: 1.64e+04   Precision: 31.985%   Recall: 54.204%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.63079834  85.5       ]
	 [ 34.03378677  20.        ]
	 [ 64.65438843  27.96666667]
	 [ 50.55319214  50.45      ]
	 [136.84333801  92.63333333]]
Train   Epoch: 229 / 600   Loss: 1.028e+04   Precision: 70.205%   Recall: 58.953%
Valid                   Loss: 1.667e+04   Precision: 31.093%   Recall: 49.374%
Train   Epoch: 230 / 600   Loss: 1.08e+04   Precision: 70.972%   Recall: 59.246%
Valid                   Loss: 1.66e+04   Precision: 32.192%   Recall: 47.645%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.88219833 34.4       ]
	 [60.55881119 30.61666667]
	 [46.98653412 35.16666667]
	 [67.78339386 17.93333333]
	 [32.30104446 51.78333333]]
Train   Epoch: 231 / 600   Loss:    9655   Precision: 71.697%   Recall: 59.671%
Valid                   Loss: 1.665e+04   Precision: 29.820%   Recall: 55.337%
Train   Epoch: 232 / 600   Loss: 1.046e+04   Precision: 69.271%   Recall: 59.751%
Valid                   Loss: 1.768e+04   Precision: 29.458%   Recall: 58.617%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.98413467  16.45      ]
	 [ 91.51570892 185.5       ]
	 [ 49.10117722  51.41666667]
	 [ 55.15215683  30.61666667]
	 [ 66.06031036  26.25      ]]
Train   Epoch: 233 / 600   Loss: 1.101e+04   Precision: 69.860%   Recall: 58.508%
Valid                   Loss: 1.652e+04   Precision: 32.589%   Recall: 50.507%
Train   Epoch: 234 / 600   Loss: 1.037e+04   Precision: 71.204%   Recall: 60.044%
Valid                   Loss: 1.688e+04   Precision: 29.243%   Recall: 60.823%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 83.54920197  78.88333333]
	 [ 89.8859787   40.        ]
	 [159.27825928 165.33333333]
	 [204.89659119  19.08333333]
	 [ 46.02794647  37.21666667]]
Train   Epoch: 235 / 600   Loss:    9337   Precision: 71.051%   Recall: 60.641%
Valid                   Loss: 1.651e+04   Precision: 33.375%   Recall: 47.406%
Train   Epoch: 236 / 600   Loss: 1.052e+04   Precision: 71.769%   Recall: 61.116%
Valid                   Loss: 1.717e+04   Precision: 31.795%   Recall: 48.062%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.35688782 66.05      ]
	 [43.27054596 93.75      ]
	 [43.79991531 24.        ]
	 [45.36693573 23.        ]
	 [34.62498856 46.5       ]]
Train   Epoch: 237 / 600   Loss: 1.014e+04   Precision: 71.332%   Recall: 59.590%
Valid                   Loss: 1.648e+04   Precision: 32.572%   Recall: 48.479%
Train   Epoch: 238 / 600   Loss:   1e+04   Precision: 71.198%   Recall: 59.852%
Valid                   Loss: 1.631e+04   Precision: 33.936%   Recall: 39.177%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[27.70534134 33.38      ]
	 [63.25837326 32.75      ]
	 [60.69662476 25.05      ]
	 [63.31315994 82.2       ]
	 [41.96529007 24.        ]]
Train   Epoch: 239 / 600   Loss: 1.005e+04   Precision: 70.914%   Recall: 58.145%
Valid                   Loss: 1.663e+04   Precision: 30.132%   Recall: 51.640%
Train   Epoch: 240 / 600   Loss: 1.012e+04   Precision: 70.324%   Recall: 59.650%
Valid                   Loss: 1.697e+04   Precision: 33.409%   Recall: 43.888%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.603302   28.93333333]
	 [34.60845184 39.83333333]
	 [55.78778839 83.75      ]
	 [38.54211426 29.33333333]
	 [72.29374695 31.        ]]
Train   Epoch: 241 / 600   Loss: 1.032e+04   Precision: 69.673%   Recall: 59.943%
Valid                   Loss: 1.691e+04   Precision: 28.032%   Recall: 56.231%
Train   Epoch: 242 / 600   Loss: 1.118e+04   Precision: 65.973%   Recall: 58.347%
Valid                   Loss: 1.695e+04   Precision: 27.838%   Recall: 53.369%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 70.80648804  25.26666667]
	 [ 73.91650391 130.13333333]
	 [ 38.15699387  27.81666667]
	 [ 43.26794434  49.11666667]
	 [ 95.78347778  28.21666667]]
Train   Epoch: 243 / 600   Loss: 1.009e+04   Precision: 70.820%   Recall: 57.660%
Valid                   Loss: 1.693e+04   Precision: 31.220%   Recall: 46.094%
Train   Epoch: 244 / 600   Loss:    9948   Precision: 69.413%   Recall: 59.852%
Valid                   Loss: 1.668e+04   Precision: 31.196%   Recall: 42.934%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.32525253 12.46666667]
	 [89.35536194 54.33333333]
	 [54.9315834  31.38333333]
	 [37.92078781 26.25      ]
	 [48.22161484 38.11666667]]
Train   Epoch: 245 / 600   Loss: 1.013e+04   Precision: 70.217%   Recall: 60.418%
Valid                   Loss: 1.671e+04   Precision: 32.358%   Recall: 45.498%
Train   Epoch: 246 / 600   Loss:    9962   Precision: 70.118%   Recall: 58.165%
Valid                   Loss: 1.684e+04   Precision: 28.483%   Recall: 57.901%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.77924347  24.5       ]
	 [ 50.12255478  31.95      ]
	 [ 36.0195961   49.11666667]
	 [ 53.34488678  27.61666667]
	 [ 63.41151047 113.11666667]]
Train   Epoch: 247 / 600   Loss:    9317   Precision: 71.465%   Recall: 60.105%
Valid                   Loss: 1.675e+04   Precision: 29.248%   Recall: 53.787%
Train   Epoch: 248 / 600   Loss:    9713   Precision: 73.318%   Recall: 59.782%
Valid                   Loss: 1.67e+04   Precision: 31.671%   Recall: 51.878%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[329.07144165 418.41666667]
	 [ 31.50274658  39.78333333]
	 [ 48.86877441  44.        ]
	 [ 31.68647003  18.33333333]
	 [ 51.42969131  36.28333333]]
Train   Epoch: 249 / 600   Loss:    8993   Precision: 72.416%   Recall: 60.034%
Valid                   Loss: 1.663e+04   Precision: 30.988%   Recall: 47.525%
Train   Epoch: 250 / 600   Loss:    8897   Precision: 72.344%   Recall: 61.591%
Valid                   Loss: 1.673e+04   Precision: 30.724%   Recall: 53.131%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.09305191 25.66666667]
	 [39.5200119  32.66666667]
	 [45.20484161 33.45      ]
	 [58.67647552 50.91666667]
	 [39.12234116 64.05      ]]
Train   Epoch: 251 / 600   Loss: 1.01e+04   Precision: 70.291%   Recall: 60.752%
Valid                   Loss: 1.644e+04   Precision: 32.939%   Recall: 34.824%
Train   Epoch: 252 / 600   Loss:    9438   Precision: 72.591%   Recall: 58.236%
Valid                   Loss: 1.654e+04   Precision: 33.333%   Recall: 45.200%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.06876755  47.83333333]
	 [ 38.12032318  21.86666667]
	 [ 47.23518372  34.83333333]
	 [ 63.3550415   35.73333333]
	 [100.00017548  73.5       ]]
Train   Epoch: 253 / 600   Loss:    9924   Precision: 73.488%   Recall: 60.024%
Valid                   Loss: 1.683e+04   Precision: 31.800%   Recall: 52.355%
Train   Epoch: 254 / 600   Loss:    9436   Precision: 73.007%   Recall: 60.509%
Valid                   Loss: 1.672e+04   Precision: 31.911%   Recall: 48.599%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.53949738  26.33333333]
	 [307.94998169 421.4       ]
	 [ 66.73157501  93.08333333]
	 [ 76.26222229  76.        ]
	 [ 34.70505142  49.        ]]
Train   Epoch: 255 / 600   Loss: 1.058e+04   Precision: 64.199%   Recall: 60.560%
Valid                   Loss: 1.647e+04   Precision: 33.752%   Recall: 32.081%
Train   Epoch: 256 / 600   Loss:    9233   Precision: 71.458%   Recall: 58.620%
Valid                   Loss: 1.657e+04   Precision: 30.741%   Recall: 45.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.57497406 19.5       ]
	 [47.83935547 49.88333333]
	 [41.73225784 19.3       ]
	 [82.03664398 55.9       ]
	 [43.29072189 58.66666667]]
Train   Epoch: 257 / 600   Loss:    9961   Precision: 71.784%   Recall: 59.721%
Valid                   Loss: 1.741e+04   Precision: 29.672%   Recall: 57.185%
Train   Epoch: 258 / 600   Loss:    9679   Precision: 72.228%   Recall: 59.580%
Valid                   Loss: 1.739e+04   Precision: 24.117%   Recall: 70.006%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.38101196 25.08333333]
	 [49.81535339 21.41666667]
	 [83.35351562 89.81666667]
	 [78.98227692 48.33333333]
	 [83.55956268 59.96666667]]
Train   Epoch: 259 / 600   Loss:    9856   Precision: 70.163%   Recall: 60.762%
Valid                   Loss: 1.761e+04   Precision: 28.468%   Recall: 58.617%
Train   Epoch: 260 / 600   Loss:    9632   Precision: 72.534%   Recall: 59.671%
Valid                   Loss: 1.665e+04   Precision: 33.050%   Recall: 46.452%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.08534241 17.78333333]
	 [43.4156723  15.78333333]
	 [41.40468597 21.86666667]
	 [92.70024109 16.2       ]
	 [47.75661469 26.43333333]]
Train   Epoch: 261 / 600   Loss:    9625   Precision: 72.049%   Recall: 60.196%
Valid                   Loss: 1.763e+04   Precision: 29.074%   Recall: 55.635%
Train   Epoch: 262 / 600   Loss:    8937   Precision: 70.642%   Recall: 58.842%
Valid                   Loss: 1.665e+04   Precision: 32.117%   Recall: 48.301%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.54550934 58.78333333]
	 [71.72968292 44.33333333]
	 [52.43704987 43.88333333]
	 [60.2203598  33.31666667]
	 [97.97324371 64.5       ]]
Train   Epoch: 263 / 600   Loss:    8593   Precision: 72.007%   Recall: 59.681%
Valid                   Loss: 1.646e+04   Precision: 33.423%   Recall: 44.544%
Train   Epoch: 264 / 600   Loss:    9671   Precision: 73.653%   Recall: 61.075%
Valid                   Loss: 1.652e+04   Precision: 32.521%   Recall: 44.544%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[183.02798462  84.2       ]
	 [101.84760284  31.7       ]
	 [ 17.32720566  14.66666667]
	 [ 77.7816925   37.58333333]
	 [ 35.98753738  57.86666667]]
Train   Epoch: 265 / 600   Loss:    8919   Precision: 73.683%   Recall: 60.772%
Valid                   Loss: 1.663e+04   Precision: 30.372%   Recall: 47.704%
Train   Epoch: 266 / 600   Loss:    8642   Precision: 74.042%   Recall: 59.580%
Valid                   Loss: 1.684e+04   Precision: 29.297%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[168.17468262 113.51666667]
	 [ 32.34660721  43.3       ]
	 [ 46.61584473  41.58333333]
	 [ 39.4780426   24.75      ]
	 [ 61.80406189  44.16666667]]
Train   Epoch: 267 / 600   Loss:    9449   Precision: 74.240%   Recall: 62.641%
Valid                   Loss: 1.669e+04   Precision: 31.371%   Recall: 50.209%
Train   Epoch: 268 / 600   Loss:    8763   Precision: 73.528%   Recall: 61.692%
Valid                   Loss: 1.652e+04   Precision: 30.684%   Recall: 48.122%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 75.07305908  19.96666667]
	 [196.96984863  53.7       ]
	 [ 28.79838371  19.8       ]
	 [ 51.17031097  42.66666667]
	 [ 85.38874054  80.05      ]]
Train   Epoch: 269 / 600   Loss:    8410   Precision: 74.241%   Recall: 60.085%
Valid                   Loss: 1.681e+04   Precision: 32.821%   Recall: 44.544%
Train   Epoch: 270 / 600   Loss:    9126   Precision: 74.217%   Recall: 61.085%
Valid                   Loss: 1.678e+04   Precision: 31.245%   Recall: 45.498%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[106.93335724  84.91666667]
	 [ 53.92113876  76.53333333]
	 [ 57.4766922   40.        ]
	 [ 68.58135223  38.05      ]
	 [ 82.60940552  40.4       ]]
Train   Epoch: 271 / 600   Loss:    8785   Precision: 73.392%   Recall: 61.904%
Valid                   Loss: 1.669e+04   Precision: 32.780%   Recall: 44.723%
Train   Epoch: 272 / 600   Loss:    8591   Precision: 73.772%   Recall: 61.166%
Valid                   Loss: 1.658e+04   Precision: 30.999%   Recall: 46.452%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[59.28560638 30.71666667]
	 [35.37608337 38.33333333]
	 [60.85649109 28.36666667]
	 [51.73280334 35.65      ]
	 [39.67993546 24.53333333]]
Train   Epoch: 273 / 600   Loss:    8960   Precision: 74.161%   Recall: 60.964%
Valid                   Loss: 1.715e+04   Precision: 30.112%   Recall: 52.952%
Train   Epoch: 274 / 600   Loss:    8092   Precision: 73.252%   Recall: 61.075%
Valid                   Loss: 1.654e+04   Precision: 31.301%   Recall: 43.172%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[153.43673706 187.91666667]
	 [ 42.50141144  47.        ]
	 [ 36.94758224  74.        ]
	 [ 47.23071671  22.11666667]
	 [ 29.497715    32.03333333]]
Train   Epoch: 275 / 600   Loss:    9253   Precision: 73.043%   Recall: 60.732%
Valid                   Loss: 1.712e+04   Precision: 27.983%   Recall: 60.405%
Train   Epoch: 276 / 600   Loss:    8379   Precision: 73.610%   Recall: 60.459%
Valid                   Loss: 1.672e+04   Precision: 29.977%   Recall: 46.869%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.26094437 13.25      ]
	 [39.26577377 12.        ]
	 [28.31538391 11.9       ]
	 [42.45144272 34.95      ]
	 [25.28085327 22.        ]]
Train   Epoch: 277 / 600   Loss:    8234   Precision: 73.047%   Recall: 60.085%
Valid                   Loss: 1.723e+04   Precision: 26.058%   Recall: 63.864%
Train   Epoch: 278 / 600   Loss:    8705   Precision: 72.522%   Recall: 61.449%
Valid                   Loss: 1.663e+04   Precision: 33.847%   Recall: 35.361%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.59635544 28.23333333]
	 [45.87461472 61.6       ]
	 [42.17011642 41.66666667]
	 [62.41981125 36.16666667]
	 [53.74015045 25.16666667]]
Train   Epoch: 279 / 600   Loss:    8766   Precision: 72.484%   Recall: 60.691%
Valid                   Loss: 1.68e+04   Precision: 31.238%   Recall: 38.819%
Train   Epoch: 280 / 600   Loss:    8901   Precision: 73.592%   Recall: 61.247%
Valid                   Loss: 1.7e+04   Precision: 29.867%   Recall: 48.301%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.46206284 32.21666667]
	 [49.62212372 44.78333333]
	 [43.73797607 41.18333333]
	 [59.5978508  23.16666667]
	 [35.92959595 10.28333333]]
Train   Epoch: 281 / 600   Loss:    8474   Precision: 74.344%   Recall: 60.671%
Valid                   Loss: 1.698e+04   Precision: 27.986%   Recall: 57.007%
Train   Epoch: 282 / 600   Loss:    8832   Precision: 69.557%   Recall: 59.084%
Valid                   Loss: 1.732e+04   Precision: 27.144%   Recall: 59.273%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.6844101  48.5       ]
	 [55.66617966 34.11666667]
	 [50.98693085 34.38333333]
	 [68.01203918 31.25      ]
	 [68.59445953 40.03333333]]
Train   Epoch: 283 / 600   Loss:    8407   Precision: 72.924%   Recall: 60.418%
Valid                   Loss: 1.676e+04   Precision: 32.902%   Recall: 40.966%
Train   Epoch: 284 / 600   Loss:    8306   Precision: 74.750%   Recall: 59.681%
Valid                   Loss: 1.672e+04   Precision: 33.835%   Recall: 37.507%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[77.31433105 56.33333333]
	 [39.61709976 59.16666667]
	 [59.05699539 40.16666667]
	 [45.75611877 33.        ]
	 [48.74796677 30.78333333]]
Train   Epoch: 285 / 600   Loss:    7903   Precision: 74.852%   Recall: 62.682%
Valid                   Loss: 1.679e+04   Precision: 30.892%   Recall: 48.539%
Train   Epoch: 286 / 600   Loss:    7980   Precision: 74.849%   Recall: 61.196%
Valid                   Loss: 1.674e+04   Precision: 27.069%   Recall: 55.575%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[57.65690613 14.41666667]
	 [45.62189102 33.78333333]
	 [80.9270401  43.        ]
	 [54.30498505 38.33333333]
	 [41.24406433 33.91666667]]
Train   Epoch: 287 / 600   Loss:    8022   Precision: 74.988%   Recall: 60.742%
Valid                   Loss: 1.669e+04   Precision: 30.784%   Recall: 48.479%
Train   Epoch: 288 / 600   Loss:    7926   Precision: 75.942%   Recall: 60.671%
Valid                   Loss: 1.68e+04   Precision: 30.596%   Recall: 51.998%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.57343292  58.91666667]
	 [104.49014282  47.35      ]
	 [ 31.02591133  16.        ]
	 [ 47.83231354  56.3       ]
	 [ 35.75257111  47.4       ]]
Train   Epoch: 289 / 600   Loss:    8512   Precision: 74.043%   Recall: 62.177%
Valid                   Loss: 1.671e+04   Precision: 30.759%   Recall: 43.232%
Train   Epoch: 290 / 600   Loss:    8113   Precision: 75.750%   Recall: 61.520%
Valid                   Loss: 1.665e+04   Precision: 32.434%   Recall: 42.993%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.34267044 33.15      ]
	 [41.40654755 31.3       ]
	 [45.94674301 23.        ]
	 [41.36803436 25.91666667]
	 [39.55634689 22.33333333]]
Train   Epoch: 291 / 600   Loss:    8103   Precision: 74.403%   Recall: 62.621%
Valid                   Loss: 1.695e+04   Precision: 30.435%   Recall: 48.837%
Train   Epoch: 292 / 600   Loss: 1.192e+04   Precision: 57.821%   Recall: 61.671%
Valid                   Loss: 1.675e+04   Precision: 25.663%   Recall: 65.772%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.25292969  32.96666667]
	 [ 83.72126007  38.3       ]
	 [ 60.54987335  59.5       ]
	 [ 57.8088913   17.5       ]
	 [ 55.51438522  21.31666667]]
Train   Epoch: 293 / 600   Loss:    9047   Precision: 67.658%   Recall: 57.710%
Valid                   Loss: 1.672e+04   Precision: 31.868%   Recall: 36.732%
Train   Epoch: 294 / 600   Loss:    8467   Precision: 72.784%   Recall: 60.317%
Valid                   Loss: 1.661e+04   Precision: 31.662%   Recall: 40.668%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.95252609 40.53333333]
	 [27.746521   25.65      ]
	 [61.06690216 38.28333333]
	 [39.84594345 65.38333333]
	 [47.79325485 36.28333333]]
Train   Epoch: 295 / 600   Loss:    8201   Precision: 73.552%   Recall: 60.055%
Valid                   Loss: 1.663e+04   Precision: 30.028%   Recall: 44.246%
Train   Epoch: 296 / 600   Loss:    7902   Precision: 75.003%   Recall: 62.065%
Valid                   Loss: 1.658e+04   Precision: 32.551%   Recall: 42.993%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[209.14553833 273.73333333]
	 [ 40.34150696  31.96666667]
	 [ 14.58061409  58.26666667]
	 [ 49.11417389  34.        ]
	 [ 36.27170181  32.08333333]]
Train   Epoch: 297 / 600   Loss:    8384   Precision: 74.821%   Recall: 60.024%
Valid                   Loss: 1.69e+04   Precision: 31.258%   Recall: 46.094%
Train   Epoch: 298 / 600   Loss:    9037   Precision: 74.814%   Recall: 60.903%
Valid                   Loss: 1.653e+04   Precision: 30.729%   Recall: 45.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.98913574 27.83333333]
	 [60.78850937 52.33333333]
	 [67.64909363 36.13333333]
	 [32.49345779 24.73333333]
	 [57.06399155 24.41666667]]
Train   Epoch: 299 / 600   Loss:    8614   Precision: 75.160%   Recall: 62.773%
Valid                   Loss: 1.71e+04   Precision: 29.009%   Recall: 53.071%
Train   Epoch: 300 / 600   Loss:    8252   Precision: 74.658%   Recall: 60.044%
Valid                   Loss: 1.688e+04   Precision: 30.758%   Recall: 48.897%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.94439316 22.71666667]
	 [51.87923431 69.88333333]
	 [99.98768616 74.63333333]
	 [46.18893433 35.28333333]
	 [55.98343658 26.91666667]]
Train   Epoch: 301 / 600   Loss:    7617   Precision: 75.496%   Recall: 62.298%
Valid                   Loss: 1.667e+04   Precision: 29.954%   Recall: 46.154%
Train   Epoch: 302 / 600   Loss:    7714   Precision: 75.680%   Recall: 60.974%
Valid                   Loss: 1.661e+04   Precision: 33.286%   Recall: 42.278%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.67001724  42.        ]
	 [ 68.34432983  62.33333333]
	 [ 20.22076607  22.03333333]
	 [ 48.60826874  23.7       ]
	 [791.55334473 864.73333333]]
Train   Epoch: 303 / 600   Loss:    7382   Precision: 76.193%   Recall: 61.934%
Valid                   Loss: 1.667e+04   Precision: 31.641%   Recall: 38.283%
Train   Epoch: 304 / 600   Loss:    7854   Precision: 72.475%   Recall: 62.076%
Valid                   Loss: 1.698e+04   Precision: 29.405%   Recall: 43.590%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 56.95140076  49.75      ]
	 [ 41.51248932  21.15      ]
	 [ 48.69859695  40.33333333]
	 [ 95.30106354 118.86666667]
	 [ 49.28556061  39.58333333]]
Train   Epoch: 305 / 600   Loss:    7195   Precision: 76.490%   Recall: 62.399%
Valid                   Loss: 1.662e+04   Precision: 33.409%   Recall: 35.003%
Train   Epoch: 306 / 600   Loss:    7577   Precision: 76.057%   Recall: 61.793%
Valid                   Loss: 1.659e+04   Precision: 31.657%   Recall: 42.039%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.94717026 38.96666667]
	 [33.43027496 20.23333333]
	 [52.70046616 69.61666667]
	 [34.98822403 39.88333333]
	 [43.92336273 66.73333333]]
Train   Epoch: 307 / 600   Loss:    7827   Precision: 75.504%   Recall: 62.449%
Valid                   Loss: 1.7e+04   Precision: 29.446%   Recall: 48.778%
Train   Epoch: 308 / 600   Loss:    7491   Precision: 74.024%   Recall: 62.662%
Valid                   Loss: 1.656e+04   Precision: 34.247%   Recall: 30.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 0.65732229 42.36666667]
	 [36.77452469 36.9       ]
	 [18.27820969 32.26666667]
	 [34.8742981  44.66666667]
	 [37.78462601 31.85      ]]
Train   Epoch: 309 / 600   Loss:    7347   Precision: 77.063%   Recall: 61.520%
Valid                   Loss: 1.68e+04   Precision: 31.058%   Recall: 42.874%
Train   Epoch: 310 / 600   Loss:    6965   Precision: 76.881%   Recall: 62.268%
Valid                   Loss: 1.666e+04   Precision: 32.416%   Recall: 37.925%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.31978607  44.21666667]
	 [ 35.08368301  30.66666667]
	 [ 36.4774971   26.25      ]
	 [ 43.88768768  31.        ]
	 [151.29081726  67.91666667]]
Train   Epoch: 311 / 600   Loss:    7635   Precision: 75.573%   Recall: 62.278%
Valid                   Loss: 1.674e+04   Precision: 31.944%   Recall: 44.782%
Train   Epoch: 312 / 600   Loss:    7398   Precision: 76.516%   Recall: 62.621%
Valid                   Loss: 1.677e+04   Precision: 32.002%   Recall: 42.516%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.03105927 67.25      ]
	 [42.31623077 20.        ]
	 [47.7419014  45.88333333]
	 [35.50963974 17.21666667]
	 [71.04931641 35.66666667]]
Train   Epoch: 313 / 600   Loss:    7448   Precision: 76.259%   Recall: 62.449%
Valid                   Loss: 1.674e+04   Precision: 31.659%   Recall: 43.590%
Train   Epoch: 314 / 600   Loss:    7303   Precision: 76.929%   Recall: 62.470%
Valid                   Loss: 1.703e+04   Precision: 31.833%   Recall: 39.654%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.95676041 24.2       ]
	 [86.37736511 62.03333333]
	 [35.07025909 29.76666667]
	 [54.05725861 24.8       ]
	 [42.07973862 26.53333333]]
Train   Epoch: 315 / 600   Loss:    7304   Precision: 77.471%   Recall: 62.480%
Valid                   Loss: 1.661e+04   Precision: 31.421%   Recall: 41.145%
Train   Epoch: 316 / 600   Loss:    7379   Precision: 76.274%   Recall: 63.804%
Valid                   Loss: 1.69e+04   Precision: 35.724%   Recall: 30.292%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  1.69701743  14.38333333]
	 [316.09506226 319.        ]
	 [ 49.27997589  39.7       ]
	 [ 33.14887619  16.        ]
	 [ 47.73999786  20.6       ]]
Train   Epoch: 317 / 600   Loss:    6995   Precision: 76.403%   Recall: 62.854%
Valid                   Loss: 1.699e+04   Precision: 31.690%   Recall: 42.159%
Train   Epoch: 318 / 600   Loss:    6947   Precision: 77.443%   Recall: 62.793%
Valid                   Loss: 1.687e+04   Precision: 28.151%   Recall: 55.277%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 26.98407173  48.73333333]
	 [ 43.23349762  24.33333333]
	 [ 50.91301727  17.11666667]
	 [225.00132751  46.53333333]
	 [ 59.11224365  59.7       ]]
Train   Epoch: 319 / 600   Loss:    7062   Precision: 76.755%   Recall: 63.531%
Valid                   Loss: 1.708e+04   Precision: 30.465%   Recall: 47.287%
Train   Epoch: 320 / 600   Loss:    7794   Precision: 74.305%   Recall: 63.470%
Valid                   Loss: 1.756e+04   Precision: 29.836%   Recall: 37.984%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[20.08645248 12.95      ]
	 [46.70000458 30.7       ]
	 [51.53365707 18.33333333]
	 [49.20783234 64.25      ]
	 [47.41907501 45.46666667]]
Train   Epoch: 321 / 600   Loss:    7711   Precision: 74.991%   Recall: 62.298%
Valid                   Loss: 1.678e+04   Precision: 33.662%   Recall: 32.558%
Train   Epoch: 322 / 600   Loss:    7112   Precision: 77.109%   Recall: 61.985%
Valid                   Loss: 1.689e+04   Precision: 31.166%   Recall: 41.443%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[325.48168945 271.5       ]
	 [ 57.05215073  28.46666667]
	 [ 45.47413254  31.33333333]
	 [ 35.90000153  27.91666667]
	 [ 42.51633835  87.33333333]]
Train   Epoch: 323 / 600   Loss:    7019   Precision: 75.194%   Recall: 63.622%
Valid                   Loss: 1.651e+04   Precision: 33.815%   Recall: 30.710%
Train   Epoch: 324 / 600   Loss:    7433   Precision: 76.270%   Recall: 62.520%
Valid                   Loss: 1.66e+04   Precision: 31.286%   Recall: 44.961%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.20911789 26.45      ]
	 [46.04352951 49.58333333]
	 [14.69844723  6.66666667]
	 [40.31039047 36.53333333]
	 [31.80978394 37.06666667]]
Train   Epoch: 325 / 600   Loss:    6928   Precision: 76.382%   Recall: 63.268%
Valid                   Loss: 1.698e+04   Precision: 34.282%   Recall: 28.742%
Train   Epoch: 326 / 600   Loss:    7058   Precision: 75.606%   Recall: 61.823%
Valid                   Loss: 1.709e+04   Precision: 28.548%   Recall: 42.099%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.90276337 16.        ]
	 [71.95719147 94.01666667]
	 [47.04951477 49.83333333]
	 [56.1427269  24.25      ]
	 [62.89795685 88.43333333]]
Train   Epoch: 327 / 600   Loss:    7530   Precision: 73.628%   Recall: 63.703%
Valid                   Loss: 1.734e+04   Precision: 31.264%   Recall: 41.741%
Train   Epoch: 328 / 600   Loss:    6903   Precision: 75.779%   Recall: 63.389%
Valid                   Loss: 1.694e+04   Precision: 31.786%   Recall: 37.567%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.44541168 25.9       ]
	 [81.44255829 55.93333333]
	 [41.8215065  27.58333333]
	 [42.84903717 22.38333333]
	 [44.73303223 29.75      ]]
Train   Epoch: 329 / 600   Loss:    7190   Precision: 76.170%   Recall: 63.177%
Valid                   Loss: 1.677e+04   Precision: 31.020%   Recall: 44.782%
Train   Epoch: 330 / 600   Loss:    6763   Precision: 77.927%   Recall: 62.682%
Valid                   Loss: 1.691e+04   Precision: 29.858%   Recall: 45.259%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.62107849 40.06666667]
	 [63.41603088 91.96666667]
	 [44.82076263 42.33333333]
	 [31.98083878 33.21666667]
	 [46.26048279 61.33333333]]
Train   Epoch: 331 / 600   Loss:    6745   Precision: 77.731%   Recall: 63.278%
Valid                   Loss: 1.684e+04   Precision: 34.024%   Recall: 34.287%
Train   Epoch: 332 / 600   Loss:    6500   Precision: 77.121%   Recall: 62.743%
Valid                   Loss: 1.697e+04   Precision: 30.113%   Recall: 42.934%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 35.45209885  18.        ]
	 [ 67.16304779 122.2       ]
	 [ 57.19159317  71.25      ]
	 [ 72.30943298  25.3       ]
	 [ 51.15395737  21.03333333]]
Train   Epoch: 333 / 600   Loss:    6971   Precision: 76.208%   Recall: 64.248%
Valid                   Loss: 1.697e+04   Precision: 32.921%   Recall: 34.884%
Train   Epoch: 334 / 600   Loss:    6891   Precision: 76.544%   Recall: 60.974%
Valid                   Loss: 1.713e+04   Precision: 27.582%   Recall: 48.420%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.3302536  47.11666667]
	 [62.30549622 31.63333333]
	 [43.75894928 33.46666667]
	 [56.09560013 48.21666667]
	 [48.77220154 33.66666667]]
Train   Epoch: 335 / 600   Loss:    6423   Precision: 77.509%   Recall: 64.390%
Valid                   Loss: 1.676e+04   Precision: 30.856%   Recall: 40.847%
Train   Epoch: 336 / 600   Loss:    6821   Precision: 76.732%   Recall: 63.015%
Valid                   Loss: 1.661e+04   Precision: 33.703%   Recall: 30.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[63.55571365 56.7       ]
	 [37.84805298 14.85      ]
	 [24.00240898 12.88333333]
	 [62.93093491 48.76666667]
	 [34.0771904  57.66666667]]
Train   Epoch: 337 / 600   Loss:    6655   Precision: 75.085%   Recall: 62.732%
Valid                   Loss: 1.661e+04   Precision: 33.961%   Recall: 32.260%
Train   Epoch: 338 / 600   Loss:    6479   Precision: 77.055%   Recall: 63.359%
Valid                   Loss: 1.672e+04   Precision: 32.394%   Recall: 36.315%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.89561462  27.        ]
	 [ 23.71064949  44.05      ]
	 [ 45.87182999   8.83333333]
	 [ 32.83252335  34.5       ]
	 [145.33425903  85.33333333]]
Train   Epoch: 339 / 600   Loss:    6411   Precision: 78.482%   Recall: 63.723%
Valid                   Loss: 1.685e+04   Precision: 32.958%   Recall: 36.673%
Train   Epoch: 340 / 600   Loss:    6259   Precision: 77.597%   Recall: 64.925%
Valid                   Loss: 1.691e+04   Precision: 30.809%   Recall: 38.819%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[59.8401947  22.38333333]
	 [45.93490982 45.7       ]
	 [45.72792053 58.05      ]
	 [49.58430099 54.75      ]
	 [61.93516922 62.        ]]
Train   Epoch: 341 / 600   Loss:    6420   Precision: 77.819%   Recall: 63.248%
Valid                   Loss: 1.682e+04   Precision: 34.557%   Recall: 32.558%
Train   Epoch: 342 / 600   Loss:    5825   Precision: 78.420%   Recall: 66.097%
Valid                   Loss: 1.663e+04   Precision: 33.705%   Recall: 34.228%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.36689377 65.8       ]
	 [30.87924385 33.33333333]
	 [64.52976227 21.36666667]
	 [ 6.75603533 19.06666667]
	 [48.53921509 23.95      ]]
Train   Epoch: 343 / 600   Loss:    6005   Precision: 78.326%   Recall: 63.359%
Valid                   Loss: 1.684e+04   Precision: 35.714%   Recall: 28.026%
Train   Epoch: 344 / 600   Loss:    6619   Precision: 77.680%   Recall: 63.268%
Valid                   Loss: 1.688e+04   Precision: 31.558%   Recall: 38.521%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.44150925 48.83333333]
	 [47.84157562 35.63333333]
	 [54.33147049 58.15      ]
	 [46.5021553  22.71666667]
	 [34.6741333  16.43333333]]
Train   Epoch: 345 / 600   Loss:    6251   Precision: 78.571%   Recall: 64.248%
Valid                   Loss: 1.66e+04   Precision: 32.661%   Recall: 39.595%
Train   Epoch: 346 / 600   Loss:    6643   Precision: 78.632%   Recall: 65.521%
Valid                   Loss: 1.729e+04   Precision: 28.766%   Recall: 52.832%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[71.01170349 42.58333333]
	 [43.73155975 34.66666667]
	 [40.1640892  37.        ]
	 [59.58324814 28.25      ]
	 [70.41447449 45.73333333]]
Train   Epoch: 347 / 600   Loss:    6156   Precision: 78.420%   Recall: 64.481%
Valid                   Loss: 1.694e+04   Precision: 30.879%   Recall: 47.764%
Train   Epoch: 348 / 600   Loss:    6823   Precision: 77.998%   Recall: 64.339%
Valid                   Loss: 1.73e+04   Precision: 31.478%   Recall: 39.117%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[20.02005005 41.95      ]
	 [46.3997879  21.38333333]
	 [56.77690125 75.25      ]
	 [44.53700256 44.41666667]
	 [51.21697235 50.88333333]]
Train   Epoch: 349 / 600   Loss:    6163   Precision: 78.123%   Recall: 64.521%
Valid                   Loss: 1.693e+04   Precision: 31.591%   Recall: 34.228%
Train   Epoch: 350 / 600   Loss:    5893   Precision: 79.026%   Recall: 65.107%
Valid                   Loss: 1.697e+04   Precision: 32.539%   Recall: 39.058%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[70.25869751 76.5       ]
	 [40.21311188 22.4       ]
	 [26.21160507 45.88333333]
	 [57.08569717 26.88333333]
	 [40.73672485 55.71666667]]
Train   Epoch: 351 / 600   Loss:    5962   Precision: 78.360%   Recall: 65.097%
Valid                   Loss: 1.729e+04   Precision: 29.377%   Recall: 47.525%
Train   Epoch: 352 / 600   Loss:    6767   Precision: 75.012%   Recall: 63.733%
Valid                   Loss: 1.802e+04   Precision: 28.818%   Recall: 40.847%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.54336929  34.66666667]
	 [ 52.9883461   63.36666667]
	 [ 28.28574562  31.98333333]
	 [130.13850403 178.83333333]
	 [ 57.00263977  59.88333333]]
Train   Epoch: 353 / 600   Loss:    6403   Precision: 75.479%   Recall: 63.359%
Valid                   Loss: 1.72e+04   Precision: 28.044%   Recall: 56.172%
Train   Epoch: 354 / 600   Loss:    6425   Precision: 75.886%   Recall: 63.157%
Valid                   Loss: 1.737e+04   Precision: 32.353%   Recall: 32.797%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.17432022 47.21666667]
	 [31.83903122 19.25      ]
	 [32.72377777 36.16666667]
	 [51.45678329 22.9       ]
	 [39.44260406 68.        ]]
Train   Epoch: 355 / 600   Loss:    6273   Precision: 78.200%   Recall: 63.036%
Valid                   Loss: 1.672e+04   Precision: 29.933%   Recall: 45.498%
Train   Epoch: 356 / 600   Loss:    6138   Precision: 75.941%   Recall: 64.814%
Valid                   Loss: 1.672e+04   Precision: 37.023%   Recall: 22.540%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.60223389 28.55      ]
	 [43.00452805 21.95      ]
	 [45.76794434 44.        ]
	 [27.27654648 31.8       ]
	 [37.79333878 45.73333333]]
Train   Epoch: 357 / 600   Loss:    6769   Precision: 71.228%   Recall: 59.388%
Valid                   Loss: 1.697e+04   Precision: 29.489%   Recall: 49.255%
Train   Epoch: 358 / 600   Loss:    6251   Precision: 77.564%   Recall: 63.268%
Valid                   Loss: 1.676e+04   Precision: 34.932%   Recall: 30.411%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.99404144 36.41666667]
	 [44.57913208 31.5       ]
	 [32.19922638 50.5       ]
	 [41.09432983 43.96666667]
	 [51.0497818  43.5       ]]
Train   Epoch: 359 / 600   Loss:    5839   Precision: 78.254%   Recall: 64.400%
Valid                   Loss: 1.697e+04   Precision: 31.316%   Recall: 40.727%
Train   Epoch: 360 / 600   Loss:    5865   Precision: 79.279%   Recall: 65.107%
Valid                   Loss: 1.699e+04   Precision: 30.370%   Recall: 38.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.39474106 27.58333333]
	 [54.77734375 32.6       ]
	 [49.81669617 19.18333333]
	 [47.62479782 34.53333333]
	 [55.47595215 56.35      ]]
Train   Epoch: 361 / 600   Loss:    5567   Precision: 79.584%   Recall: 64.602%
Valid                   Loss: 1.699e+04   Precision: 32.031%   Recall: 37.150%
Train   Epoch: 362 / 600   Loss:    5914   Precision: 78.853%   Recall: 64.208%
Valid                   Loss: 1.707e+04   Precision: 30.533%   Recall: 40.310%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.73484802 24.9       ]
	 [52.68635559 61.76666667]
	 [32.54200745 32.11666667]
	 [41.64835739 28.96666667]
	 [43.38726044 43.        ]]
Train   Epoch: 363 / 600   Loss:    5623   Precision: 79.937%   Recall: 66.229%
Valid                   Loss: 1.727e+04   Precision: 30.396%   Recall: 50.388%
Train   Epoch: 364 / 600   Loss:    5604   Precision: 79.670%   Recall: 65.461%
Valid                   Loss: 1.726e+04   Precision: 30.148%   Recall: 48.539%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.53956223 17.75      ]
	 [84.92553711 51.36666667]
	 [57.69909286 89.53333333]
	 [62.78896332 23.25      ]
	 [58.90526581 29.3       ]]
Train   Epoch: 365 / 600   Loss:    5642   Precision: 79.890%   Recall: 64.632%
Valid                   Loss: 1.681e+04   Precision: 34.235%   Recall: 37.746%
Train   Epoch: 366 / 600   Loss:    5657   Precision: 79.148%   Recall: 64.743%
Valid                   Loss: 1.68e+04   Precision: 35.657%   Recall: 29.278%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[23.01592064 12.        ]
	 [39.27961349 85.05      ]
	 [31.06586838 35.45      ]
	 [10.26415634 36.28333333]
	 [32.72090912 48.01666667]]
Train   Epoch: 367 / 600   Loss:    5400   Precision: 79.006%   Recall: 65.521%
Valid                   Loss: 1.679e+04   Precision: 32.441%   Recall: 38.283%
Train   Epoch: 368 / 600   Loss:    5592   Precision: 78.358%   Recall: 64.612%
Valid                   Loss: 1.704e+04   Precision: 32.113%   Recall: 36.076%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[111.61875153  65.91666667]
	 [ 54.760849    58.95      ]
	 [ 46.05229568  46.63333333]
	 [ 35.44975281  42.25      ]
	 [ 79.93289948  53.24      ]]
Train   Epoch: 369 / 600   Loss:    5866   Precision: 78.898%   Recall: 66.006%
Valid                   Loss: 1.738e+04   Precision: 28.149%   Recall: 47.168%
Train   Epoch: 370 / 600   Loss:    5464   Precision: 78.825%   Recall: 66.320%
Valid                   Loss: 1.699e+04   Precision: 31.213%   Recall: 44.186%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.52775574  45.76666667]
	 [ 50.50811768  35.5       ]
	 [ 39.78090668  30.71666667]
	 [ 80.82315826  17.61666667]
	 [345.50439453 311.08333333]]
Train   Epoch: 371 / 600   Loss:    5751   Precision: 78.710%   Recall: 64.966%
Valid                   Loss: 1.704e+04   Precision: 29.828%   Recall: 40.250%
Train   Epoch: 372 / 600   Loss:    5751   Precision: 79.049%   Recall: 64.167%
Valid                   Loss: 1.739e+04   Precision: 28.048%   Recall: 47.883%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.08529663 28.16666667]
	 [68.44115448 10.05      ]
	 [50.46445465 54.95      ]
	 [38.18600082 21.33333333]
	 [51.44976044 76.91666667]]
Train   Epoch: 373 / 600   Loss:    6028   Precision: 76.034%   Recall: 65.400%
Valid                   Loss: 1.715e+04   Precision: 30.788%   Recall: 38.462%
Train   Epoch: 374 / 600   Loss:    5661   Precision: 79.066%   Recall: 65.036%
Valid                   Loss: 1.705e+04   Precision: 32.715%   Recall: 42.039%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.78267288 48.43333333]
	 [40.65437698 36.33333333]
	 [74.47684479 77.66666667]
	 [44.22542953 26.9       ]
	 [70.18149567 84.78333333]]
Train   Epoch: 375 / 600   Loss:    5781   Precision: 78.987%   Recall: 65.218%
Valid                   Loss: 1.716e+04   Precision: 29.804%   Recall: 41.622%
Train   Epoch: 376 / 600   Loss:    5601   Precision: 79.550%   Recall: 65.804%
Valid                   Loss: 1.702e+04   Precision: 30.999%   Recall: 39.595%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[321.78588867 248.66666667]
	 [ 43.29209137  31.38333333]
	 [ 17.433218    17.66666667]
	 [ 35.46163559  36.36666667]
	 [ 42.06853104  29.36666667]]
Train   Epoch: 377 / 600   Loss:    5194   Precision: 79.721%   Recall: 64.673%
Valid                   Loss: 1.706e+04   Precision: 29.984%   Recall: 45.558%
Train   Epoch: 378 / 600   Loss:    5183   Precision: 80.400%   Recall: 66.694%
Valid                   Loss: 1.704e+04   Precision: 30.747%   Recall: 38.044%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[71.52321625 86.98333333]
	 [40.6016655  57.81666667]
	 [45.74499512 44.53333333]
	 [47.2488327  56.38333333]
	 [35.87994385 47.33333333]]
Train   Epoch: 379 / 600   Loss:    5445   Precision: 80.408%   Recall: 65.734%
Valid                   Loss: 1.712e+04   Precision: 31.208%   Recall: 38.521%
Train   Epoch: 380 / 600   Loss:    5332   Precision: 80.811%   Recall: 66.087%
Valid                   Loss: 1.684e+04   Precision: 33.158%   Recall: 37.507%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[189.65284729 203.71666667]
	 [ 50.00858307  57.01666667]
	 [ 47.60473251  38.21666667]
	 [ 95.22248077 145.58333333]
	 [ 44.96739578  19.71666667]]
Train   Epoch: 381 / 600   Loss:    5654   Precision: 78.874%   Recall: 66.966%
Valid                   Loss: 1.727e+04   Precision: 28.977%   Recall: 51.699%
Train   Epoch: 382 / 600   Loss:    5346   Precision: 79.888%   Recall: 66.431%
Valid                   Loss: 1.713e+04   Precision: 31.175%   Recall: 45.081%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.5633316  76.61666667]
	 [50.32816696 66.15      ]
	 [46.82770538 34.5       ]
	 [50.82566071 34.5       ]
	 [48.6470871  17.11666667]]
Train   Epoch: 383 / 600   Loss:    5208   Precision: 79.489%   Recall: 65.400%
Valid                   Loss: 1.705e+04   Precision: 30.499%   Recall: 37.209%
Train   Epoch: 384 / 600   Loss:    5213   Precision: 79.944%   Recall: 66.178%
Valid                   Loss: 1.723e+04   Precision: 29.269%   Recall: 48.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[73.29535675 25.78333333]
	 [40.94317627 52.        ]
	 [63.79563522 20.        ]
	 [31.89146423 19.86666667]
	 [41.49136734 44.88333333]]
Train   Epoch: 385 / 600   Loss:    5206   Precision: 80.649%   Recall: 66.754%
Valid                   Loss: 1.707e+04   Precision: 33.016%   Recall: 35.122%
Train   Epoch: 386 / 600   Loss:    4883   Precision: 80.628%   Recall: 66.916%
Valid                   Loss: 1.726e+04   Precision: 30.776%   Recall: 44.007%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.89518356  76.        ]
	 [ 55.94309998  40.91666667]
	 [477.14172363 479.58333333]
	 [ 40.89626312  30.55      ]
	 [ 40.80183411  48.26666667]]
Train   Epoch: 387 / 600   Loss:    5080   Precision: 78.801%   Recall: 65.208%
Valid                   Loss: 1.731e+04   Precision: 29.575%   Recall: 44.425%
Train   Epoch: 388 / 600   Loss:    4959   Precision: 80.231%   Recall: 66.108%
Valid                   Loss: 1.712e+04   Precision: 31.086%   Recall: 39.595%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.12228012  80.33333333]
	 [ 40.36936951  30.15      ]
	 [ 52.64590836  54.16666667]
	 [117.1204071   53.61666667]
	 [ 38.28902435  17.45      ]]
Train   Epoch: 389 / 600   Loss:    4814   Precision: 80.705%   Recall: 67.371%
Valid                   Loss: 1.727e+04   Precision: 33.609%   Recall: 29.100%
Train   Epoch: 390 / 600   Loss:    5421   Precision: 80.464%   Recall: 66.178%
Valid                   Loss: 1.693e+04   Precision: 28.590%   Recall: 52.475%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.27404022  35.95      ]
	 [ 56.58439255  45.78333333]
	 [ 43.40815353  27.45      ]
	 [ 56.09675598  33.11666667]
	 [131.97424316  92.8       ]]
Train   Epoch: 391 / 600   Loss:    5036   Precision: 79.990%   Recall: 66.613%
Valid                   Loss: 1.73e+04   Precision: 31.444%   Recall: 40.370%
Train   Epoch: 392 / 600   Loss:    5274   Precision: 80.059%   Recall: 65.521%
Valid                   Loss: 1.704e+04   Precision: 29.729%   Recall: 41.801%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 30.06171036  32.03333333]
	 [ 83.96842957  72.86666667]
	 [ 49.6608963   50.15      ]
	 [ 44.43883896  15.        ]
	 [217.5682373  234.5       ]]
Train   Epoch: 393 / 600   Loss:    5103   Precision: 80.212%   Recall: 66.603%
Valid                   Loss: 1.706e+04   Precision: 32.079%   Recall: 36.613%
Train   Epoch: 394 / 600   Loss:    5327   Precision: 79.664%   Recall: 66.148%
Valid                   Loss: 1.703e+04   Precision: 30.791%   Recall: 39.475%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 60.54788971  13.83333333]
	 [373.61114502 354.18333333]
	 [ 48.99124527  17.45      ]
	 [ 81.62274933 110.41666667]
	 [ 90.29230499  88.88333333]]
Train   Epoch: 395 / 600   Loss:    5454   Precision: 78.910%   Recall: 67.300%
Valid                   Loss: 1.702e+04   Precision: 30.583%   Recall: 41.324%
Train   Epoch: 396 / 600   Loss:    5321   Precision: 79.324%   Recall: 64.511%
Valid                   Loss: 1.729e+04   Precision: 31.788%   Recall: 40.489%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[65.4958725  38.71666667]
	 [43.36133575 22.45      ]
	 [29.8240242  83.08333333]
	 [38.00964355 50.13333333]
	 [44.09206009 53.66666667]]
Train   Epoch: 397 / 600   Loss:    4959   Precision: 79.851%   Recall: 68.118%
Valid                   Loss: 1.743e+04   Precision: 31.627%   Recall: 40.906%
Train   Epoch: 398 / 600   Loss:    4795   Precision: 81.315%   Recall: 67.462%
Valid                   Loss: 1.716e+04   Precision: 31.910%   Recall: 33.870%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[20.16952324 15.46666667]
	 [40.76348114 28.76666667]
	 [30.74497795 49.1       ]
	 [72.97644806 56.93333333]
	 [65.09976196 82.5       ]]
Train   Epoch: 399 / 600   Loss:    4719   Precision: 81.486%   Recall: 66.714%
Valid                   Loss: 1.709e+04   Precision: 31.205%   Recall: 41.383%
Train   Epoch: 400 / 600   Loss:    5011   Precision: 81.752%   Recall: 67.724%
Valid                   Loss: 1.735e+04   Precision: 29.702%   Recall: 40.966%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[242.59571838 259.88333333]
	 [ 39.95559692  30.03333333]
	 [187.54608154  15.33333333]
	 [ 70.62215424  43.33333333]
	 [ 29.9082737   32.83333333]]
Train   Epoch: 401 / 600   Loss:    5055   Precision: 81.289%   Recall: 66.512%
Valid                   Loss: 1.716e+04   Precision: 29.856%   Recall: 46.929%
Train   Epoch: 402 / 600   Loss:    4719   Precision: 80.504%   Recall: 66.886%
Valid                   Loss: 1.724e+04   Precision: 32.080%   Recall: 41.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.36959457  39.        ]
	 [104.89272308  27.33333333]
	 [ 52.35507202  63.65      ]
	 [ 26.02765465  36.06666667]
	 [ 44.42711639  19.66666667]]
Train   Epoch: 403 / 600   Loss:    4585   Precision: 82.190%   Recall: 67.057%
Valid                   Loss: 1.709e+04   Precision: 32.698%   Recall: 28.623%
Train   Epoch: 404 / 600   Loss:    4945   Precision: 80.640%   Recall: 66.754%
Valid                   Loss: 1.735e+04   Precision: 33.460%   Recall: 31.544%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.73028564 17.86666667]
	 [47.18450928 43.88333333]
	 [33.31327057 13.83333333]
	 [11.71240997 23.38333333]
	 [36.81254578 34.11666667]]
Train   Epoch: 405 / 600   Loss:    4707   Precision: 81.781%   Recall: 67.947%
Valid                   Loss: 1.737e+04   Precision: 30.845%   Recall: 45.081%
Train   Epoch: 406 / 600   Loss:    5161   Precision: 78.482%   Recall: 66.785%
Valid                   Loss: 1.748e+04   Precision: 29.760%   Recall: 42.874%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.54521942 39.41666667]
	 [42.27210999 25.        ]
	 [13.4164114  16.38333333]
	 [48.07954788 35.61666667]
	 [61.67028427 29.33333333]]
Train   Epoch: 407 / 600   Loss:    4812   Precision: 80.100%   Recall: 66.704%
Valid                   Loss: 1.731e+04   Precision: 31.047%   Recall: 39.416%
Train   Epoch: 408 / 600   Loss:    5184   Precision: 80.837%   Recall: 65.602%
Valid                   Loss: 1.728e+04   Precision: 29.667%   Recall: 44.067%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[26.92461014 39.16666667]
	 [70.66470337 59.03333333]
	 [39.05802155 20.33333333]
	 [51.04341125 38.91666667]
	 [36.36729431 23.75      ]]
Train   Epoch: 409 / 600   Loss:    5099   Precision: 80.821%   Recall: 67.664%
Valid                   Loss: 1.701e+04   Precision: 32.839%   Recall: 33.035%
Train   Epoch: 410 / 600   Loss:    4842   Precision: 81.365%   Recall: 67.724%
Valid                   Loss: 1.74e+04   Precision: 32.227%   Recall: 37.627%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  34.21839142   20.53333333]
	 [  32.21956635   42.75      ]
	 [1168.90917969 2199.03333333]
	 [  37.26115417   26.71666667]
	 [  70.3883667    61.61666667]]
Train   Epoch: 411 / 600   Loss:    4685   Precision: 80.867%   Recall: 66.330%
Valid                   Loss: 1.71e+04   Precision: 33.544%   Recall: 38.044%
Train   Epoch: 412 / 600   Loss:    5363   Precision: 76.094%   Recall: 66.097%
Valid                   Loss: 1.746e+04   Precision: 29.336%   Recall: 40.549%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[74.77988434 41.13333333]
	 [53.45597076 24.38333333]
	 [64.87156677 55.95      ]
	 [47.02389526 57.2       ]
	 [49.23534012 31.61666667]]
Train   Epoch: 413 / 600   Loss:    5152   Precision: 79.228%   Recall: 66.330%
Valid                   Loss: 1.709e+04   Precision: 31.504%   Recall: 42.099%
Train   Epoch: 414 / 600   Loss:    4638   Precision: 80.179%   Recall: 66.956%
Valid                   Loss: 1.699e+04   Precision: 31.106%   Recall: 32.200%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 36.96600723  85.13333333]
	 [ 35.22232437  24.98333333]
	 [157.09619141 140.        ]
	 [ 50.07921982  13.21666667]
	 [  8.97695446  10.8       ]]
Train   Epoch: 415 / 600   Loss:    5012   Precision: 81.945%   Recall: 66.825%
Valid                   Loss: 1.74e+04   Precision: 31.089%   Recall: 36.613%
Train   Epoch: 416 / 600   Loss:    4877   Precision: 81.551%   Recall: 67.138%
Valid                   Loss: 1.693e+04   Precision: 32.920%   Recall: 34.824%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[24.12249374 22.43333333]
	 [35.55178452 50.86666667]
	 [60.52787018 68.33333333]
	 [88.36097717 89.73333333]
	 [74.56157684 34.95      ]]
Train   Epoch: 417 / 600   Loss:    4648   Precision: 81.302%   Recall: 68.149%
Valid                   Loss: 1.734e+04   Precision: 31.412%   Recall: 36.076%
Train   Epoch: 418 / 600   Loss:    4630   Precision: 82.560%   Recall: 67.977%
Valid                   Loss: 1.721e+04   Precision: 31.250%   Recall: 42.934%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[75.69480896 47.46666667]
	 [35.2809639  35.7       ]
	 [36.11806488 44.66666667]
	 [33.44260788 29.5       ]
	 [49.13740158 47.73333333]]
Train   Epoch: 419 / 600   Loss:    4200   Precision: 81.842%   Recall: 68.321%
Valid                   Loss: 1.713e+04   Precision: 32.339%   Recall: 35.540%
Train   Epoch: 420 / 600   Loss:    4230   Precision: 81.798%   Recall: 68.664%
Valid                   Loss: 1.718e+04   Precision: 31.152%   Recall: 35.480%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 75.83674622  41.93333333]
	 [212.36399841 194.75      ]
	 [ 51.25320053  19.        ]
	 [ 30.38050652  71.26666667]
	 [ 65.01287842  31.        ]]
Train   Epoch: 421 / 600   Loss:    4246   Precision: 83.016%   Recall: 68.755%
Valid                   Loss: 1.742e+04   Precision: 31.204%   Recall: 33.214%
Train   Epoch: 422 / 600   Loss:    4272   Precision: 83.031%   Recall: 69.321%
Valid                   Loss: 1.734e+04   Precision: 33.718%   Recall: 34.824%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 22.9968853   23.83333333]
	 [120.25879669 103.21666667]
	 [ 41.36569595  25.55      ]
	 [ 41.12301254  52.2       ]
	 [ 33.7874527   62.1       ]]
Train   Epoch: 423 / 600   Loss:    4328   Precision: 82.817%   Recall: 68.038%
Valid                   Loss: 1.71e+04   Precision: 32.876%   Recall: 28.563%
Train   Epoch: 424 / 600   Loss:    5008   Precision: 78.073%   Recall: 68.108%
Valid                   Loss: 1.708e+04   Precision: 33.212%   Recall: 32.737%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  33.63870239   33.23333333]
	 [  27.24455833    8.66666667]
	 [  64.16254425   54.86666667]
	 [1232.33459473 1539.26666667]
	 [  64.26010132   17.66666667]]
Train   Epoch: 425 / 600   Loss:    5366   Precision: 77.370%   Recall: 65.471%
Valid                   Loss: 1.772e+04   Precision: 32.566%   Recall: 34.586%
Train   Epoch: 426 / 600   Loss:    4654   Precision: 80.838%   Recall: 66.673%
Valid                   Loss: 1.731e+04   Precision: 30.189%   Recall: 41.026%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.138237    69.        ]
	 [ 48.6895256   16.63333333]
	 [ 53.30354309  32.75      ]
	 [ 34.68832779  19.        ]
	 [112.92357635 121.21666667]]
Train   Epoch: 427 / 600   Loss:    4621   Precision: 80.927%   Recall: 66.714%
Valid                   Loss: 1.738e+04   Precision: 31.125%   Recall: 34.466%
Train   Epoch: 428 / 600   Loss:    4390   Precision: 81.275%   Recall: 67.502%
Valid                   Loss: 1.775e+04   Precision: 30.474%   Recall: 38.342%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.57657623 63.78333333]
	 [36.31477737 18.08333333]
	 [44.497715   43.91666667]
	 [45.11997223 30.53333333]
	 [56.32807922 35.55      ]]
Train   Epoch: 429 / 600   Loss:    4250   Precision: 80.979%   Recall: 66.552%
Valid                   Loss: 1.731e+04   Precision: 30.998%   Recall: 36.673%
Train   Epoch: 430 / 600   Loss:    4186   Precision: 82.159%   Recall: 67.896%
Valid                   Loss: 1.734e+04   Precision: 31.564%   Recall: 40.429%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.30734634 45.96666667]
	 [32.73625565  8.21666667]
	 [21.32249451 33.73333333]
	 [29.29232788 52.7       ]
	 [54.31970215 27.61666667]]
Train   Epoch: 431 / 600   Loss:    4115   Precision: 82.158%   Recall: 68.260%
Valid                   Loss: 1.74e+04   Precision: 30.240%   Recall: 40.608%
Train   Epoch: 432 / 600   Loss:    4018   Precision: 82.741%   Recall: 69.422%
Valid                   Loss: 1.732e+04   Precision: 30.912%   Recall: 39.595%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[170.19602966 167.5       ]
	 [ 41.85650635  37.5       ]
	 [ 23.99212265  18.5       ]
	 [ 16.77080917  14.21666667]
	 [ 60.35355759  76.        ]]
Train   Epoch: 433 / 600   Loss:    4262   Precision: 83.424%   Recall: 67.997%
Valid                   Loss: 1.746e+04   Precision: 31.654%   Recall: 35.957%
Train   Epoch: 434 / 600   Loss:    3914   Precision: 82.907%   Recall: 69.452%
Valid                   Loss: 1.717e+04   Precision: 34.416%   Recall: 30.948%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 19.35836029  28.95      ]
	 [ 41.30813217  28.78333333]
	 [177.39538574  61.73333333]
	 [ 37.07926559  43.26666667]
	 [  9.19874477  30.61666667]]
Train   Epoch: 435 / 600   Loss:    3632   Precision: 82.621%   Recall: 68.169%
Valid                   Loss: 1.742e+04   Precision: 31.786%   Recall: 35.122%
Train   Epoch: 436 / 600   Loss:    3971   Precision: 82.919%   Recall: 69.068%
Valid                   Loss: 1.723e+04   Precision: 31.441%   Recall: 38.640%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.45998383 57.36666667]
	 [40.66954803 66.3       ]
	 [10.57218742 27.01666667]
	 [37.67795944 41.13333333]
	 [17.15301323 39.58333333]]
Train   Epoch: 437 / 600   Loss:    4009   Precision: 83.849%   Recall: 70.139%
Valid                   Loss: 1.723e+04   Precision: 31.907%   Recall: 34.228%
Train   Epoch: 438 / 600   Loss:    4045   Precision: 83.798%   Recall: 68.674%
Valid                   Loss: 1.745e+04   Precision: 33.647%   Recall: 32.021%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[31.74964714 45.3       ]
	 [29.65754318 21.66666667]
	 [40.88225555 25.5       ]
	 [73.7330246  30.28333333]
	 [44.09541702 38.28333333]]
Train   Epoch: 439 / 600   Loss:    3924   Precision: 83.743%   Recall: 69.594%
Valid                   Loss: 1.749e+04   Precision: 29.721%   Recall: 42.516%
Train   Epoch: 440 / 600   Loss:    3921   Precision: 82.787%   Recall: 68.967%
Valid                   Loss: 1.726e+04   Precision: 31.174%   Recall: 30.411%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.29486084 33.1       ]
	 [36.5700798  38.86666667]
	 [26.0689106  26.875     ]
	 [40.68733978 19.5       ]
	 [35.26717377 22.03333333]]
Train   Epoch: 441 / 600   Loss:    3863   Precision: 83.047%   Recall: 69.250%
Valid                   Loss: 1.726e+04   Precision: 29.746%   Recall: 46.810%
Train   Epoch: 442 / 600   Loss:    3800   Precision: 83.119%   Recall: 69.210%
Valid                   Loss: 1.778e+04   Precision: 30.073%   Recall: 39.237%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.20265579 25.        ]
	 [42.55191803 21.58333333]
	 [35.51986694 51.91666667]
	 [52.35028458 45.83333333]
	 [33.45247269 28.48333333]]
Train   Epoch: 443 / 600   Loss:    4016   Precision: 83.354%   Recall: 68.310%
Valid                   Loss: 1.751e+04   Precision: 28.525%   Recall: 47.406%
Train   Epoch: 444 / 600   Loss:    4061   Precision: 81.163%   Recall: 68.098%
Valid                   Loss: 1.75e+04   Precision: 30.637%   Recall: 38.402%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[25.18373108 36.41666667]
	 [43.50757217 57.08333333]
	 [22.59533119 19.88333333]
	 [61.27022552 31.58333333]
	 [46.1686058  29.08333333]]
Train   Epoch: 445 / 600   Loss:    3861   Precision: 82.578%   Recall: 68.492%
Valid                   Loss: 1.737e+04   Precision: 32.948%   Recall: 30.590%
Train   Epoch: 446 / 600   Loss:    4016   Precision: 81.909%   Recall: 68.947%
Valid                   Loss: 1.706e+04   Precision: 31.615%   Recall: 38.402%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.39481544 25.45      ]
	 [ 1.40538502 11.33333333]
	 [46.6729126  75.28333333]
	 [35.65795517 40.11666667]
	 [29.02319908 33.25      ]]
Train   Epoch: 447 / 600   Loss:    4051   Precision: 81.947%   Recall: 69.766%
Valid                   Loss: 1.719e+04   Precision: 33.884%   Recall: 28.145%
Train   Epoch: 448 / 600   Loss:    3910   Precision: 83.077%   Recall: 67.815%
Valid                   Loss: 1.773e+04   Precision: 31.377%   Recall: 40.489%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.96727753  81.5       ]
	 [ 36.29177475  43.25      ]
	 [ 25.67094994  21.08333333]
	 [ 44.87526321  45.06666667]
	 [441.24337769 445.75      ]]
Train   Epoch: 449 / 600   Loss:    3946   Precision: 83.259%   Recall: 69.755%
Valid                   Loss: 1.747e+04   Precision: 32.179%   Recall: 38.223%
Train   Epoch: 450 / 600   Loss:    3953   Precision: 82.485%   Recall: 67.148%
Valid                   Loss: 1.732e+04   Precision: 31.965%   Recall: 37.150%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[84.35225677 65.65      ]
	 [29.18470955 17.13333333]
	 [54.69497299 33.65      ]
	 [35.03365326 43.06666667]
	 [51.29879761 44.55      ]]
Train   Epoch: 451 / 600   Loss:    3812   Precision: 82.222%   Recall: 67.300%
Valid                   Loss: 1.726e+04   Precision: 34.620%   Recall: 31.544%
Train   Epoch: 452 / 600   Loss:    3689   Precision: 82.457%   Recall: 68.583%
Valid                   Loss: 1.707e+04   Precision: 32.913%   Recall: 32.677%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[22.88521957 36.03333333]
	 [26.80694389 31.63333333]
	 [38.49866486 39.33333333]
	 [24.02645111 10.25      ]
	 [35.78987122 42.2       ]]
Train   Epoch: 453 / 600   Loss:    3694   Precision: 83.508%   Recall: 69.331%
Valid                   Loss: 1.725e+04   Precision: 31.267%   Recall: 33.691%
Train   Epoch: 454 / 600   Loss:    3935   Precision: 83.641%   Recall: 69.078%
Valid                   Loss: 1.757e+04   Precision: 31.963%   Recall: 31.067%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.32940674 47.11666667]
	 [42.17158127 30.66666667]
	 [27.04935265 51.75      ]
	 [65.3924942  66.16666667]
	 [61.36036682 10.        ]]
Train   Epoch: 455 / 600   Loss:    4509   Precision: 79.120%   Recall: 66.704%
Valid                   Loss: 1.717e+04   Precision: 29.636%   Recall: 39.356%
Train   Epoch: 456 / 600   Loss:    3572   Precision: 82.023%   Recall: 69.250%
Valid                   Loss: 1.721e+04   Precision: 34.294%   Recall: 26.953%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.2839241  30.81666667]
	 [30.80447578 32.95      ]
	 [50.68682861 51.15      ]
	 [36.42383957 30.46666667]
	 [ 2.60308146 23.35      ]]
Train   Epoch: 457 / 600   Loss:    4075   Precision: 83.063%   Recall: 69.331%
Valid                   Loss: 1.682e+04   Precision: 36.390%   Recall: 22.719%
Train   Epoch: 458 / 600   Loss:    3952   Precision: 82.691%   Recall: 67.876%
Valid                   Loss: 1.759e+04   Precision: 31.409%   Recall: 35.361%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.52657318  3.7       ]
	 [50.56793976 19.46666667]
	 [ 5.74520016 38.78333333]
	 [92.57875061 15.6       ]
	 [11.04893398 18.2       ]]
Train   Epoch: 459 / 600   Loss:    3501   Precision: 83.339%   Recall: 68.543%
Valid                   Loss: 1.685e+04   Precision: 31.201%   Recall: 33.453%
Train   Epoch: 460 / 600   Loss:    3781   Precision: 82.641%   Recall: 69.371%
Valid                   Loss: 1.706e+04   Precision: 32.922%   Recall: 27.013%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 32.24013519  59.5       ]
	 [ 23.31819916  18.38333333]
	 [ 47.39702988 121.66666667]
	 [ 51.99222183  37.78333333]
	 [  0.49533713  13.83333333]]
Train   Epoch: 461 / 600   Loss:    3787   Precision: 81.893%   Recall: 66.542%
Valid                   Loss: 1.73e+04   Precision: 30.252%   Recall: 32.976%
Train   Epoch: 462 / 600   Loss:    3974   Precision: 80.324%   Recall: 67.734%
Valid                   Loss: 1.721e+04   Precision: 29.577%   Recall: 39.595%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.17357635  50.16666667]
	 [ 41.82235718  39.25      ]
	 [ 56.98686218  43.28333333]
	 [ 40.19049835  19.83333333]
	 [128.12425232 118.65      ]]
Train   Epoch: 463 / 600   Loss:    3988   Precision: 82.229%   Recall: 66.492%
Valid                   Loss: 1.708e+04   Precision: 33.094%   Recall: 30.292%
Train   Epoch: 464 / 600   Loss:    3828   Precision: 83.879%   Recall: 68.614%
Valid                   Loss: 1.685e+04   Precision: 31.741%   Recall: 33.274%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.46724319 34.55      ]
	 [11.40661716 19.26666667]
	 [39.06748581 39.        ]
	 [28.60892105 47.16666667]
	 [36.50345993 29.58333333]]
Train   Epoch: 465 / 600   Loss:    3473   Precision: 82.814%   Recall: 70.362%
Valid                   Loss: 1.713e+04   Precision: 32.621%   Recall: 28.205%
Train   Epoch: 466 / 600   Loss:    3220   Precision: 84.433%   Recall: 70.372%
Valid                   Loss: 1.705e+04   Precision: 30.845%   Recall: 35.480%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.42892838 54.91666667]
	 [55.05543518 68.2       ]
	 [94.3129425  77.16666667]
	 [42.19189072 28.75      ]
	 [39.61117935 39.7       ]]
Train   Epoch: 467 / 600   Loss:    3704   Precision: 84.455%   Recall: 70.766%
Valid                   Loss: 1.724e+04   Precision: 31.533%   Recall: 34.466%
Train   Epoch: 468 / 600   Loss:    3564   Precision: 83.456%   Recall: 69.735%
Valid                   Loss: 1.693e+04   Precision: 36.664%   Recall: 26.476%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.53343201 48.46666667]
	 [33.09310913 22.58333333]
	 [42.117836   31.88333333]
	 [37.18134308 47.16666667]
	 [34.19405365 22.03333333]]
Train   Epoch: 469 / 600   Loss:    3327   Precision: 83.550%   Recall: 68.826%
Valid                   Loss: 1.757e+04   Precision: 29.601%   Recall: 38.462%
Train   Epoch: 470 / 600   Loss:    4076   Precision: 81.370%   Recall: 69.958%
Valid                   Loss: 1.724e+04   Precision: 31.572%   Recall: 29.577%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.60767365 21.75      ]
	 [17.99816513 37.21666667]
	 [46.53018188 26.55      ]
	 [37.46498108 38.68333333]
	 [54.74869919 22.7       ]]
Train   Epoch: 471 / 600   Loss:    3780   Precision: 83.874%   Recall: 69.533%
Valid                   Loss: 1.71e+04   Precision: 30.512%   Recall: 32.677%
Train   Epoch: 472 / 600   Loss:    3622   Precision: 84.278%   Recall: 69.988%
Valid                   Loss: 1.726e+04   Precision: 30.212%   Recall: 38.283%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[18.52480125 27.83333333]
	 [58.7663765  32.4       ]
	 [73.63560486 69.23333333]
	 [45.4818573  56.88333333]
	 [27.78118134 45.76666667]]
Train   Epoch: 473 / 600   Loss:    3595   Precision: 82.842%   Recall: 67.866%
Valid                   Loss: 1.703e+04   Precision: 31.638%   Recall: 33.751%
Train   Epoch: 474 / 600   Loss:    3654   Precision: 81.900%   Recall: 69.503%
Valid                   Loss: 1.699e+04   Precision: 30.945%   Recall: 37.865%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.76987076 26.        ]
	 [ 1.26271641 14.08333333]
	 [43.18488693 44.88333333]
	 [37.55284882 33.33333333]
	 [19.42666817 52.2       ]]
Train   Epoch: 475 / 600   Loss:    3391   Precision: 83.208%   Recall: 69.503%
Valid                   Loss: 1.738e+04   Precision: 31.614%   Recall: 34.347%
Train   Epoch: 476 / 600   Loss:    3470   Precision: 83.263%   Recall: 69.422%
Valid                   Loss: 1.716e+04   Precision: 31.366%   Recall: 35.182%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.7002449  33.66666667]
	 [50.86257553 32.        ]
	 [71.96051025 17.55      ]
	 [91.27964783 46.9       ]
	 [29.00108528 28.01666667]]
Train   Epoch: 477 / 600   Loss:    3352   Precision: 84.392%   Recall: 71.686%
Valid                   Loss: 1.742e+04   Precision: 30.435%   Recall: 39.654%
Train   Epoch: 478 / 600   Loss:    3213   Precision: 84.449%   Recall: 69.634%
Valid                   Loss: 1.74e+04   Precision: 32.400%   Recall: 36.553%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.1833725  23.        ]
	 [44.87668991 35.9       ]
	 [39.77655792 47.16666667]
	 [36.84613037 27.38333333]
	 [35.53826904 20.        ]]
Train   Epoch: 479 / 600   Loss:    3186   Precision: 84.845%   Recall: 70.432%
Valid                   Loss: 1.732e+04   Precision: 34.689%   Recall: 31.544%
Train   Epoch: 480 / 600   Loss:    3318   Precision: 84.758%   Recall: 70.918%
Valid                   Loss: 1.716e+04   Precision: 32.371%   Recall: 31.425%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.83744049 31.45      ]
	 [33.53733063 70.75      ]
	 [36.57429123 18.91666667]
	 [25.26334572 29.75      ]
	 [38.12477112 21.11666667]]
Train   Epoch: 481 / 600   Loss:    2922   Precision: 84.874%   Recall: 71.332%
Valid                   Loss: 1.702e+04   Precision: 31.811%   Recall: 34.466%
Train   Epoch: 482 / 600   Loss:    3103   Precision: 84.285%   Recall: 70.726%
Valid                   Loss: 1.72e+04   Precision: 31.832%   Recall: 30.352%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 28.7928772   26.33333333]
	 [ 78.6374054   77.        ]
	 [ 44.26522827  22.78333333]
	 [ 39.12007141  33.33333333]
	 [216.06646729 264.03333333]]
Train   Epoch: 483 / 600   Loss:    3183   Precision: 85.092%   Recall: 71.463%
Valid                   Loss: 1.706e+04   Precision: 31.558%   Recall: 33.572%
Train   Epoch: 484 / 600   Loss:    3093   Precision: 84.670%   Recall: 71.999%
Valid                   Loss: 1.723e+04   Precision: 30.387%   Recall: 37.925%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[74.563591   85.21666667]
	 [51.95063019 34.33333333]
	 [50.29239655 71.2       ]
	 [13.47855377 20.58333333]
	 [42.64550781 48.08333333]]
Train   Epoch: 485 / 600   Loss:    2955   Precision: 84.993%   Recall: 70.624%
Valid                   Loss: 1.731e+04   Precision: 32.978%   Recall: 33.214%
Train   Epoch: 486 / 600   Loss:    2913   Precision: 85.174%   Recall: 71.170%
Valid                   Loss: 1.716e+04   Precision: 32.198%   Recall: 31.008%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[81.21411896 73.65      ]
	 [37.80040741 38.83333333]
	 [38.24149704 27.63333333]
	 [40.8650589  34.21666667]
	 [39.06312561 20.58333333]]
Train   Epoch: 487 / 600   Loss:    2908   Precision: 84.436%   Recall: 71.211%
Valid                   Loss: 1.723e+04   Precision: 31.755%   Recall: 35.182%
Train   Epoch: 488 / 600   Loss:    2876   Precision: 85.219%   Recall: 70.786%
Valid                   Loss: 1.803e+04   Precision: 27.628%   Recall: 48.122%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.72433472 65.78333333]
	 [55.65754318 35.88333333]
	 [54.42235947 34.18333333]
	 [62.80053711 20.11666667]
	 [57.50058746 34.08333333]]
Train   Epoch: 489 / 600   Loss:    3728   Precision: 80.969%   Recall: 68.361%
Valid                   Loss: 1.724e+04   Precision: 28.627%   Recall: 39.535%
Train   Epoch: 490 / 600   Loss:    3393   Precision: 84.140%   Recall: 70.230%
Valid                   Loss: 1.766e+04   Precision: 30.789%   Recall: 29.100%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[12.49646187 44.81666667]
	 [28.94782066 22.38333333]
	 [46.29119873 54.2       ]
	 [35.7587471  35.        ]
	 [-0.36108786 29.05      ]]
Train   Epoch: 491 / 600   Loss:    3536   Precision: 83.963%   Recall: 68.937%
Valid                   Loss: 1.703e+04   Precision: 31.334%   Recall: 37.388%
Train   Epoch: 492 / 600   Loss:    3198   Precision: 84.221%   Recall: 70.281%
Valid                   Loss: 1.736e+04   Precision: 31.442%   Recall: 36.017%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 18.54265404  17.55      ]
	 [ 74.77484894  55.55      ]
	 [230.54200745 269.03333333]
	 [ 44.33922577  17.11666667]
	 [ 49.30235672 106.46666667]]
Train   Epoch: 493 / 600   Loss:    3114   Precision: 85.128%   Recall: 70.796%
Valid                   Loss: 1.702e+04   Precision: 32.973%   Recall: 30.888%
Train   Epoch: 494 / 600   Loss:    3101   Precision: 85.281%   Recall: 71.312%
Valid                   Loss: 1.729e+04   Precision: 29.633%   Recall: 38.521%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.54624939 51.5       ]
	 [20.19577789 21.33333333]
	 [39.59949493 22.26666667]
	 [89.84659576 76.25      ]
	 [33.25600433 30.61666667]]
Train   Epoch: 495 / 600   Loss:    2840   Precision: 85.327%   Recall: 72.282%
Valid                   Loss: 1.68e+04   Precision: 32.998%   Recall: 33.274%
Train   Epoch: 496 / 600   Loss:    2979   Precision: 85.555%   Recall: 71.221%
Valid                   Loss: 1.704e+04   Precision: 32.213%   Recall: 37.150%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.16449738 53.23333333]
	 [53.35395432 26.66666667]
	 [62.43309784 43.03333333]
	 [73.23839569 40.11666667]
	 [44.96921158 32.        ]]
Train   Epoch: 497 / 600   Loss:    2793   Precision: 85.792%   Recall: 70.412%
Valid                   Loss: 1.699e+04   Precision: 33.888%   Recall: 26.714%
Train   Epoch: 498 / 600   Loss:    2915   Precision: 85.101%   Recall: 71.746%
Valid                   Loss: 1.708e+04   Precision: 32.885%   Recall: 32.081%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.5729866  33.11666667]
	 [58.57839203 35.5       ]
	 [19.6454277  22.65      ]
	 [49.04151535 44.5       ]
	 [31.70620728 30.88333333]]
Train   Epoch: 499 / 600   Loss:    2928   Precision: 85.505%   Recall: 71.231%
Valid                   Loss: 1.707e+04   Precision: 33.898%   Recall: 31.008%
Train   Epoch: 500 / 600   Loss:    3075   Precision: 81.759%   Recall: 70.069%
Valid                   Loss: 1.759e+04   Precision: 31.960%   Recall: 26.357%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.50286102 46.        ]
	 [51.89950562 41.08333333]
	 [53.48781204 37.45      ]
	 [38.01064682 27.46666667]
	 [22.64232445 23.25      ]]
Train   Epoch: 501 / 600   Loss:    3280   Precision: 84.865%   Recall: 68.785%
Valid                   Loss: 1.723e+04   Precision: 30.385%   Recall: 33.393%
Train   Epoch: 502 / 600   Loss:    2839   Precision: 85.398%   Recall: 71.807%
Valid                   Loss: 1.697e+04   Precision: 31.872%   Recall: 27.311%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.69212341  68.88333333]
	 [ 41.64538956  29.5       ]
	 [ 27.02048874  15.08333333]
	 [ 34.97053146  25.08333333]
	 [124.88554382  37.48333333]]
Train   Epoch: 503 / 600   Loss:    3229   Precision: 82.781%   Recall: 70.150%
Valid                   Loss: 1.773e+04   Precision: 30.769%   Recall: 34.586%
Train   Epoch: 504 / 600   Loss:    3505   Precision: 82.751%   Recall: 69.129%
Valid                   Loss: 1.738e+04   Precision: 30.068%   Recall: 39.535%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.98386002  24.55      ]
	 [104.62388611  71.21666667]
	 [ 59.96964645  69.73333333]
	 [ 38.61865616  32.01666667]
	 [ 50.62046432  49.71666667]]
Train   Epoch: 505 / 600   Loss:    3261   Precision: 83.819%   Recall: 70.352%
Valid                   Loss: 1.707e+04   Precision: 31.667%   Recall: 31.723%
Train   Epoch: 506 / 600   Loss:    2874   Precision: 85.779%   Recall: 71.008%
Valid                   Loss: 1.722e+04   Precision: 34.169%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.54153824 57.        ]
	 [64.57750702 40.88333333]
	 [60.63056183 83.83333333]
	 [50.56050491 16.75      ]
	 [17.31094742 12.51666667]]
Train   Epoch: 507 / 600   Loss:    2699   Precision: 85.793%   Recall: 72.191%
Valid                   Loss: 1.724e+04   Precision: 31.198%   Recall: 42.695%
Train   Epoch: 508 / 600   Loss:    3073   Precision: 83.836%   Recall: 70.493%
Valid                   Loss: 1.703e+04   Precision: 31.697%   Recall: 36.971%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 72.18740082  41.66666667]
	 [ 11.04769325  33.25      ]
	 [ 41.15876007  48.08333333]
	 [ 38.54512405  65.66666667]
	 [ 84.14863586 115.3       ]]
Train   Epoch: 509 / 600   Loss:    3026   Precision: 84.283%   Recall: 68.765%
Valid                   Loss: 1.757e+04   Precision: 30.561%   Recall: 37.030%
Train   Epoch: 510 / 600   Loss:    2982   Precision: 84.038%   Recall: 72.302%
Valid                   Loss: 1.717e+04   Precision: 33.975%   Recall: 26.297%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.96149826 28.95      ]
	 [62.51816177 35.4       ]
	 [22.15565491 38.03333333]
	 [32.43311691 42.13333333]
	 [41.07378769 52.66666667]]
Train   Epoch: 511 / 600   Loss:    3084   Precision: 84.440%   Recall: 70.139%
Valid                   Loss: 1.723e+04   Precision: 34.892%   Recall: 28.921%
Train   Epoch: 512 / 600   Loss:    3098   Precision: 84.028%   Recall: 70.281%
Valid                   Loss: 1.736e+04   Precision: 31.864%   Recall: 39.237%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.92724228 40.66666667]
	 [61.96233749 46.3       ]
	 [65.25789642 31.65      ]
	 [42.44974899 22.66666667]
	 [45.90880203 25.83333333]]
Train   Epoch: 513 / 600   Loss:    3006   Precision: 85.051%   Recall: 70.372%
Valid                   Loss: 1.708e+04   Precision: 31.609%   Recall: 32.797%
Train   Epoch: 514 / 600   Loss:    3216   Precision: 83.223%   Recall: 70.180%
Valid                   Loss: 1.753e+04   Precision: 33.121%   Recall: 27.967%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  27.57517052   30.25      ]
	 [  19.92338753   29.66666667]
	 [  20.22256851   17.33333333]
	 [  23.85461998   57.66666667]
	 [1351.54638672 1484.26666667]]
Train   Epoch: 515 / 600   Loss:    3182   Precision: 83.529%   Recall: 69.846%
Valid                   Loss: 1.709e+04   Precision: 32.703%   Recall: 30.948%
Train   Epoch: 516 / 600   Loss:    3108   Precision: 85.478%   Recall: 70.604%
Valid                   Loss: 1.719e+04   Precision: 32.394%   Recall: 31.544%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 41.9897728   46.48333333]
	 [ 25.75786781  43.46666667]
	 [ 26.47887993  53.86666667]
	 [190.68397522 192.75      ]
	 [ 37.80238724  26.75      ]]
Train   Epoch: 517 / 600   Loss:    2823   Precision: 84.582%   Recall: 71.736%
Valid                   Loss: 1.702e+04   Precision: 34.130%   Recall: 30.650%
Train   Epoch: 518 / 600   Loss:    2798   Precision: 85.850%   Recall: 70.079%
Valid                   Loss: 1.724e+04   Precision: 33.540%   Recall: 32.320%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[18.08961105 18.71666667]
	 [24.40300369 45.46666667]
	 [38.39398575 47.66666667]
	 [58.54259109 56.11666667]
	 [38.39388657 58.38333333]]
Train   Epoch: 519 / 600   Loss:    3202   Precision: 83.388%   Recall: 69.897%
Valid                   Loss: 1.726e+04   Precision: 29.549%   Recall: 35.540%
Train   Epoch: 520 / 600   Loss:    2846   Precision: 84.718%   Recall: 70.806%
Valid                   Loss: 1.695e+04   Precision: 33.936%   Recall: 27.967%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[17.41277695 28.11666667]
	 [37.15871811 37.21666667]
	 [47.73411942 41.3       ]
	 [68.64138031 35.21666667]
	 [39.06152725 28.45      ]]
Train   Epoch: 521 / 600   Loss:    2759   Precision: 85.791%   Recall: 72.363%
Valid                   Loss: 1.707e+04   Precision: 32.474%   Recall: 33.810%
Train   Epoch: 522 / 600   Loss:    2765   Precision: 84.843%   Recall: 72.403%
Valid                   Loss: 1.716e+04   Precision: 30.887%   Recall: 38.402%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.20843124 61.88333333]
	 [43.33660507 38.65      ]
	 [56.83680725 20.6       ]
	 [28.19428635 40.11666667]
	 [37.90059662 34.38333333]]
Train   Epoch: 523 / 600   Loss:    2882   Precision: 83.895%   Recall: 69.644%
Valid                   Loss: 1.7e+04   Precision: 29.234%   Recall: 37.984%
Train   Epoch: 524 / 600   Loss:    2760   Precision: 84.069%   Recall: 69.857%
Valid                   Loss: 1.769e+04   Precision: 33.194%   Recall: 37.865%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.31648254 35.        ]
	 [39.72653961 39.        ]
	 [88.5152359  77.75      ]
	 [49.15312958 44.5       ]
	 [35.12987137 47.25      ]]
Train   Epoch: 525 / 600   Loss:    3277   Precision: 83.074%   Recall: 71.221%
Valid                   Loss: 1.7e+04   Precision: 29.515%   Recall: 35.957%
Train   Epoch: 526 / 600   Loss:    2717   Precision: 85.349%   Recall: 69.816%
Valid                   Loss: 1.691e+04   Precision: 35.131%   Recall: 29.517%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.38396835 59.41666667]
	 [17.18381882 21.15      ]
	 [36.29251099 25.46666667]
	 [42.49520111 21.5       ]
	 [37.02059555 16.75      ]]
Train   Epoch: 527 / 600   Loss:    2757   Precision: 85.464%   Recall: 71.473%
Valid                   Loss: 1.723e+04   Precision: 31.309%   Recall: 35.659%
Train   Epoch: 528 / 600   Loss:    2834   Precision: 85.928%   Recall: 70.958%
Valid                   Loss: 1.707e+04   Precision: 32.674%   Recall: 36.434%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.9337616  43.7       ]
	 [66.7250824  63.        ]
	 [43.01612091 63.01666667]
	 [57.55804443 47.96666667]
	 [61.09437943 39.45      ]]
Train   Epoch: 529 / 600   Loss:    2549   Precision: 85.962%   Recall: 72.767%
Valid                   Loss: 1.714e+04   Precision: 34.405%   Recall: 27.430%
Train   Epoch: 530 / 600   Loss:    2670   Precision: 85.747%   Recall: 71.797%
Valid                   Loss: 1.699e+04   Precision: 33.992%   Recall: 29.756%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[25.69094086 28.88333333]
	 [74.01125336 20.26666667]
	 [21.55962944 20.66666667]
	 [41.13410187 30.48333333]
	 [30.56112862 30.83333333]]
Train   Epoch: 531 / 600   Loss:    4107   Precision: 79.167%   Recall: 70.119%
Valid                   Loss: 1.878e+04   Precision: 28.032%   Recall: 46.035%
Train   Epoch: 532 / 600   Loss:    3935   Precision: 79.915%   Recall: 66.340%
Valid                   Loss: 1.775e+04   Precision: 29.109%   Recall: 38.760%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.01425171  69.        ]
	 [ 62.2293396   49.26666667]
	 [493.92871094 306.11666667]
	 [ 42.14151764  12.78333333]
	 [ 28.14050484  19.7       ]]
Train   Epoch: 533 / 600   Loss:    2694   Precision: 84.576%   Recall: 69.816%
Valid                   Loss: 1.75e+04   Precision: 31.344%   Recall: 33.512%
Train   Epoch: 534 / 600   Loss:    2554   Precision: 85.196%   Recall: 69.786%
Valid                   Loss: 1.734e+04   Precision: 31.219%   Recall: 33.453%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.89136505 23.25      ]
	 [14.27906609 24.91666667]
	 [53.65471649 31.9       ]
	 [26.29960823 37.5       ]
	 [13.26095295 37.78333333]]
Train   Epoch: 535 / 600   Loss:    2782   Precision: 84.742%   Recall: 71.554%
Valid                   Loss: 1.723e+04   Precision: 30.944%   Recall: 35.003%
Train   Epoch: 536 / 600   Loss:    2804   Precision: 86.079%   Recall: 70.109%
Valid                   Loss: 1.735e+04   Precision: 33.791%   Recall: 32.320%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.01626587 28.26666667]
	 [23.37270164 28.95      ]
	 [50.17251968 13.66666667]
	 [16.76413345  6.31666667]
	 [ 7.19976711 39.33333333]]
Train   Epoch: 537 / 600   Loss:    2748   Precision: 85.792%   Recall: 71.878%
Valid                   Loss: 1.701e+04   Precision: 34.125%   Recall: 28.265%
Train   Epoch: 538 / 600   Loss:    2641   Precision: 85.721%   Recall: 70.493%
Valid                   Loss: 1.732e+04   Precision: 32.918%   Recall: 36.196%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 64.84550476  64.83333333]
	 [ 59.74142075  48.66666667]
	 [122.26783752 128.58333333]
	 [ 99.41440582  60.25      ]
	 [ 37.2004509   40.2       ]]
Train   Epoch: 539 / 600   Loss:    2456   Precision: 85.947%   Recall: 71.938%
Valid                   Loss: 1.716e+04   Precision: 32.333%   Recall: 34.705%
Train   Epoch: 540 / 600   Loss:    2531   Precision: 86.773%   Recall: 71.332%
Valid                   Loss: 1.71e+04   Precision: 33.231%   Recall: 32.200%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 22.60489845  25.65      ]
	 [299.0138855  291.88333333]
	 [ 44.47214127  27.21666667]
	 [ 17.95930099   5.11666667]
	 [ 53.51975632  59.13333333]]
Train   Epoch: 541 / 600   Loss:    2484   Precision: 86.064%   Recall: 71.766%
Valid                   Loss: 1.716e+04   Precision: 31.361%   Recall: 34.764%
Train   Epoch: 542 / 600   Loss:    2372   Precision: 86.526%   Recall: 71.898%
Valid                   Loss: 1.712e+04   Precision: 32.173%   Recall: 31.962%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 33.50619888  17.        ]
	 [ 47.11365128  45.36666667]
	 [114.3090744  120.46666667]
	 [ 38.08142853  21.        ]
	 [ 96.91699982  93.        ]]
Train   Epoch: 543 / 600   Loss:    2139   Precision: 87.257%   Recall: 72.443%
Valid                   Loss: 1.712e+04   Precision: 35.039%   Recall: 29.815%
Train   Epoch: 544 / 600   Loss:    2287   Precision: 87.126%   Recall: 73.040%
Valid                   Loss: 1.705e+04   Precision: 33.224%   Recall: 30.233%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 10.10153389  16.21666667]
	 [-11.90947151  14.2       ]
	 [ 84.02045441  79.1       ]
	 [ 37.38220215  38.56666667]
	 [ 32.49546051  20.25      ]]
Train   Epoch: 545 / 600   Loss:    2406   Precision: 86.716%   Recall: 72.959%
Valid                   Loss: 1.705e+04   Precision: 32.958%   Recall: 36.673%
Train   Epoch: 546 / 600   Loss:    2407   Precision: 86.396%   Recall: 72.454%
Valid                   Loss: 1.714e+04   Precision: 30.201%   Recall: 34.884%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.66262436 30.13333333]
	 [59.33191681 61.9       ]
	 [71.90859222 51.78333333]
	 [44.65151596 41.23333333]
	 [61.15831757 32.86666667]]
Train   Epoch: 547 / 600   Loss:    2352   Precision: 87.171%   Recall: 73.403%
Valid                   Loss: 1.704e+04   Precision: 35.741%   Recall: 28.324%
Train   Epoch: 548 / 600   Loss:    2329   Precision: 86.986%   Recall: 73.353%
Valid                   Loss: 1.691e+04   Precision: 33.901%   Recall: 29.696%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.15307236 26.55      ]
	 [18.34324455 26.96666667]
	 [48.2287674  49.        ]
	 [26.77765083 53.9       ]
	 [39.24251175 71.33333333]]
Train   Epoch: 549 / 600   Loss:    2418   Precision: 86.203%   Recall: 73.555%
Valid                   Loss: 1.722e+04   Precision: 34.894%   Recall: 22.660%
Train   Epoch: 550 / 600   Loss:    3350   Precision: 79.813%   Recall: 66.401%
Valid                   Loss: 1.747e+04   Precision: 29.487%   Recall: 34.586%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.78242493 32.75      ]
	 [33.17274094 21.66666667]
	 [64.2237854  50.86666667]
	 [17.2674675  30.33333333]
	 [92.88607025 60.96666667]]
Train   Epoch: 551 / 600   Loss:    3280   Precision: 83.164%   Recall: 72.130%
Valid                   Loss: 1.717e+04   Precision: 31.857%   Recall: 28.742%
Train   Epoch: 552 / 600   Loss:    3196   Precision: 82.000%   Recall: 67.623%
Valid                   Loss: 1.727e+04   Precision: 31.824%   Recall: 28.086%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[113.38957977 145.43333333]
	 [ 45.50916672  39.75      ]
	 [ 49.92819977  31.23333333]
	 [ 28.94953728  35.83333333]
	 [ 22.48163033  27.75      ]]
Train   Epoch: 553 / 600   Loss:    2904   Precision: 83.684%   Recall: 72.302%
Valid                   Loss: 1.748e+04   Precision: 29.594%   Recall: 40.906%
Train   Epoch: 554 / 600   Loss:    2573   Precision: 85.902%   Recall: 70.806%
Valid                   Loss: 1.718e+04   Precision: 32.025%   Recall: 33.572%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.63402939 21.95      ]
	 [21.05268097 16.58333333]
	 [22.39523315 42.61666667]
	 [ 0.08145553 28.18333333]
	 [62.06817627 72.46666667]]
Train   Epoch: 555 / 600   Loss:    2271   Precision: 86.061%   Recall: 73.434%
Valid                   Loss: 1.74e+04   Precision: 34.863%   Recall: 28.086%
Train   Epoch: 556 / 600   Loss:    2419   Precision: 86.052%   Recall: 71.756%
Valid                   Loss: 1.701e+04   Precision: 32.555%   Recall: 29.100%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[17.94742775 33.16666667]
	 [39.19290924 38.88333333]
	 [28.44828796 42.53333333]
	 [16.53370857 17.7       ]
	 [20.01063728 29.3       ]]
Train   Epoch: 557 / 600   Loss:    2577   Precision: 85.250%   Recall: 72.302%
Valid                   Loss: 1.711e+04   Precision: 31.203%   Recall: 32.320%
Train   Epoch: 558 / 600   Loss:    2953   Precision: 83.606%   Recall: 69.826%
Valid                   Loss: 1.746e+04   Precision: 32.776%   Recall: 30.411%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.76929474  55.33333333]
	 [ 37.22014236  63.7       ]
	 [ 66.03646088  32.16666667]
	 [ 42.88899231  46.38333333]
	 [ 85.17105103 129.        ]]
Train   Epoch: 559 / 600   Loss:    2511   Precision: 86.184%   Recall: 72.363%
Valid                   Loss: 1.689e+04   Precision: 33.545%   Recall: 31.544%
Train   Epoch: 560 / 600   Loss:    2392   Precision: 86.995%   Recall: 72.868%
Valid                   Loss: 1.708e+04   Precision: 31.389%   Recall: 35.301%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.67470932 32.75      ]
	 [97.54284668 76.28333333]
	 [15.38384724 11.61666667]
	 [61.23646927 83.53333333]
	 [ 6.70115185  7.88333333]]
Train   Epoch: 561 / 600   Loss:    2290   Precision: 86.999%   Recall: 73.232%
Valid                   Loss: 1.734e+04   Precision: 32.133%   Recall: 34.049%
Train   Epoch: 562 / 600   Loss:    2363   Precision: 86.247%   Recall: 73.444%
Valid                   Loss: 1.704e+04   Precision: 33.256%   Recall: 25.522%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.98352051 53.33333333]
	 [41.86344147 36.6       ]
	 [12.86536121 36.71666667]
	 [44.03009796 12.58333333]
	 [15.50376892 39.6       ]]
Train   Epoch: 563 / 600   Loss:    2246   Precision: 86.663%   Recall: 73.474%
Valid                   Loss: 1.712e+04   Precision: 32.418%   Recall: 31.664%
Train   Epoch: 564 / 600   Loss:    2508   Precision: 85.953%   Recall: 72.160%
Valid                   Loss: 1.717e+04   Precision: 31.192%   Recall: 29.815%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.37322617  59.45      ]
	 [ 36.38959503  30.16666667]
	 [206.46354675 212.18333333]
	 [ 40.25494003  37.05      ]
	 [ 46.72402191  45.11666667]]
Train   Epoch: 565 / 600   Loss:    2221   Precision: 86.877%   Recall: 72.716%
Valid                   Loss: 1.728e+04   Precision: 31.347%   Recall: 33.572%
Train   Epoch: 566 / 600   Loss:    2267   Precision: 87.011%   Recall: 73.110%
Valid                   Loss: 1.737e+04   Precision: 31.598%   Recall: 35.838%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[11.38913155 28.46666667]
	 [23.25074387 19.33333333]
	 [49.65969467 26.        ]
	 [12.17180347 43.96666667]
	 [42.7256012  45.11666667]]
Train   Epoch: 567 / 600   Loss:    2473   Precision: 85.798%   Recall: 72.706%
Valid                   Loss: 1.732e+04   Precision: 32.427%   Recall: 31.306%
Train   Epoch: 568 / 600   Loss:    2259   Precision: 86.762%   Recall: 73.181%
Valid                   Loss: 1.729e+04   Precision: 32.258%   Recall: 33.393%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[17.61175919 15.5       ]
	 [47.74010086 49.01666667]
	 [50.50118256 40.83333333]
	 [61.55264664 25.61666667]
	 [43.44405365 55.66666667]]
Train   Epoch: 569 / 600   Loss:    2539   Precision: 85.198%   Recall: 72.181%
Valid                   Loss: 1.734e+04   Precision: 31.646%   Recall: 30.948%
Train   Epoch: 570 / 600   Loss:    2307   Precision: 86.287%   Recall: 72.676%
Valid                   Loss: 1.708e+04   Precision: 30.805%   Recall: 35.122%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.55627441  16.53333333]
	 [ 23.6900177   13.03333333]
	 [  9.37892532  13.86666667]
	 [ 46.04872131  23.51666667]
	 [ 52.99278259  82.5       ]]
Train   Epoch: 571 / 600   Loss:    2522   Precision: 85.404%   Recall: 71.069%
Valid                   Loss: 1.706e+04   Precision: 30.965%   Recall: 34.824%
Train   Epoch: 572 / 600   Loss:    2328   Precision: 86.599%   Recall: 73.464%
Valid                   Loss: 1.71e+04   Precision: 33.440%   Recall: 31.246%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[22.63482475 11.46666667]
	 [27.14106178 36.81666667]
	 [65.96373749 73.75      ]
	 [25.57876396 50.06666667]
	 [41.47378922 62.61666667]]
Train   Epoch: 573 / 600   Loss:    2146   Precision: 87.206%   Recall: 73.080%
Valid                   Loss: 1.699e+04   Precision: 34.338%   Recall: 29.219%
Train   Epoch: 574 / 600   Loss:    2220   Precision: 87.540%   Recall: 73.909%
Valid                   Loss: 1.732e+04   Precision: 31.828%   Recall: 36.971%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.33852768 83.26666667]
	 [35.53510666 34.21666667]
	 [35.77863312 50.        ]
	 [89.01768494 65.2       ]
	 [75.07001495 43.95      ]]
Train   Epoch: 575 / 600   Loss:    2285   Precision: 86.787%   Recall: 74.070%
Valid                   Loss: 1.759e+04   Precision: 33.831%   Recall: 33.751%
Train   Epoch: 576 / 600   Loss:    2198   Precision: 86.757%   Recall: 73.151%
Valid                   Loss: 1.739e+04   Precision: 33.289%   Recall: 29.875%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[117.93190765 133.38333333]
	 [ 65.61559296  32.68333333]
	 [ 39.50434875  28.5       ]
	 [ 34.60831451  50.21666667]
	 [ 40.11066055  18.75      ]]
Train   Epoch: 577 / 600   Loss:    2170   Precision: 87.470%   Recall: 73.646%
Valid                   Loss: 1.716e+04   Precision: 34.393%   Recall: 28.384%
Train   Epoch: 578 / 600   Loss:    2091   Precision: 87.395%   Recall: 73.848%
Valid                   Loss: 1.718e+04   Precision: 34.077%   Recall: 26.416%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.88117981 24.71666667]
	 [10.28579807 12.58333333]
	 [ 3.86053038 22.6       ]
	 [28.19035339 40.98333333]
	 [54.84601593 63.21666667]]
Train   Epoch: 579 / 600   Loss:    2209   Precision: 87.876%   Recall: 74.636%
Valid                   Loss: 1.73e+04   Precision: 33.654%   Recall: 31.306%
Train   Epoch: 580 / 600   Loss:    2099   Precision: 86.378%   Recall: 72.726%
Valid                   Loss: 1.735e+04   Precision: 33.699%   Recall: 29.338%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.83243942 44.38333333]
	 [75.77005005 78.66666667]
	 [30.53481674 47.58333333]
	 [55.87617111 53.68333333]
	 [ 7.99831724 12.2       ]]
Train   Epoch: 581 / 600   Loss:    2262   Precision: 87.416%   Recall: 73.353%
Valid                   Loss: 1.754e+04   Precision: 32.894%   Recall: 31.246%
Train   Epoch: 582 / 600   Loss:    1974   Precision: 87.520%   Recall: 73.626%
Valid                   Loss: 1.731e+04   Precision: 33.462%   Recall: 31.008%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.94400024 32.66666667]
	 [23.97199249 23.11666667]
	 [25.86168861 36.03333333]
	 [37.92739868 51.35      ]
	 [43.07339096 43.95      ]]
Train   Epoch: 583 / 600   Loss:    2031   Precision: 87.887%   Recall: 73.171%
Valid                   Loss: 1.712e+04   Precision: 32.342%   Recall: 31.783%
Train   Epoch: 584 / 600   Loss:    2209   Precision: 87.653%   Recall: 73.747%
Valid                   Loss: 1.723e+04   Precision: 32.303%   Recall: 27.430%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[29.5892067  29.75      ]
	 [32.25972748 25.73333333]
	 [34.88820648 27.66666667]
	 [67.74752045 40.96666667]
	 [30.07972527 38.46666667]]
Train   Epoch: 585 / 600   Loss:    2149   Precision: 87.356%   Recall: 74.141%
Valid                   Loss: 1.735e+04   Precision: 32.347%   Recall: 35.838%
Train   Epoch: 586 / 600   Loss:    2109   Precision: 87.513%   Recall: 73.868%
Valid                   Loss: 1.703e+04   Precision: 33.251%   Recall: 32.141%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[23.63438988 27.91666667]
	 [25.74472046 23.85      ]
	 [42.88384247 59.        ]
	 [38.73703384 35.5       ]
	 [40.77313614 33.11666667]]
Train   Epoch: 587 / 600   Loss:    2012   Precision: 87.609%   Recall: 73.949%
Valid                   Loss: 1.73e+04   Precision: 34.660%   Recall: 23.375%
Train   Epoch: 588 / 600   Loss:    2401   Precision: 86.971%   Recall: 72.646%
Valid                   Loss: 1.74e+04   Precision: 32.632%   Recall: 30.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.23152161 32.85      ]
	 [54.95609665 60.66666667]
	 [48.1645813  23.51666667]
	 [58.0862236  24.83333333]
	 [21.8653717  37.25      ]]
Train   Epoch: 589 / 600   Loss:    2275   Precision: 86.406%   Recall: 73.605%
Valid                   Loss: 1.732e+04   Precision: 33.097%   Recall: 27.788%
Train   Epoch: 590 / 600   Loss:    2458   Precision: 86.326%   Recall: 72.534%
Valid                   Loss: 1.72e+04   Precision: 33.820%   Recall: 30.411%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.63611984 27.66666667]
	 [29.1668396  19.33333333]
	 [28.1459446  22.71666667]
	 [19.47641182 12.        ]
	 [21.68178368 35.33333333]]
Train   Epoch: 591 / 600   Loss:    2306   Precision: 86.643%   Recall: 72.757%
Valid                   Loss: 1.707e+04   Precision: 32.130%   Recall: 31.843%
Train   Epoch: 592 / 600   Loss:    2190   Precision: 87.020%   Recall: 73.100%
Valid                   Loss: 1.701e+04   Precision: 33.612%   Recall: 36.017%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.52196503 54.88333333]
	 [13.82061768 16.8       ]
	 [77.522789   30.58333333]
	 [75.28886414 19.46666667]
	 [61.24266815 52.58333333]]
Train   Epoch: 593 / 600   Loss:    2178   Precision: 87.015%   Recall: 72.797%
Valid                   Loss: 1.693e+04   Precision: 31.826%   Recall: 34.824%
Train   Epoch: 594 / 600   Loss:    1989   Precision: 87.273%   Recall: 73.171%
Valid                   Loss: 1.701e+04   Precision: 30.321%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.85931396 35.85      ]
	 [66.84867859 81.33333333]
	 [34.1695137  44.98333333]
	 [38.36591339 35.45      ]
	 [44.67523575 32.5       ]]
Train   Epoch: 595 / 600   Loss:    2364   Precision: 85.728%   Recall: 72.959%
Valid                   Loss: 1.721e+04   Precision: 32.320%   Recall: 32.320%
Train   Epoch: 596 / 600   Loss:    2201   Precision: 85.721%   Recall: 72.070%
Valid                   Loss: 1.695e+04   Precision: 29.911%   Recall: 27.967%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 28.85723686  26.5       ]
	 [ 25.28512764  29.        ]
	 [100.16730499  63.48333333]
	 [ 86.996315    42.5       ]
	 [ 32.0633812   46.86666667]]
Train   Epoch: 597 / 600   Loss:    2148   Precision: 86.931%   Recall: 71.453%
Valid                   Loss: 1.692e+04   Precision: 31.655%   Recall: 31.485%
Train   Epoch: 598 / 600   Loss:    2114   Precision: 86.254%   Recall: 74.060%
Valid                   Loss: 1.725e+04   Precision: 32.063%   Recall: 34.109%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.24799728  49.75      ]
	 [ 29.73267937  63.61666667]
	 [ 26.99909019  25.75      ]
	 [ 47.14583588  45.        ]
	 [101.2077713  104.        ]]
Train   Epoch: 599 / 600   Loss:    3530   Precision: 80.963%   Recall: 69.018%
Valid                   Loss: 1.71e+04   Precision: 31.551%   Recall: 28.503%
Train   Epoch: 600 / 600   Loss:    3325   Precision: 80.555%   Recall: 68.401%
Valid                   Loss: 1.816e+04   Precision: 29.703%   Recall: 35.778%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.2632637   57.        ]
	 [ 72.18356323  41.66666667]
	 [ 12.66618729  27.75      ]
	 [109.4382782   88.3       ]
	 [ 34.34575272  44.31666667]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Weighted loss for presentation - presentation set</span>

<span class="kn">from</span> <span class="nn">training.dust_loss</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_train_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_train.pkl&quot;</span><span class="p">))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_meteorology.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;tensor_valid_dust.pkl&quot;</span><span class="p">),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">presentation_dir</span><span class="o">+</span><span class="s2">&quot;times_valid.pkl&quot;</span><span class="p">))</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">results_dir_specific</span> <span class="o">=</span> <span class="n">results_dir</span><span class="o">+</span><span class="s2">&quot;presentation_set_e600_lr0p00001_noaugmentation_weighted_loss/&quot;</span>


<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;best_model_state.pt&quot;</span>
<span class="n">last_model_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;last_model_state.pt&quot;</span>

<span class="n">train_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_losses.pkl&quot;</span>
<span class="n">train_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_lags_losses.pkl&quot;</span>
<span class="n">train_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;train_delta_lags_losses.pkl&quot;</span>
<span class="n">valid_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_losses.pkl&quot;</span>
<span class="n">valid_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_lags_losses.pkl&quot;</span>
<span class="n">valid_delta_lags_losses_path</span> <span class="o">=</span> <span class="n">results_dir_specific</span><span class="o">+</span><span class="s2">&quot;valid_delta_lags_losses.pkl&quot;</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>

<span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span><span class="p">,</span>
 <span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses</span><span class="p">,</span>
 <span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                         <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                                         <span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span> <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                         <span class="n">save_best_model_dict_to</span><span class="o">=</span><span class="n">best_model_path</span><span class="p">,</span> 
                                                         <span class="n">save_last_model_dict_to</span><span class="o">=</span><span class="n">last_model_path</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="n">train_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_lags_losses</span><span class="p">,</span><span class="n">train_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_delta_lags_losses</span><span class="p">,</span><span class="n">train_delta_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span><span class="n">valid_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_lags_losses</span><span class="p">,</span><span class="n">valid_lags_losses_path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">valid_delta_lags_losses</span><span class="p">,</span><span class="n">valid_delta_lags_losses_path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 600   Loss: 2.63e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2957   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 600   Loss: 2.776e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2635   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 14.2605114   46.66666667]
	 [ 14.26084137  24.98333333]
	 [ 14.26066685 131.11666667]
	 [ 14.26048565  38.66666667]
	 [ 14.26047802  33.53333333]]
Train   Epoch: 003 / 600   Loss: 2.409e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2329   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 600   Loss: 2.563e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    2054   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[22.37702751 56.68333333]
	 [22.37714958 55.95      ]
	 [22.37701225 34.86666667]
	 [22.37742996 25.33333333]
	 [22.37734222 22.23333333]]
Train   Epoch: 005 / 600   Loss: 2.414e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1828   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 006 / 600   Loss: 2.569e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1639   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.4323616   49.5       ]
	 [ 31.43243027  62.41666667]
	 [ 31.43258667  25.7       ]
	 [ 31.4325695  100.66666667]
	 [ 31.43229866  94.73333333]]
Train   Epoch: 007 / 600   Loss: 2.339e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1498   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 600   Loss: 2.543e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1396   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.02285385  10.83333333]
	 [ 40.02287674  28.15      ]
	 [ 40.02299881 117.4       ]
	 [ 40.02248001  29.11666667]
	 [ 40.02263641  32.4       ]]
Train   Epoch: 009 / 600   Loss: 2.237e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1335   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 600   Loss: 2.235e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1306   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.4789772   24.03333333]
	 [ 48.47901917 203.58333333]
	 [ 48.47851562  25.05      ]
	 [ 48.47934723   8.        ]
	 [ 48.47922516  19.98333333]]
Train   Epoch: 011 / 600   Loss: 2.111e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1307   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 600   Loss: 2.286e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1332   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  56.24667358 1049.        ]
	 [  56.24676895   33.21666667]
	 [  56.24637222   54.66666667]
	 [  56.24678421   49.55      ]
	 [  56.2467041    27.63333333]]
Train   Epoch: 013 / 600   Loss: 2.322e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1378   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 600   Loss: 2.168e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1436   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[63.38667297 36.11666667]
	 [63.38646698 42.55      ]
	 [63.3865242  51.21666667]
	 [63.3867569  24.05      ]
	 [63.38656998 46.33333333]]
Train   Epoch: 015 / 600   Loss: 2.123e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1502   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 016 / 600   Loss: 2.231e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1578   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 69.77425385  18.11666667]
	 [ 69.77423859 115.3       ]
	 [ 69.77424622  19.86666667]
	 [ 69.7742157  175.03333333]
	 [ 69.77417755  65.46666667]]
Train   Epoch: 017 / 600   Loss: 2.262e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    1654   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 600   Loss: 2.158e+04   Precision: 39.815%   Recall: 65.289%
Valid                   Loss:    1730   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[75.30937195 31.8       ]
	 [75.30988312  9.        ]
	 [75.30945587 48.75      ]
	 [75.30973053 41.        ]
	 [75.30973053 51.33333333]]
Train   Epoch: 019 / 600   Loss: 2.057e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1798   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 020 / 600   Loss: 2.244e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1867   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 79.94836426  37.95      ]
	 [ 79.9482193   49.13333333]
	 [ 79.94844055  14.5       ]
	 [ 79.94845581  27.66666667]
	 [ 79.94805908 344.88333333]]
Train   Epoch: 021 / 600   Loss: 2.041e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1918   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 022 / 600   Loss: 2.178e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    1972   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 83.21380615 104.05      ]
	 [ 83.21393585 166.78333333]
	 [ 83.21401215  24.86666667]
	 [ 83.21380615  76.75      ]
	 [ 83.21385193  31.08333333]]
Train   Epoch: 023 / 600   Loss: 2.064e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2027   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 024 / 600   Loss: 2.223e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2070   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.0044632  34.85      ]
	 [86.00432587 50.55      ]
	 [86.00448608 44.66666667]
	 [86.00453949 24.4       ]
	 [86.00468445 18.16666667]]
Train   Epoch: 025 / 600   Loss: 2.292e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2114   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 026 / 600   Loss: 2.176e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2140   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.33546448 48.66666667]
	 [88.33585358 41.3       ]
	 [88.33621216 41.46666667]
	 [88.33573914 65.33333333]
	 [88.33620453 33.21666667]]
Train   Epoch: 027 / 600   Loss: 2.132e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2163   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 028 / 600   Loss: 2.074e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2169   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.81845856 60.45      ]
	 [89.81908417 24.88333333]
	 [89.8190918  18.5       ]
	 [89.81816864 32.33333333]
	 [89.81866455 44.03333333]]
Train   Epoch: 029 / 600   Loss: 2.16e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2207   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 030 / 600   Loss: 2.298e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2238   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.22458649 36.08333333]
	 [91.22455597 42.45      ]
	 [91.22470856 31.51666667]
	 [91.22363281 58.33333333]
	 [91.22465515 16.        ]]
Train   Epoch: 031 / 600   Loss: 2.018e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2234   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 032 / 600   Loss: 2.154e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2266   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.98332977 80.33333333]
	 [91.98442841 36.23333333]
	 [91.98413086 32.68333333]
	 [91.9826355  16.4       ]
	 [91.98397064 43.83333333]]
Train   Epoch: 033 / 600   Loss: 2.149e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2282   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 034 / 600   Loss: 2.051e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2281   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.88415527 75.        ]
	 [92.88420868 63.41666667]
	 [92.88420868 32.7       ]
	 [92.88407898 13.66666667]
	 [92.88419342 38.73333333]]
Train   Epoch: 035 / 600   Loss: 2.129e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2280   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 036 / 600   Loss: 2.027e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2259   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.51696014  17.5       ]
	 [ 92.52050781 100.61666667]
	 [ 92.51911163  31.83333333]
	 [ 92.51860809  45.        ]
	 [ 92.51882935  37.38333333]]
Train   Epoch: 037 / 600   Loss: 2.008e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2246   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 038 / 600   Loss: 2.111e+04   Precision: 39.875%   Recall: 98.935%
Valid                   Loss:    2237   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.42391968 34.05      ]
	 [92.465065   57.06666667]
	 [92.4315567  45.        ]
	 [92.42586517 13.83333333]
	 [92.47283173 33.        ]]
Train   Epoch: 039 / 600   Loss: 2.11e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2264   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 040 / 600   Loss: 2.109e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2255   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.84091187 25.38333333]
	 [92.83774567 49.        ]
	 [92.83929443 48.71666667]
	 [92.84136963 31.95      ]
	 [92.8363266  33.05      ]]
Train   Epoch: 041 / 600   Loss: 2.076e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2253   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 042 / 600   Loss: 2.038e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2262   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.05885315 38.4       ]
	 [93.0381546  29.96666667]
	 [93.02967072 16.68333333]
	 [93.04534149 48.78333333]
	 [93.04494476 23.78333333]]
Train   Epoch: 043 / 600   Loss: 2.213e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2274   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 044 / 600   Loss: 2.089e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2259   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.06138611 32.16666667]
	 [93.07186127 42.83333333]
	 [93.01379395 29.71666667]
	 [93.03594208 60.55      ]
	 [93.05207062 79.91666667]]
Train   Epoch: 045 / 600   Loss: 2.064e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2223   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 046 / 600   Loss: 2.034e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2247   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.95936584 119.45      ]
	 [ 91.97033691  55.55      ]
	 [ 92.0260849   43.06666667]
	 [ 92.00878143  30.66666667]
	 [ 92.01772308  90.66666667]]
Train   Epoch: 047 / 600   Loss: 2.02e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2262   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 048 / 600   Loss: 2.216e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2285   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.53965759  33.11666667]
	 [ 93.59912872  81.75      ]
	 [ 93.58242798  44.55      ]
	 [ 93.54418182  41.08333333]
	 [ 93.58985901 116.6       ]]
Train   Epoch: 049 / 600   Loss: 2.241e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2301   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 050 / 600   Loss: 2.165e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2284   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.64472198 28.63333333]
	 [93.64649963 47.13333333]
	 [93.72107697 30.25      ]
	 [93.64594269 44.28333333]
	 [93.67986298 37.36666667]]
Train   Epoch: 051 / 600   Loss: 2.142e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2272   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 052 / 600   Loss: 2.174e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2278   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.4653244  26.36666667]
	 [93.4955368  33.5       ]
	 [93.44058228 53.11666667]
	 [93.53494263 41.95      ]
	 [93.49359894  5.36666667]]
Train   Epoch: 053 / 600   Loss: 2.03e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2236   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 054 / 600   Loss: 2.255e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2295   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.39359283 34.38333333]
	 [93.54233551 93.75      ]
	 [93.43820953 24.61666667]
	 [93.51998138 55.45      ]
	 [93.44093323 41.83333333]]
Train   Epoch: 055 / 600   Loss: 2.09e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2205   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 056 / 600   Loss: 2.225e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2282   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.88704681 20.5       ]
	 [92.96086884 19.21666667]
	 [92.88197327 95.86666667]
	 [92.83662415 53.75      ]
	 [92.97577667 73.        ]]
Train   Epoch: 057 / 600   Loss: 2.076e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2278   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 058 / 600   Loss: 1.995e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2335   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.69110107 11.33333333]
	 [93.70425415 39.03333333]
	 [93.59539032 50.13333333]
	 [93.60238647 93.16666667]
	 [93.57621002 18.78333333]]
Train   Epoch: 059 / 600   Loss: 2.11e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2252   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 060 / 600   Loss: 2.095e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2333   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  94.26850128  107.08333333]
	 [  94.5982132    59.95      ]
	 [  94.43832397   32.81666667]
	 [  94.47042847 1684.56666667]
	 [  94.41508484   26.45      ]]
Train   Epoch: 061 / 600   Loss: 2.109e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2212   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 062 / 600   Loss: 2.102e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2163   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.73079681 40.13333333]
	 [91.23490143 24.8       ]
	 [91.33777618 32.4       ]
	 [90.95367432 63.66666667]
	 [91.00512695 37.45      ]]
Train   Epoch: 063 / 600   Loss: 2.068e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2165   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 064 / 600   Loss: 2.22e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2387   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.88832092 59.33333333]
	 [95.58563995 16.45      ]
	 [94.00444794 68.48333333]
	 [92.1290741  54.98333333]
	 [93.12562561 71.75      ]]
Train   Epoch: 065 / 600   Loss: 2.335e+04   Precision: 39.854%   Recall: 100.000%
Valid                   Loss:    2294   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 066 / 600   Loss: 1.975e+04   Precision: 40.199%   Recall: 98.660%
Valid                   Loss:    2456   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.70574951 31.63333333]
	 [97.71144104 12.        ]
	 [93.31489563 18.71666667]
	 [93.58988953 52.08333333]
	 [95.15814972 26.3       ]]
Train   Epoch: 067 / 600   Loss: 2.064e+04   Precision: 40.300%   Recall: 98.754%
Valid                   Loss:    2449   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 068 / 600   Loss: 2.019e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2220   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.74525452 43.83333333]
	 [93.16683197 41.28333333]
	 [92.58087158 45.33333333]
	 [92.24234772 23.53333333]
	 [92.54147339 20.96666667]]
Train   Epoch: 069 / 600   Loss: 2.036e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2127   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 070 / 600   Loss: 2.197e+04   Precision: 39.844%   Recall: 100.000%
Valid                   Loss:    2022   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 90.34502411 106.33333333]
	 [ 89.09815216  19.78333333]
	 [ 87.46568298  18.66666667]
	 [ 91.34889221  25.21666667]
	 [ 86.2401886   65.91666667]]
Train   Epoch: 071 / 600   Loss: 2.277e+04   Precision: 40.619%   Recall: 97.535%
Valid                   Loss:    2233   Precision: 13.744%   Recall: 98.168%
Train   Epoch: 072 / 600   Loss: 2.061e+04   Precision: 42.623%   Recall: 91.553%
Valid                   Loss:    1682   Precision: 17.239%   Recall: 80.952%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.2646637  393.53333333]
	 [ 63.09588242  35.51666667]
	 [ 71.3374939   25.66666667]
	 [ 96.33665466  83.66666667]
	 [ 86.54639435  12.18333333]]
Train   Epoch: 073 / 600   Loss: 2.184e+04   Precision: 42.244%   Recall: 92.317%
Valid                   Loss:    1969   Precision: 14.511%   Recall: 94.505%
Train   Epoch: 074 / 600   Loss: 2.245e+04   Precision: 45.533%   Recall: 85.578%
Valid                   Loss:    1899   Precision: 16.995%   Recall: 85.348%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.15089417  34.58333333]
	 [ 83.45542145  45.75      ]
	 [ 80.41196442  40.78333333]
	 [102.55615997  21.53333333]
	 [ 64.74737549  27.21666667]]
Train   Epoch: 075 / 600   Loss: 2.159e+04   Precision: 40.691%   Recall: 97.006%
Valid                   Loss:    2441   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 076 / 600   Loss: 2.193e+04   Precision: 40.081%   Recall: 98.975%
Valid                   Loss:    2788   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.89953613  42.83333333]
	 [100.47261047  27.25      ]
	 [100.91879272  41.8       ]
	 [104.21896362  14.7       ]
	 [104.33235168 226.38333333]]
Train   Epoch: 077 / 600   Loss: 2.189e+04   Precision: 42.909%   Recall: 91.486%
Valid                   Loss:    1977   Precision: 15.803%   Recall: 87.179%
Train   Epoch: 078 / 600   Loss: 2.231e+04   Precision: 42.769%   Recall: 91.433%
Valid                   Loss:    2743   Precision: 12.506%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.93218994  43.33333333]
	 [108.8453598  344.21666667]
	 [106.27482605  68.5       ]
	 [102.58905792  60.33333333]
	 [108.26396942  24.66666667]]
Train   Epoch: 079 / 600   Loss: 2.112e+04   Precision: 44.872%   Recall: 86.168%
Valid                   Loss:    2157   Precision: 15.923%   Recall: 87.546%
Train   Epoch: 080 / 600   Loss: 2.132e+04   Precision: 46.237%   Recall: 83.180%
Valid                   Loss:    1576   Precision: 20.435%   Recall: 68.864%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 87.84346771  28.18333333]
	 [ 74.01591492  39.11666667]
	 [112.59426117 148.56666667]
	 [ 68.57746887  39.58333333]
	 [ 54.60111237  61.78333333]]
Train   Epoch: 081 / 600   Loss: 2.037e+04   Precision: 46.691%   Recall: 81.794%
Valid                   Loss:    1861   Precision: 18.868%   Recall: 84.249%
Train   Epoch: 082 / 600   Loss: 1.976e+04   Precision: 47.408%   Recall: 80.320%
Valid                   Loss:    1446   Precision: 22.061%   Recall: 50.183%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 54.89628983  45.        ]
	 [ 77.11577606  42.86666667]
	 [110.14825439  66.11666667]
	 [ 71.18656921 152.56666667]
	 [ 71.83802032  46.08333333]]
Train   Epoch: 083 / 600   Loss: 2.006e+04   Precision: 44.768%   Recall: 84.132%
Valid                   Loss:    1673   Precision: 19.634%   Recall: 82.418%
Train   Epoch: 084 / 600   Loss: 1.959e+04   Precision: 46.372%   Recall: 81.251%
Valid                   Loss:    1769   Precision: 18.866%   Recall: 85.348%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 78.00336456 143.26666667]
	 [ 82.70304108  45.7       ]
	 [119.24517059  13.71666667]
	 [ 78.60764313  26.95      ]
	 [ 85.6966095   20.5       ]]
Train   Epoch: 085 / 600   Loss: 2.029e+04   Precision: 46.896%   Recall: 80.146%
Valid                   Loss:    1848   Precision: 17.087%   Recall: 86.813%
Train   Epoch: 086 / 600   Loss: 2.025e+04   Precision: 48.687%   Recall: 74.901%
Valid                   Loss:    2094   Precision: 16.248%   Recall: 85.348%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.55560303  25.55      ]
	 [ 84.52931213  28.55      ]
	 [123.99002838 984.34      ]
	 [ 67.45397949  48.78333333]
	 [ 70.98271179  30.78333333]]
Train   Epoch: 087 / 600   Loss: 2.144e+04   Precision: 48.926%   Recall: 71.699%
Valid                   Loss:    2018   Precision: 17.266%   Recall: 87.912%
Train   Epoch: 088 / 600   Loss: 1.966e+04   Precision: 51.337%   Recall: 68.143%
Valid                   Loss:    2026   Precision: 18.533%   Recall: 87.912%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.26905823  48.86666667]
	 [128.84767151  27.55      ]
	 [ 69.48943329  31.        ]
	 [132.94612122  42.66666667]
	 [ 67.17110443  26.55      ]]
Train   Epoch: 089 / 600   Loss: 1.966e+04   Precision: 49.819%   Recall: 68.323%
Valid                   Loss:    3390   Precision: 12.692%   Recall: 100.000%
Train   Epoch: 090 / 600   Loss: 1.98e+04   Precision: 46.793%   Recall: 73.059%
Valid                   Loss:    2071   Precision: 19.234%   Recall: 82.784%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.01209259  70.55      ]
	 [ 80.2649765   30.88333333]
	 [139.17596436  15.        ]
	 [137.71980286  31.16666667]
	 [ 70.91000366  33.        ]]
Train   Epoch: 091 / 600   Loss: 1.918e+04   Precision: 52.015%   Recall: 66.575%
Valid                   Loss:    1564   Precision: 22.746%   Recall: 61.905%
Train   Epoch: 092 / 600   Loss: 2.183e+04   Precision: 52.406%   Recall: 67.486%
Valid                   Loss:    1951   Precision: 20.279%   Recall: 79.853%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 54.1660614   19.        ]
	 [ 61.15689468  36.66666667]
	 [ 70.51829529  11.96666667]
	 [107.60096741  36.96666667]
	 [116.89195251  21.38333333]]
Train   Epoch: 093 / 600   Loss: 2.056e+04   Precision: 52.223%   Recall: 68.143%
Valid                   Loss:    2157   Precision: 18.960%   Recall: 86.813%
Train   Epoch: 094 / 600   Loss: 2.046e+04   Precision: 52.303%   Recall: 67.928%
Valid                   Loss:    2447   Precision: 17.456%   Recall: 90.476%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.74994278  26.33333333]
	 [ 73.38492584  67.2       ]
	 [130.01792908  24.86666667]
	 [ 75.34039307  26.66666667]
	 [157.29594421  16.75      ]]
Train   Epoch: 095 / 600   Loss: 2.132e+04   Precision: 51.135%   Recall: 69.429%
Valid                   Loss:    1668   Precision: 22.209%   Recall: 67.033%
Train   Epoch: 096 / 600   Loss: 2.136e+04   Precision: 52.367%   Recall: 66.990%
Valid                   Loss:    1832   Precision: 20.757%   Recall: 74.359%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.05394363  21.        ]
	 [ 60.69371033  32.61666667]
	 [ 93.13630676  24.13333333]
	 [101.18286896  35.78333333]
	 [ 68.22284698  54.33333333]]
Train   Epoch: 097 / 600   Loss: 1.983e+04   Precision: 51.765%   Recall: 65.014%
Valid                   Loss:    1740   Precision: 23.298%   Recall: 65.201%
Train   Epoch: 098 / 600   Loss:   2e+04   Precision: 50.643%   Recall: 66.702%
Valid                   Loss:    1801   Precision: 20.672%   Recall: 74.359%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  89.73561859   43.        ]
	 [  58.21288681   21.95      ]
	 [  63.62811661   27.06666667]
	 [ 172.21078491 1903.66666667]
	 [ 173.03619385  124.78333333]]
Train   Epoch: 099 / 600   Loss: 2.016e+04   Precision: 52.826%   Recall: 64.679%
Valid                   Loss:    2287   Precision: 18.770%   Recall: 87.179%
Train   Epoch: 100 / 600   Loss: 2.066e+04   Precision: 52.491%   Recall: 67.607%
Valid                   Loss:    1721   Precision: 21.758%   Recall: 72.527%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.44088745  22.        ]
	 [178.03482056  24.91666667]
	 [ 58.22752762  58.66666667]
	 [114.24121857  40.5       ]
	 [113.13980103 899.45      ]]
Train   Epoch: 101 / 600   Loss: 2.19e+04   Precision: 52.132%   Recall: 66.756%
Valid                   Loss:    2122   Precision: 18.223%   Recall: 88.645%
Train   Epoch: 102 / 600   Loss: 1.915e+04   Precision: 52.021%   Recall: 65.430%
Valid                   Loss:    1783   Precision: 22.894%   Recall: 70.696%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[179.13130188 182.86666667]
	 [ 98.53568268  34.18333333]
	 [ 57.88766098  37.        ]
	 [ 56.56040573  16.83333333]
	 [ 88.33608246 141.25      ]]
Train   Epoch: 103 / 600   Loss: 2.07e+04   Precision: 52.331%   Recall: 64.204%
Valid                   Loss:    2886   Precision: 12.506%   Recall: 100.000%
Train   Epoch: 104 / 600   Loss: 2.016e+04   Precision: 52.677%   Recall: 65.450%
Valid                   Loss:    1481   Precision: 32.143%   Recall: 26.374%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.42719269 59.45      ]
	 [62.34030151 21.45      ]
	 [43.51538086 31.85      ]
	 [73.75821686 84.88333333]
	 [54.1227417  34.38333333]]
Train   Epoch: 105 / 600   Loss: 1.967e+04   Precision: 50.960%   Recall: 65.456%
Valid                   Loss:    1815   Precision: 22.986%   Recall: 71.062%
Train   Epoch: 106 / 600   Loss: 2.006e+04   Precision: 53.177%   Recall: 65.189%
Valid                   Loss:    1723   Precision: 24.352%   Recall: 61.905%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.28153992 20.16666667]
	 [79.05776978 33.2       ]
	 [62.06150818 25.33333333]
	 [71.64160919 53.11666667]
	 [57.7973175  41.38333333]]
Train   Epoch: 107 / 600   Loss: 2.068e+04   Precision: 51.338%   Recall: 63.742%
Valid                   Loss:    1443   Precision: 31.333%   Recall: 17.216%
Train   Epoch: 108 / 600   Loss: 1.977e+04   Precision: 51.613%   Recall: 66.006%
Valid                   Loss:    2053   Precision: 20.547%   Recall: 74.359%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.28931427  20.25      ]
	 [ 57.06286621  44.5       ]
	 [177.58222961  39.75      ]
	 [ 54.30558014  25.16666667]
	 [ 66.52663422  43.66666667]]
Train   Epoch: 109 / 600   Loss: 2.019e+04   Precision: 51.250%   Recall: 66.334%
Valid                   Loss:    1918   Precision: 22.508%   Recall: 73.626%
Train   Epoch: 110 / 600   Loss: 2.133e+04   Precision: 53.410%   Recall: 64.994%
Valid                   Loss:    2717   Precision: 17.302%   Recall: 89.744%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[76.57161713 22.38333333]
	 [76.09977722 51.5       ]
	 [60.34925842 20.33333333]
	 [64.08374786 27.        ]
	 [57.03258514 26.48333333]]
Train   Epoch: 111 / 600   Loss: 1.97e+04   Precision: 53.904%   Recall: 64.271%
Valid                   Loss:    2253   Precision: 19.310%   Recall: 86.081%
Train   Epoch: 112 / 600   Loss: 1.978e+04   Precision: 53.909%   Recall: 63.641%
Valid                   Loss:    1638   Precision: 31.540%   Recall: 47.253%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.84975815  33.21666667]
	 [ 79.85681915 252.11666667]
	 [ 43.5946846   25.8       ]
	 [ 67.47834778  57.86666667]
	 [ 51.25032806  13.7       ]]
Train   Epoch: 113 / 600   Loss: 2.142e+04   Precision: 50.929%   Recall: 65.905%
Valid                   Loss:    1379   Precision: 40.458%   Recall: 19.414%
Train   Epoch: 114 / 600   Loss: 1.927e+04   Precision: 52.244%   Recall: 61.377%
Valid                   Loss:    1960   Precision: 19.929%   Recall: 81.685%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[222.01548767  46.21666667]
	 [ 58.51501846  21.95      ]
	 [ 69.3031311   10.88333333]
	 [ 79.82183075  16.05      ]
	 [ 65.33470154  40.5       ]]
Train   Epoch: 115 / 600   Loss: 2.13e+04   Precision: 53.148%   Recall: 64.961%
Valid                   Loss:    1821   Precision: 25.859%   Recall: 63.370%
Train   Epoch: 116 / 600   Loss: 1.975e+04   Precision: 54.723%   Recall: 58.557%
Valid                   Loss:    2154   Precision: 20.661%   Recall: 82.418%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.6843605   39.33333333]
	 [ 78.68199158  35.75      ]
	 [210.63475037  63.15      ]
	 [ 68.8801651   23.66666667]
	 [ 62.23640442  10.38333333]]
Train   Epoch: 117 / 600   Loss: 1.919e+04   Precision: 53.981%   Recall: 63.085%
Valid                   Loss:    2933   Precision: 16.299%   Recall: 91.941%
Train   Epoch: 118 / 600   Loss: 2.007e+04   Precision: 51.260%   Recall: 63.628%
Valid                   Loss:    2748   Precision: 18.780%   Recall: 84.615%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[238.72756958  23.58333333]
	 [ 56.09102249  39.58333333]
	 [ 63.47287369  37.95      ]
	 [ 58.80778122  27.06666667]
	 [ 54.56500244  75.        ]]
Train   Epoch: 119 / 600   Loss: 2.057e+04   Precision: 53.922%   Recall: 63.085%
Valid                   Loss:    1861   Precision: 24.284%   Recall: 65.201%
Train   Epoch: 120 / 600   Loss: 1.926e+04   Precision: 54.093%   Recall: 64.713%
Valid                   Loss:    1689   Precision: 28.657%   Recall: 52.381%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.02184677  33.78333333]
	 [ 57.45633698  43.78333333]
	 [ 79.25737762  54.        ]
	 [180.44732666  66.66666667]
	 [ 63.63729477  15.        ]]
Train   Epoch: 121 / 600   Loss: 1.846e+04   Precision: 53.796%   Recall: 60.520%
Valid                   Loss:    1590   Precision: 32.976%   Recall: 45.055%
Train   Epoch: 122 / 600   Loss: 2.062e+04   Precision: 54.606%   Recall: 60.232%
Valid                   Loss:    1765   Precision: 24.468%   Recall: 58.974%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.1930542  28.55      ]
	 [54.06515121 71.5       ]
	 [98.91815186 37.7       ]
	 [75.8184433  21.28333333]
	 [62.68450928 95.25      ]]
Train   Epoch: 123 / 600   Loss: 1.998e+04   Precision: 53.489%   Recall: 60.888%
Valid                   Loss:    1871   Precision: 27.255%   Recall: 50.916%
Train   Epoch: 124 / 600   Loss: 1.959e+04   Precision: 54.823%   Recall: 60.727%
Valid                   Loss:    2386   Precision: 18.381%   Recall: 89.011%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 69.02005768  81.78333333]
	 [ 66.00525665  37.23333333]
	 [ 68.25508881 117.28333333]
	 [182.52737427  54.        ]
	 [ 71.52667999  28.1       ]]
Train   Epoch: 125 / 600   Loss: 1.845e+04   Precision: 55.348%   Recall: 60.031%
Valid                   Loss:    2303   Precision: 19.686%   Recall: 87.179%
Train   Epoch: 126 / 600   Loss: 1.971e+04   Precision: 53.469%   Recall: 62.362%
Valid                   Loss:    1993   Precision: 20.265%   Recall: 78.388%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.95812988 63.08333333]
	 [62.14047623 31.73333333]
	 [67.17180634 42.6       ]
	 [86.10338593 46.66666667]
	 [89.16297913 32.        ]]
Train   Epoch: 127 / 600   Loss: 1.935e+04   Precision: 54.532%   Recall: 60.292%
Valid                   Loss:    1937   Precision: 23.415%   Recall: 66.300%
Train   Epoch: 128 / 600   Loss: 2.039e+04   Precision: 52.893%   Recall: 63.012%
Valid                   Loss:    1927   Precision: 21.816%   Recall: 76.557%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.58410645 29.05      ]
	 [59.05846024 55.11666667]
	 [90.47846222 19.46666667]
	 [62.91368866 55.55      ]
	 [63.66514969 20.3       ]]
Train   Epoch: 129 / 600   Loss: 2.051e+04   Precision: 52.303%   Recall: 63.815%
Valid                   Loss:    1598   Precision: 30.625%   Recall: 35.897%
Train   Epoch: 130 / 600   Loss: 1.842e+04   Precision: 53.378%   Recall: 61.076%
Valid                   Loss:    2283   Precision: 19.370%   Recall: 87.912%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[73.95126343 40.16666667]
	 [63.34174728 26.61666667]
	 [81.18218231 81.66666667]
	 [87.30684662 55.71666667]
	 [68.45178223 26.95      ]]
Train   Epoch: 131 / 600   Loss: 1.915e+04   Precision: 55.026%   Recall: 60.975%
Valid                   Loss:    1803   Precision: 31.928%   Recall: 38.828%
Train   Epoch: 132 / 600   Loss: 2.001e+04   Precision: 54.641%   Recall: 61.437%
Valid                   Loss:    1854   Precision: 28.037%   Recall: 54.945%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[77.7117691  45.51666667]
	 [45.74342346 88.58333333]
	 [60.07660675 92.25      ]
	 [50.01498795 43.8       ]
	 [50.97885895 20.88333333]]
Train   Epoch: 133 / 600   Loss: 1.802e+04   Precision: 53.450%   Recall: 59.830%
Valid                   Loss:    1802   Precision: 29.374%   Recall: 49.817%
Train   Epoch: 134 / 600   Loss: 1.874e+04   Precision: 54.815%   Recall: 59.897%
Valid                   Loss:    2069   Precision: 27.667%   Recall: 56.044%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.34750366 40.55      ]
	 [78.08236694 18.33333333]
	 [55.5742569  89.21666667]
	 [85.01107025 29.08333333]
	 [50.83034134 22.11666667]]
Train   Epoch: 135 / 600   Loss: 1.941e+04   Precision: 52.146%   Recall: 62.991%
Valid                   Loss:    1700   Precision: 30.479%   Recall: 32.601%
Train   Epoch: 136 / 600   Loss: 2.052e+04   Precision: 54.874%   Recall: 58.932%
Valid                   Loss:    2242   Precision: 21.668%   Recall: 78.022%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 85.27613068  39.05      ]
	 [ 67.22388458  27.45      ]
	 [237.03849792  30.71666667]
	 [ 84.34502411  16.61666667]
	 [ 63.30818558  25.25      ]]
Train   Epoch: 137 / 600   Loss: 1.777e+04   Precision: 55.423%   Recall: 59.247%
Valid                   Loss:    2763   Precision: 20.670%   Recall: 81.319%
Train   Epoch: 138 / 600   Loss:   2e+04   Precision: 54.787%   Recall: 59.569%
Valid                   Loss:    1279   Precision: 37.143%   Recall: 14.286%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[30.29356194 19.63333333]
	 [33.94998932 27.28333333]
	 [40.50427628 48.03333333]
	 [37.71264648 38.86666667]
	 [52.29838562 12.5       ]]
Train   Epoch: 139 / 600   Loss: 1.812e+04   Precision: 52.949%   Recall: 58.571%
Valid                   Loss:    1818   Precision: 24.332%   Recall: 63.370%
Train   Epoch: 140 / 600   Loss: 1.977e+04   Precision: 54.215%   Recall: 58.504%
Valid                   Loss:    2249   Precision: 18.923%   Recall: 82.418%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.4587326   25.5       ]
	 [308.30691528  28.11666667]
	 [ 76.52164459  51.65      ]
	 [ 73.48638153  43.31666667]
	 [ 78.44911194  37.78333333]]
Train   Epoch: 141 / 600   Loss: 1.904e+04   Precision: 52.783%   Recall: 61.357%
Valid                   Loss:    1517   Precision: 36.237%   Recall: 38.095%
Train   Epoch: 142 / 600   Loss: 1.903e+04   Precision: 55.089%   Recall: 58.336%
Valid                   Loss:    2147   Precision: 22.026%   Recall: 68.498%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.59049988  36.        ]
	 [ 58.43078613   8.93333333]
	 [ 69.33518219  54.86666667]
	 [ 71.23206329  86.36666667]
	 [162.1925354   39.33333333]]
Train   Epoch: 143 / 600   Loss: 1.872e+04   Precision: 53.528%   Recall: 61.592%
Valid                   Loss:    1585   Precision: 30.365%   Recall: 48.718%
Train   Epoch: 144 / 600   Loss: 1.934e+04   Precision: 53.633%   Recall: 58.979%
Valid                   Loss:    1980   Precision: 24.646%   Recall: 63.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[108.62524414  61.53333333]
	 [ 65.08621979  49.66666667]
	 [ 38.40939713  29.61666667]
	 [ 76.05526733  30.46666667]
	 [ 81.33661652  34.66666667]]
Train   Epoch: 145 / 600   Loss: 1.889e+04   Precision: 54.298%   Recall: 60.125%
Valid                   Loss:    2025   Precision: 22.795%   Recall: 72.894%
Train   Epoch: 146 / 600   Loss: 1.882e+04   Precision: 55.423%   Recall: 59.214%
Valid                   Loss:    1949   Precision: 24.029%   Recall: 61.172%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.40840912  29.98333333]
	 [ 59.94336319  41.        ]
	 [ 48.65828323  32.        ]
	 [120.21926117  36.21666667]
	 [ 78.40891266  56.16666667]]
Train   Epoch: 147 / 600   Loss: 1.774e+04   Precision: 55.369%   Recall: 60.372%
Valid                   Loss:    1810   Precision: 23.068%   Recall: 69.963%
Train   Epoch: 148 / 600   Loss: 2.028e+04   Precision: 56.247%   Recall: 59.676%
Valid                   Loss:    2271   Precision: 19.119%   Recall: 84.249%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[106.23007965  32.96666667]
	 [ 72.45645905  21.61666667]
	 [ 44.93261337  32.71666667]
	 [110.04820251  37.25      ]
	 [ 60.76160812  28.51666667]]
Train   Epoch: 149 / 600   Loss: 2.054e+04   Precision: 53.148%   Recall: 63.896%
Valid                   Loss:    1740   Precision: 27.974%   Recall: 46.520%
Train   Epoch: 150 / 600   Loss:   2e+04   Precision: 55.539%   Recall: 57.693%
Valid                   Loss:    1892   Precision: 22.935%   Recall: 68.132%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[72.43286133 17.75      ]
	 [49.30711365 34.41666667]
	 [89.1206665  35.38333333]
	 [58.97704697 56.        ]
	 [80.29825592 19.75      ]]
Train   Epoch: 151 / 600   Loss: 1.827e+04   Precision: 54.216%   Recall: 61.672%
Valid                   Loss:    1493   Precision: 32.407%   Recall: 25.641%
Train   Epoch: 152 / 600   Loss: 1.833e+04   Precision: 55.582%   Recall: 60.091%
Valid                   Loss:    1821   Precision: 22.234%   Recall: 75.092%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.85956955  59.38333333]
	 [139.05171204 146.73333333]
	 [103.03313446  16.75      ]
	 [122.36191559  28.56666667]
	 [158.86271667  73.48333333]]
Train   Epoch: 153 / 600   Loss: 1.913e+04   Precision: 54.239%   Recall: 59.863%
Valid                   Loss:    1411   Precision: 36.000%   Recall: 32.967%
Train   Epoch: 154 / 600   Loss: 1.868e+04   Precision: 55.023%   Recall: 58.879%
Valid                   Loss:    2674   Precision: 17.120%   Recall: 87.546%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.76041794  29.        ]
	 [ 62.84502029 115.23333333]
	 [ 88.30123901  57.35      ]
	 [ 86.31611633  19.        ]
	 [ 76.66919708  42.31666667]]
Train   Epoch: 155 / 600   Loss: 1.89e+04   Precision: 53.798%   Recall: 59.535%
Valid                   Loss:    1652   Precision: 30.751%   Recall: 46.520%
Train   Epoch: 156 / 600   Loss: 1.909e+04   Precision: 55.550%   Recall: 61.277%
Valid                   Loss:    2428   Precision: 20.545%   Recall: 82.784%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.28808975 67.08333333]
	 [84.50899506 55.56666667]
	 [59.39247894 37.58333333]
	 [64.57265472 31.13333333]
	 [58.21443176 25.21666667]]
Train   Epoch: 157 / 600   Loss: 2.052e+04   Precision: 55.012%   Recall: 59.515%
Valid                   Loss:    1860   Precision: 26.355%   Recall: 51.648%
Train   Epoch: 158 / 600   Loss: 2.016e+04   Precision: 53.912%   Recall: 62.951%
Valid                   Loss:    2154   Precision: 22.625%   Recall: 66.300%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.24378967  37.11666667]
	 [ 61.7213974   46.66666667]
	 [ 71.5474472   47.11666667]
	 [215.09651184 187.5       ]
	 [ 83.88664246  35.33333333]]
Train   Epoch: 159 / 600   Loss: 1.888e+04   Precision: 56.077%   Recall: 59.709%
Valid                   Loss:    1912   Precision: 28.906%   Recall: 40.659%
Train   Epoch: 160 / 600   Loss: 1.792e+04   Precision: 56.489%   Recall: 58.658%
Valid                   Loss:    2002   Precision: 24.688%   Recall: 57.875%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.52648544  35.03333333]
	 [ 61.725811    34.        ]
	 [164.50254822 123.5       ]
	 [ 58.14178085  39.25      ]
	 [207.50283813  15.41666667]]
Train   Epoch: 161 / 600   Loss: 1.882e+04   Precision: 54.309%   Recall: 59.522%
Valid                   Loss:    2085   Precision: 25.350%   Recall: 66.300%
Train   Epoch: 162 / 600   Loss: 1.848e+04   Precision: 55.795%   Recall: 61.337%
Valid                   Loss:    1708   Precision: 36.864%   Recall: 31.868%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.22847748 69.91666667]
	 [52.82735443 39.11666667]
	 [42.79618073 28.5       ]
	 [50.22497559 52.96666667]
	 [56.13556671 23.8       ]]
Train   Epoch: 163 / 600   Loss: 1.973e+04   Precision: 54.839%   Recall: 60.118%
Valid                   Loss:    1537   Precision: 30.319%   Recall: 41.758%
Train   Epoch: 164 / 600   Loss: 1.831e+04   Precision: 55.869%   Recall: 60.861%
Valid                   Loss:    2367   Precision: 23.859%   Recall: 67.033%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 61.20329666  68.98333333]
	 [200.5974884   67.68333333]
	 [ 74.76965332  16.11666667]
	 [ 47.7735405   48.        ]
	 [ 74.75084686  28.66666667]]
Train   Epoch: 165 / 600   Loss: 1.848e+04   Precision: 56.139%   Recall: 61.223%
Valid                   Loss:    1733   Precision: 31.090%   Recall: 49.084%
Train   Epoch: 166 / 600   Loss: 1.959e+04   Precision: 55.222%   Recall: 61.417%
Valid                   Loss:    2061   Precision: 26.746%   Recall: 57.509%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.22403717  63.25      ]
	 [ 44.45914841  18.        ]
	 [127.04890442  38.        ]
	 [ 89.65028381 227.61666667]
	 [ 50.0541153   26.38333333]]
Train   Epoch: 167 / 600   Loss: 1.808e+04   Precision: 56.465%   Recall: 61.806%
Valid                   Loss:    2906   Precision: 18.415%   Recall: 89.377%
Train   Epoch: 168 / 600   Loss: 1.876e+04   Precision: 55.889%   Recall: 62.677%
Valid                   Loss:    2325   Precision: 23.023%   Recall: 72.527%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 71.24944305  23.5       ]
	 [212.71983337  56.5       ]
	 [ 92.88652802  45.83333333]
	 [ 85.30450439  35.08333333]
	 [ 74.29872131  15.83333333]]
Train   Epoch: 169 / 600   Loss: 1.869e+04   Precision: 55.227%   Recall: 61.009%
Valid                   Loss:    2876   Precision: 18.699%   Recall: 84.249%
Train   Epoch: 170 / 600   Loss: 1.895e+04   Precision: 56.960%   Recall: 59.562%
Valid                   Loss:    1674   Precision: 32.439%   Recall: 48.718%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 54.16959     71.33333333]
	 [194.44067383  48.61666667]
	 [ 35.86342239  20.63333333]
	 [ 43.62135696  21.88333333]
	 [ 45.62354279  34.33333333]]
Train   Epoch: 171 / 600   Loss: 1.956e+04   Precision: 55.085%   Recall: 61.243%
Valid                   Loss:    2168   Precision: 20.040%   Recall: 73.626%
Train   Epoch: 172 / 600   Loss: 1.796e+04   Precision: 55.782%   Recall: 62.683%
Valid                   Loss:    1966   Precision: 28.210%   Recall: 53.114%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 52.70542145  41.2       ]
	 [ 53.70333099  37.5       ]
	 [ 72.10189819  64.        ]
	 [259.70541382  42.93333333]
	 [ 48.36306381 105.8       ]]
Train   Epoch: 173 / 600   Loss: 1.86e+04   Precision: 56.724%   Recall: 59.133%
Valid                   Loss:    1718   Precision: 27.156%   Recall: 54.212%
Train   Epoch: 174 / 600   Loss: 1.833e+04   Precision: 55.239%   Recall: 60.104%
Valid                   Loss:    3030   Precision: 14.853%   Recall: 94.505%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 84.70346069  36.08333333]
	 [ 68.76448059  40.66666667]
	 [115.26123047  52.43333333]
	 [ 66.03398132  20.        ]
	 [ 71.03887177  34.41666667]]
Train   Epoch: 175 / 600   Loss: 1.794e+04   Precision: 55.507%   Recall: 59.415%
Valid                   Loss:    1457   Precision: 32.105%   Recall: 44.689%
Train   Epoch: 176 / 600   Loss: 1.988e+04   Precision: 56.181%   Recall: 59.421%
Valid                   Loss:    2453   Precision: 19.273%   Recall: 83.516%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 84.47641754  28.5       ]
	 [ 58.00152969  17.88333333]
	 [ 69.78218842  25.11666667]
	 [142.46183777  27.36666667]
	 [ 65.39556885  20.98333333]]
Train   Epoch: 177 / 600   Loss: 1.974e+04   Precision: 56.757%   Recall: 59.863%
Valid                   Loss:    1924   Precision: 24.016%   Recall: 67.033%
Train   Epoch: 178 / 600   Loss: 1.952e+04   Precision: 57.114%   Recall: 61.578%
Valid                   Loss:    1342   Precision: 37.427%   Recall: 23.443%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[57.34070969 74.8       ]
	 [66.54029846 43.55      ]
	 [83.28486633 54.33333333]
	 [39.91467667 57.66666667]
	 [52.9435463  23.7       ]]
Train   Epoch: 179 / 600   Loss: 2.009e+04   Precision: 51.535%   Recall: 60.393%
Valid                   Loss:    1601   Precision: 31.144%   Recall: 46.886%
Train   Epoch: 180 / 600   Loss: 1.827e+04   Precision: 55.732%   Recall: 61.089%
Valid                   Loss:    1721   Precision: 33.138%   Recall: 41.392%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.25447845 37.58333333]
	 [42.15067291 39.28333333]
	 [64.50952148 25.5       ]
	 [56.63900757 29.11666667]
	 [59.64054871 32.28333333]]
Train   Epoch: 181 / 600   Loss: 1.733e+04   Precision: 56.027%   Recall: 57.693%
Valid                   Loss:    2306   Precision: 17.011%   Recall: 87.546%
Train   Epoch: 182 / 600   Loss: 1.804e+04   Precision: 53.836%   Recall: 63.594%
Valid                   Loss:    1896   Precision: 30.471%   Recall: 40.293%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.39259338 28.        ]
	 [47.65296936 38.26666667]
	 [38.97827911 18.55      ]
	 [39.32844543 38.16666667]
	 [62.16465759 34.75      ]]
Train   Epoch: 183 / 600   Loss: 1.734e+04   Precision: 54.787%   Recall: 61.826%
Valid                   Loss:    1671   Precision: 28.908%   Recall: 49.451%
Train   Epoch: 184 / 600   Loss: 1.816e+04   Precision: 56.300%   Recall: 59.803%
Valid                   Loss:    2183   Precision: 24.597%   Recall: 61.538%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.32220078 31.61666667]
	 [77.63088226 11.08333333]
	 [56.08285141 26.33333333]
	 [45.5743866  21.13333333]
	 [69.62030792 72.06666667]]
Train   Epoch: 185 / 600   Loss: 1.823e+04   Precision: 55.739%   Recall: 63.393%
Valid                   Loss:    1834   Precision: 29.621%   Recall: 48.718%
Train   Epoch: 186 / 600   Loss: 1.821e+04   Precision: 55.378%   Recall: 61.009%
Valid                   Loss:    2158   Precision: 21.318%   Recall: 75.824%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.92956543 376.88333333]
	 [ 67.84024048  20.3       ]
	 [ 60.89099121  32.21666667]
	 [ 78.79234314  25.75      ]
	 [ 70.15753937  34.55      ]]
Train   Epoch: 187 / 600   Loss: 1.733e+04   Precision: 55.843%   Recall: 62.094%
Valid                   Loss:    1913   Precision: 26.273%   Recall: 58.608%
Train   Epoch: 188 / 600   Loss: 1.92e+04   Precision: 55.927%   Recall: 62.255%
Valid                   Loss:    2061   Precision: 23.226%   Recall: 65.934%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.24757385  47.5       ]
	 [ 54.58558655  40.58333333]
	 [ 98.95558929  26.        ]
	 [279.38195801 124.33333333]
	 [ 73.39359283  57.96666667]]
Train   Epoch: 189 / 600   Loss: 1.693e+04   Precision: 56.769%   Recall: 60.587%
Valid                   Loss:    2405   Precision: 24.520%   Recall: 60.806%
Train   Epoch: 190 / 600   Loss: 1.985e+04   Precision: 55.952%   Recall: 59.535%
Valid                   Loss:    1886   Precision: 26.380%   Recall: 63.004%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.13228226 75.88333333]
	 [44.55700302 26.45      ]
	 [47.10913849 23.2       ]
	 [56.63433838 21.91666667]
	 [67.38798523 22.38333333]]
Train   Epoch: 191 / 600   Loss: 1.871e+04   Precision: 56.915%   Recall: 60.781%
Valid                   Loss:    2421   Precision: 22.486%   Recall: 76.190%
Train   Epoch: 192 / 600   Loss: 1.905e+04   Precision: 57.015%   Recall: 61.243%
Valid                   Loss:    1674   Precision: 27.482%   Recall: 56.777%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.19517899 56.8       ]
	 [61.75986481 20.6       ]
	 [47.61117935 67.36666667]
	 [47.35506439 54.33333333]
	 [43.51455307 40.83333333]]
Train   Epoch: 193 / 600   Loss: 1.906e+04   Precision: 55.305%   Recall: 59.743%
Valid                   Loss:    2360   Precision: 22.896%   Recall: 67.766%
Train   Epoch: 194 / 600   Loss: 1.767e+04   Precision: 56.791%   Recall: 59.716%
Valid                   Loss:    1978   Precision: 26.079%   Recall: 55.311%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[59.95473099 26.9       ]
	 [68.25876617 22.36666667]
	 [77.85037231 22.8       ]
	 [54.07143402 35.58333333]
	 [95.27403259 49.16666667]]
Train   Epoch: 195 / 600   Loss: 1.897e+04   Precision: 54.133%   Recall: 60.714%
Valid                   Loss:    1876   Precision: 27.305%   Recall: 56.410%
Train   Epoch: 196 / 600   Loss: 1.83e+04   Precision: 55.747%   Recall: 59.776%
Valid                   Loss:    1971   Precision: 19.963%   Recall: 78.755%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[71.73074341 20.        ]
	 [68.51231384 44.98333333]
	 [84.47233582 33.95      ]
	 [69.07854462 37.51666667]
	 [92.25875854 19.21666667]]
Train   Epoch: 197 / 600   Loss: 1.801e+04   Precision: 54.469%   Recall: 60.413%
Valid                   Loss:    1871   Precision: 23.077%   Recall: 65.934%
Train   Epoch: 198 / 600   Loss: 1.789e+04   Precision: 55.781%   Recall: 61.076%
Valid                   Loss:    1388   Precision: 38.660%   Recall: 27.473%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.927845   45.66666667]
	 [37.0804863  31.31666667]
	 [54.69432831 24.41666667]
	 [34.67250443 36.03333333]
	 [40.42152405 38.88333333]]
Train   Epoch: 199 / 600   Loss: 1.917e+04   Precision: 56.608%   Recall: 56.749%
Valid                   Loss:    2421   Precision: 18.563%   Recall: 84.249%
Train   Epoch: 200 / 600   Loss: 1.844e+04   Precision: 56.070%   Recall: 60.882%
Valid                   Loss:    2036   Precision: 27.141%   Recall: 61.538%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.2456398   33.71666667]
	 [ 69.82672119  25.91666667]
	 [111.8970871   21.78333333]
	 [ 73.34373474  98.63333333]
	 [ 98.24208832  44.11666667]]
Train   Epoch: 201 / 600   Loss: 1.707e+04   Precision: 55.305%   Recall: 59.850%
Valid                   Loss:    1646   Precision: 27.051%   Recall: 44.689%
Train   Epoch: 202 / 600   Loss: 1.848e+04   Precision: 56.435%   Recall: 60.687%
Valid                   Loss:    2291   Precision: 25.564%   Recall: 62.271%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.60340118  27.01666667]
	 [ 64.9732132   10.75      ]
	 [ 72.34918213  70.71666667]
	 [ 74.25267792  46.33333333]
	 [173.72953796  58.2       ]]
Train   Epoch: 203 / 600   Loss: 1.737e+04   Precision: 56.483%   Recall: 61.223%
Valid                   Loss:    1888   Precision: 25.887%   Recall: 53.480%
Train   Epoch: 204 / 600   Loss: 1.707e+04   Precision: 56.107%   Recall: 59.723%
Valid                   Loss:    1665   Precision: 21.540%   Recall: 65.568%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[486.33267212 240.16666667]
	 [ 44.87781906  24.91666667]
	 [ 50.78358078  24.31666667]
	 [ 85.22153473  10.21666667]
	 [ 68.10312653  37.58333333]]
Train   Epoch: 205 / 600   Loss: 1.808e+04   Precision: 54.987%   Recall: 62.925%
Valid                   Loss:    1545   Precision: 28.931%   Recall: 33.700%
Train   Epoch: 206 / 600   Loss: 1.807e+04   Precision: 56.718%   Recall: 61.163%
Valid                   Loss:    1769   Precision: 27.140%   Recall: 47.619%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 73.84526825  22.95      ]
	 [ 36.68805695  19.28333333]
	 [ 61.23353958 104.05      ]
	 [ 45.12509155  41.83333333]
	 [ 52.751091    16.76666667]]
Train   Epoch: 207 / 600   Loss: 1.731e+04   Precision: 54.562%   Recall: 63.688%
Valid                   Loss:    1907   Precision: 28.032%   Recall: 51.648%
Train   Epoch: 208 / 600   Loss: 1.823e+04   Precision: 56.294%   Recall: 60.600%
Valid                   Loss:    1857   Precision: 30.491%   Recall: 43.223%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.17744446 60.05      ]
	 [71.39125061 33.        ]
	 [41.89619446 24.46666667]
	 [39.02219772 21.        ]
	 [56.93604279 24.        ]]
Train   Epoch: 209 / 600   Loss: 1.763e+04   Precision: 58.202%   Recall: 59.508%
Valid                   Loss:    2293   Precision: 24.317%   Recall: 68.498%
Train   Epoch: 210 / 600   Loss: 1.801e+04   Precision: 55.033%   Recall: 62.040%
Valid                   Loss:    1923   Precision: 26.810%   Recall: 63.736%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.02606964  10.        ]
	 [ 40.79277802  28.03333333]
	 [ 73.44944763  51.61666667]
	 [419.18276978  59.        ]
	 [ 75.04774475  23.16666667]]
Train   Epoch: 211 / 600   Loss: 1.66e+04   Precision: 55.492%   Recall: 61.451%
Valid                   Loss:    1855   Precision: 25.177%   Recall: 65.201%
Train   Epoch: 212 / 600   Loss: 1.718e+04   Precision: 57.783%   Recall: 60.473%
Valid                   Loss:    2367   Precision: 27.634%   Recall: 66.300%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.04550171  45.2       ]
	 [ 75.62123871  17.33333333]
	 [ 38.44271088  30.03333333]
	 [ 51.24267578  36.        ]
	 [138.99237061  44.28333333]]
Train   Epoch: 213 / 600   Loss: 1.86e+04   Precision: 53.289%   Recall: 62.348%
Valid                   Loss:    1763   Precision: 26.786%   Recall: 49.451%
Train   Epoch: 214 / 600   Loss: 1.83e+04   Precision: 54.895%   Recall: 60.660%
Valid                   Loss:    1722   Precision: 31.250%   Recall: 49.451%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 92.12591553 184.8       ]
	 [ 40.67213821  33.86666667]
	 [ 41.36561203  26.5       ]
	 [ 55.31435394  38.        ]
	 [ 39.41912079  37.95      ]]
Train   Epoch: 215 / 600   Loss: 1.76e+04   Precision: 57.592%   Recall: 61.712%
Valid                   Loss:    2362   Precision: 25.259%   Recall: 71.429%
Train   Epoch: 216 / 600   Loss: 1.778e+04   Precision: 55.961%   Recall: 62.503%
Valid                   Loss:    1942   Precision: 20.660%   Recall: 78.022%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[63.60400009 57.43333333]
	 [60.9369812  21.5       ]
	 [60.28163147 26.41666667]
	 [65.59832764 48.5       ]
	 [95.98633575 25.26666667]]
Train   Epoch: 217 / 600   Loss: 1.842e+04   Precision: 54.589%   Recall: 62.389%
Valid                   Loss:    1565   Precision: 32.277%   Recall: 41.026%
Train   Epoch: 218 / 600   Loss: 1.808e+04   Precision: 58.201%   Recall: 63.012%
Valid                   Loss:    1564   Precision: 30.531%   Recall: 50.549%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.23427582  40.        ]
	 [102.4243927   28.76666667]
	 [ 94.89591217  27.        ]
	 [112.19074249  27.23333333]
	 [ 48.76928711  51.61666667]]
Train   Epoch: 219 / 600   Loss: 1.738e+04   Precision: 57.502%   Recall: 63.380%
Valid                   Loss:    2271   Precision: 26.418%   Recall: 69.963%
Train   Epoch: 220 / 600   Loss: 1.671e+04   Precision: 55.734%   Recall: 64.880%
Valid                   Loss:    2726   Precision: 17.997%   Recall: 80.952%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[81.47734833 56.98333333]
	 [73.28884125 57.41666667]
	 [77.02812958 43.91666667]
	 [45.53900146 31.91666667]
	 [81.28058624 36.33333333]]
Train   Epoch: 221 / 600   Loss: 1.88e+04   Precision: 52.913%   Recall: 61.082%
Valid                   Loss:    1835   Precision: 28.065%   Recall: 63.736%
Train   Epoch: 222 / 600   Loss: 1.718e+04   Precision: 58.614%   Recall: 63.125%
Valid                   Loss:    2173   Precision: 23.780%   Recall: 69.597%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 60.92821121  15.13333333]
	 [174.48854065 100.16666667]
	 [ 92.36594391  59.36666667]
	 [ 42.37717819  30.38333333]
	 [ 52.39593506  21.28333333]]
Train   Epoch: 223 / 600   Loss: 1.815e+04   Precision: 55.556%   Recall: 61.618%
Valid                   Loss:    1824   Precision: 27.613%   Recall: 64.835%
Train   Epoch: 224 / 600   Loss: 1.699e+04   Precision: 58.923%   Recall: 61.062%
Valid                   Loss:    1825   Precision: 25.997%   Recall: 64.469%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.46015167 16.61666667]
	 [60.74876785 23.08333333]
	 [69.78379059 34.8       ]
	 [80.45018005 14.83333333]
	 [44.69566345 35.96666667]]
Train   Epoch: 225 / 600   Loss: 1.762e+04   Precision: 56.357%   Recall: 62.683%
Valid                   Loss:    2240   Precision: 30.153%   Recall: 57.875%
Train   Epoch: 226 / 600   Loss: 1.865e+04   Precision: 56.207%   Recall: 64.023%
Valid                   Loss:    1962   Precision: 28.997%   Recall: 39.194%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[505.07452393  77.51666667]
	 [ 65.06698608  21.61666667]
	 [125.62553406  20.03333333]
	 [ 50.84951019  54.86666667]
	 [ 59.04200745 105.21666667]]
Train   Epoch: 227 / 600   Loss: 1.695e+04   Precision: 57.733%   Recall: 61.109%
Valid                   Loss:    2209   Precision: 20.755%   Recall: 80.586%
Train   Epoch: 228 / 600   Loss: 1.862e+04   Precision: 57.372%   Recall: 63.648%
Valid                   Loss:    1272   Precision: 42.424%   Recall: 20.513%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 72.86164093  35.11666667]
	 [ 55.1477623  105.        ]
	 [ 68.38799286  44.        ]
	 [ 43.90949631  62.11666667]
	 [ 50.16786194  32.45      ]]
Train   Epoch: 229 / 600   Loss: 1.706e+04   Precision: 56.579%   Recall: 63.943%
Valid                   Loss:    1520   Precision: 33.228%   Recall: 38.462%
Train   Epoch: 230 / 600   Loss: 1.653e+04   Precision: 56.934%   Recall: 64.512%
Valid                   Loss:    2901   Precision: 25.160%   Recall: 71.795%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.77075195  45.91666667]
	 [ 55.04060745  26.78333333]
	 [ 90.34178162 104.55      ]
	 [ 64.14244843  35.        ]
	 [ 95.14414978  12.21666667]]
Train   Epoch: 231 / 600   Loss: 1.684e+04   Precision: 57.668%   Recall: 62.241%
Valid                   Loss:    1965   Precision: 27.634%   Recall: 66.300%
Train   Epoch: 232 / 600   Loss: 1.679e+04   Precision: 58.974%   Recall: 63.012%
Valid                   Loss:    1837   Precision: 26.965%   Recall: 64.103%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[247.87272644 127.5       ]
	 [ 55.42123795  27.83333333]
	 [ 44.22261429  56.71666667]
	 [ 76.3113327   46.66666667]
	 [ 40.75377655  38.21666667]]
Train   Epoch: 233 / 600   Loss: 1.658e+04   Precision: 58.904%   Recall: 62.415%
Valid                   Loss:    1476   Precision: 32.161%   Recall: 46.886%
Train   Epoch: 234 / 600   Loss: 1.665e+04   Precision: 58.878%   Recall: 63.594%
Valid                   Loss:    2000   Precision: 22.454%   Recall: 71.062%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[146.33418274  20.5       ]
	 [ 71.1925354   59.13333333]
	 [ 48.31387329  28.21666667]
	 [ 87.48581696  41.5       ]
	 [ 58.383461    46.45      ]]
Train   Epoch: 235 / 600   Loss: 1.709e+04   Precision: 58.700%   Recall: 62.797%
Valid                   Loss:    2119   Precision: 24.114%   Recall: 64.835%
Train   Epoch: 236 / 600   Loss: 1.667e+04   Precision: 57.428%   Recall: 62.348%
Valid                   Loss:    1664   Precision: 29.298%   Recall: 61.172%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[139.10012817  59.5       ]
	 [ 53.24353409  14.53333333]
	 [ 42.86960983  52.06666667]
	 [ 58.46624374  75.91666667]
	 [ 62.73479462  47.41666667]]
Train   Epoch: 237 / 600   Loss: 1.632e+04   Precision: 59.053%   Recall: 63.708%
Valid                   Loss:    2285   Precision: 25.034%   Recall: 68.132%
Train   Epoch: 238 / 600   Loss: 1.67e+04   Precision: 58.179%   Recall: 61.417%
Valid                   Loss:    2393   Precision: 23.236%   Recall: 78.388%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[113.6725769  309.83333333]
	 [254.03430176  44.78333333]
	 [ 43.99766922  29.7       ]
	 [ 54.80503082  55.        ]
	 [ 47.7601738   48.16666667]]
Train   Epoch: 239 / 600   Loss: 1.621e+04   Precision: 58.006%   Recall: 64.720%
Valid                   Loss:    2029   Precision: 22.902%   Recall: 73.993%
Train   Epoch: 240 / 600   Loss: 1.584e+04   Precision: 58.443%   Recall: 63.313%
Valid                   Loss:    2342   Precision: 29.084%   Recall: 53.480%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.16734314  88.78333333]
	 [ 79.99008179  76.83333333]
	 [139.40484619  19.86666667]
	 [ 79.50514221  20.31666667]
	 [ 50.17952347  18.33333333]]
Train   Epoch: 241 / 600   Loss: 1.558e+04   Precision: 58.700%   Recall: 63.902%
Valid                   Loss:    1525   Precision: 29.412%   Recall: 51.282%
Train   Epoch: 242 / 600   Loss: 1.761e+04   Precision: 58.638%   Recall: 63.159%
Valid                   Loss:    1603   Precision: 30.603%   Recall: 52.015%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.64096451  27.41666667]
	 [ 38.82383347  29.45      ]
	 [137.56462097  14.36666667]
	 [ 42.60631943  32.38333333]
	 [107.23723602 118.2       ]]
Train   Epoch: 243 / 600   Loss: 1.798e+04   Precision: 59.564%   Recall: 61.993%
Valid                   Loss:    1987   Precision: 29.512%   Recall: 50.916%
Train   Epoch: 244 / 600   Loss: 1.669e+04   Precision: 59.868%   Recall: 62.563%
Valid                   Loss:    1823   Precision: 28.294%   Recall: 47.985%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 82.11685181  95.16666667]
	 [102.36782074  71.75      ]
	 [ 40.41999435  27.05      ]
	 [ 89.09918976  20.41666667]
	 [ 33.06407166  23.28333333]]
Train   Epoch: 245 / 600   Loss: 1.694e+04   Precision: 60.206%   Recall: 62.791%
Valid                   Loss:    1634   Precision: 28.037%   Recall: 54.945%
Train   Epoch: 246 / 600   Loss: 1.636e+04   Precision: 57.727%   Recall: 60.855%
Valid                   Loss:    2147   Precision: 23.776%   Recall: 49.817%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.06562042 38.4       ]
	 [42.54456329 11.45      ]
	 [38.70097351 26.38333333]
	 [83.00987244 30.8       ]
	 [41.15778732 42.1       ]]
Train   Epoch: 247 / 600   Loss: 1.656e+04   Precision: 60.114%   Recall: 60.975%
Valid                   Loss:    1558   Precision: 24.542%   Recall: 63.736%
Train   Epoch: 248 / 600   Loss: 1.742e+04   Precision: 57.244%   Recall: 63.675%
Valid                   Loss:    2161   Precision: 25.568%   Recall: 65.934%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 523.2980957  1031.85      ]
	 [  60.194767     39.        ]
	 [  46.51763153   37.75      ]
	 [  75.53060913  109.03333333]
	 [ 109.45132446   21.9       ]]
Train   Epoch: 249 / 600   Loss: 1.663e+04   Precision: 60.077%   Recall: 61.538%
Valid                   Loss:    1449   Precision: 33.559%   Recall: 54.579%
Train   Epoch: 250 / 600   Loss: 1.839e+04   Precision: 57.916%   Recall: 62.556%
Valid                   Loss:    1463   Precision: 33.636%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.9477005   17.61666667]
	 [ 43.34266663  21.91666667]
	 [ 61.728508    77.33333333]
	 [ 37.04953003 220.        ]
	 [ 74.03600311 106.21666667]]
Train   Epoch: 251 / 600   Loss: 1.632e+04   Precision: 59.196%   Recall: 62.670%
Valid                   Loss:    1456   Precision: 29.619%   Recall: 36.996%
Train   Epoch: 252 / 600   Loss: 1.57e+04   Precision: 61.193%   Recall: 61.029%
Valid                   Loss:    1646   Precision: 31.061%   Recall: 45.055%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[151.28417969  37.66666667]
	 [ 40.52884293  26.13333333]
	 [ 41.95995712  32.16666667]
	 [ 36.91174316  37.06666667]
	 [ 58.9805603   37.46666667]]
Train   Epoch: 253 / 600   Loss: 1.701e+04   Precision: 59.961%   Recall: 63.226%
Valid                   Loss:    1601   Precision: 33.333%   Recall: 36.630%
Train   Epoch: 254 / 600   Loss: 1.625e+04   Precision: 61.899%   Recall: 61.190%
Valid                   Loss:    1714   Precision: 32.250%   Recall: 47.253%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.87590027 22.16666667]
	 [68.37340546 64.46666667]
	 [42.34455872 18.95      ]
	 [49.44219208 59.16666667]
	 [45.83932877 26.        ]]
Train   Epoch: 255 / 600   Loss: 1.67e+04   Precision: 60.633%   Recall: 61.993%
Valid                   Loss:    2218   Precision: 23.798%   Recall: 74.359%
Train   Epoch: 256 / 600   Loss: 1.579e+04   Precision: 60.796%   Recall: 60.506%
Valid                   Loss:    2063   Precision: 28.148%   Recall: 55.678%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.41644287  26.5       ]
	 [109.70330048  52.55      ]
	 [ 50.67845917  40.78333333]
	 [ 54.42502975  32.68333333]
	 [ 72.09329987  26.65      ]]
Train   Epoch: 257 / 600   Loss: 1.674e+04   Precision: 57.571%   Recall: 62.241%
Valid                   Loss:    2017   Precision: 27.009%   Recall: 44.322%
Train   Epoch: 258 / 600   Loss: 1.627e+04   Precision: 59.573%   Recall: 59.274%
Valid                   Loss:    1833   Precision: 27.506%   Recall: 43.223%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.46547318  54.53333333]
	 [ 61.0533638   85.        ]
	 [ 40.01803207 106.41666667]
	 [ 57.11725998  42.75      ]
	 [116.61781311  48.96666667]]
Train   Epoch: 259 / 600   Loss: 1.583e+04   Precision: 59.654%   Recall: 59.810%
Valid                   Loss:    1936   Precision: 26.676%   Recall: 67.033%
Train   Epoch: 260 / 600   Loss: 1.618e+04   Precision: 59.862%   Recall: 63.347%
Valid                   Loss:    2557   Precision: 25.050%   Recall: 46.154%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[123.2709198   39.93333333]
	 [ 76.16522217  12.88333333]
	 [102.24700165 111.08333333]
	 [ 66.61116791  26.58333333]
	 [ 99.43344116  59.66666667]]
Train   Epoch: 261 / 600   Loss: 1.618e+04   Precision: 60.582%   Recall: 60.533%
Valid                   Loss:    1781   Precision: 27.191%   Recall: 55.678%
Train   Epoch: 262 / 600   Loss: 1.699e+04   Precision: 58.522%   Recall: 61.337%
Valid                   Loss:    2475   Precision: 27.692%   Recall: 65.934%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.82564545 55.36666667]
	 [74.10839081 17.11666667]
	 [56.7156601  31.86666667]
	 [71.41773987 21.9       ]
	 [49.19576645 55.28333333]]
Train   Epoch: 263 / 600   Loss: 1.707e+04   Precision: 59.552%   Recall: 59.823%
Valid                   Loss:    1560   Precision: 25.056%   Recall: 40.659%
Train   Epoch: 264 / 600   Loss: 1.528e+04   Precision: 61.039%   Recall: 60.520%
Valid                   Loss:    1552   Precision: 36.131%   Recall: 36.264%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.8604126   25.75      ]
	 [107.44435883  26.5       ]
	 [ 53.38056183  22.        ]
	 [136.11405945  21.9       ]
	 [ 39.66368484  23.88333333]]
Train   Epoch: 265 / 600   Loss: 1.685e+04   Precision: 60.503%   Recall: 59.113%
Valid                   Loss:    1784   Precision: 30.093%   Recall: 47.619%
Train   Epoch: 266 / 600   Loss: 1.491e+04   Precision: 60.791%   Recall: 60.506%
Valid                   Loss:    1729   Precision: 33.099%   Recall: 34.432%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[74.11119843 54.66666667]
	 [32.57841492 32.86666667]
	 [29.04447937 25.8       ]
	 [70.25952911 68.21666667]
	 [46.3409996  29.13333333]]
Train   Epoch: 267 / 600   Loss: 1.62e+04   Precision: 59.106%   Recall: 60.607%
Valid                   Loss:    1359   Precision: 34.337%   Recall: 41.758%
Train   Epoch: 268 / 600   Loss: 1.649e+04   Precision: 51.991%   Recall: 65.430%
Valid                   Loss:    1664   Precision: 27.474%   Recall: 68.132%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[109.41072845 102.95      ]
	 [ 55.13425827  37.38333333]
	 [ 54.37227249  69.21666667]
	 [ 47.5620575   24.55      ]
	 [142.46237183  24.        ]]
Train   Epoch: 269 / 600   Loss: 1.773e+04   Precision: 57.618%   Recall: 62.114%
Valid                   Loss:    1817   Precision: 28.039%   Recall: 63.370%
Train   Epoch: 270 / 600   Loss: 1.594e+04   Precision: 60.323%   Recall: 61.705%
Valid                   Loss:    1389   Precision: 31.233%   Recall: 41.758%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 52.15735245  24.35      ]
	 [ 43.21969604  22.66666667]
	 [ 50.0626297   44.28333333]
	 [ 90.66189575 110.33333333]
	 [ 36.11252213  41.45      ]]
Train   Epoch: 271 / 600   Loss: 1.602e+04   Precision: 62.630%   Recall: 59.060%
Valid                   Loss:    3066   Precision: 21.088%   Recall: 80.952%
Train   Epoch: 272 / 600   Loss: 1.612e+04   Precision: 62.090%   Recall: 60.252%
Valid                   Loss:    2375   Precision: 23.394%   Recall: 70.696%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 85.94342804  30.        ]
	 [119.12324524 149.38333333]
	 [ 87.07108307  43.05      ]
	 [ 54.69938278  12.38333333]
	 [ 58.18848038  63.73333333]]
Train   Epoch: 273 / 600   Loss: 2.007e+04   Precision: 52.274%   Recall: 59.897%
Valid                   Loss:    2464   Precision: 26.337%   Recall: 72.161%
Train   Epoch: 274 / 600   Loss: 1.68e+04   Precision: 55.368%   Recall: 64.941%
Valid                   Loss:    2991   Precision: 19.866%   Recall: 86.813%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.08068085  32.6       ]
	 [ 63.83337784  24.        ]
	 [ 66.32963562  24.81666667]
	 [105.38583374  38.33333333]
	 [ 85.74934387  25.        ]]
Train   Epoch: 275 / 600   Loss: 1.673e+04   Precision: 59.401%   Recall: 60.547%
Valid                   Loss:    1815   Precision: 23.901%   Recall: 63.736%
Train   Epoch: 276 / 600   Loss: 1.504e+04   Precision: 62.052%   Recall: 60.372%
Valid                   Loss:    1917   Precision: 27.706%   Recall: 46.886%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.68319321  91.25      ]
	 [262.8354187    6.13333333]
	 [ 40.10387802  18.85      ]
	 [ 64.68888092  40.08333333]
	 [ 39.87541962  24.5       ]]
Train   Epoch: 277 / 600   Loss: 1.452e+04   Precision: 60.940%   Recall: 62.422%
Valid                   Loss:    1912   Precision: 28.125%   Recall: 56.044%
Train   Epoch: 278 / 600   Loss: 1.624e+04   Precision: 60.016%   Recall: 60.808%
Valid                   Loss:    1609   Precision: 31.818%   Recall: 35.897%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.22029877 50.28333333]
	 [36.3352623  34.75      ]
	 [48.31258011 46.28333333]
	 [73.88842773 27.83333333]
	 [41.49252701 22.41666667]]
Train   Epoch: 279 / 600   Loss: 1.555e+04   Precision: 60.986%   Recall: 59.662%
Valid                   Loss:    2391   Precision: 24.730%   Recall: 67.033%
Train   Epoch: 280 / 600   Loss: 1.543e+04   Precision: 60.246%   Recall: 63.862%
Valid                   Loss:    1483   Precision: 29.691%   Recall: 45.788%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.1276474  25.83333333]
	 [41.49899673 21.3       ]
	 [61.81254578 47.65      ]
	 [44.98129272 31.88333333]
	 [42.58216858 46.76666667]]
Train   Epoch: 281 / 600   Loss: 1.612e+04   Precision: 62.035%   Recall: 59.368%
Valid                   Loss:    1548   Precision: 33.441%   Recall: 38.095%
Train   Epoch: 282 / 600   Loss: 1.727e+04   Precision: 58.167%   Recall: 64.097%
Valid                   Loss:    1596   Precision: 28.395%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.5651474    8.61666667]
	 [ 46.4829216   25.        ]
	 [ 43.91883087  22.33333333]
	 [ 44.65113449  41.68333333]
	 [103.52388     39.13333333]]
Train   Epoch: 283 / 600   Loss: 1.436e+04   Precision: 61.640%   Recall: 61.417%
Valid                   Loss:    1624   Precision: 31.875%   Recall: 37.363%
Train   Epoch: 284 / 600   Loss: 1.57e+04   Precision: 60.525%   Recall: 60.419%
Valid                   Loss:    2023   Precision: 26.336%   Recall: 50.549%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.69467545  44.35      ]
	 [ 80.70233917 104.31666667]
	 [ 75.88708496  85.        ]
	 [ 53.17120361  26.2       ]
	 [120.4249115   17.78333333]]
Train   Epoch: 285 / 600   Loss: 1.424e+04   Precision: 62.171%   Recall: 62.154%
Valid                   Loss:    2133   Precision: 24.161%   Recall: 65.934%
Train   Epoch: 286 / 600   Loss: 1.564e+04   Precision: 61.657%   Recall: 61.277%
Valid                   Loss:    1517   Precision: 29.577%   Recall: 46.154%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.9789505  60.36666667]
	 [34.6799202  40.        ]
	 [42.63710785 26.03333333]
	 [37.6612587  24.        ]
	 [56.0950737  45.15      ]]
Train   Epoch: 287 / 600   Loss: 1.457e+04   Precision: 62.447%   Recall: 59.904%
Valid                   Loss:    1481   Precision: 38.258%   Recall: 36.996%
Train   Epoch: 288 / 600   Loss: 1.474e+04   Precision: 61.749%   Recall: 61.960%
Valid                   Loss:    3016   Precision: 17.722%   Recall: 56.410%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 94.27687836  30.33333333]
	 [ 51.92876434  33.08333333]
	 [ 66.92050934  67.        ]
	 [ 89.27256012  28.78333333]
	 [100.63751221  31.66666667]]
Train   Epoch: 289 / 600   Loss: 1.419e+04   Precision: 60.019%   Recall: 60.171%
Valid                   Loss:    1699   Precision: 36.774%   Recall: 41.758%
Train   Epoch: 290 / 600   Loss: 1.461e+04   Precision: 63.051%   Recall: 60.031%
Valid                   Loss:    1749   Precision: 28.022%   Recall: 56.044%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.98898315  21.2       ]
	 [ 42.87320328  21.33333333]
	 [ 80.80895996  35.        ]
	 [ 42.96657562  41.        ]
	 [131.1325531   48.53333333]]
Train   Epoch: 291 / 600   Loss: 1.511e+04   Precision: 62.850%   Recall: 61.839%
Valid                   Loss:    1965   Precision: 29.647%   Recall: 46.154%
Train   Epoch: 292 / 600   Loss: 1.446e+04   Precision: 62.673%   Recall: 63.882%
Valid                   Loss:    1879   Precision: 33.060%   Recall: 44.322%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.39647675 48.71666667]
	 [51.58830643 33.63333333]
	 [54.4272995  30.08333333]
	 [71.75656891 37.48333333]
	 [65.76148224 38.15      ]]
Train   Epoch: 293 / 600   Loss: 1.507e+04   Precision: 63.082%   Recall: 61.029%
Valid                   Loss:    1542   Precision: 30.591%   Recall: 53.114%
Train   Epoch: 294 / 600   Loss: 1.348e+04   Precision: 63.494%   Recall: 62.027%
Valid                   Loss:    1759   Precision: 33.243%   Recall: 44.689%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 82.28170776  59.11666667]
	 [ 67.36689758  70.13333333]
	 [ 60.01643372  30.16666667]
	 [109.26318359  89.        ]
	 [ 44.1919136   40.88333333]]
Train   Epoch: 295 / 600   Loss: 1.529e+04   Precision: 59.042%   Recall: 61.210%
Valid                   Loss:    1633   Precision: 30.861%   Recall: 38.095%
Train   Epoch: 296 / 600   Loss: 1.47e+04   Precision: 61.957%   Recall: 61.571%
Valid                   Loss:    1706   Precision: 27.273%   Recall: 46.154%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.14225006  67.26666667]
	 [293.55801392  23.7       ]
	 [ 92.19695282  35.53333333]
	 [ 45.69073105  32.9       ]
	 [ 47.15631104  17.33333333]]
Train   Epoch: 297 / 600   Loss: 1.551e+04   Precision: 62.257%   Recall: 62.415%
Valid                   Loss:    2378   Precision: 26.118%   Recall: 53.480%
Train   Epoch: 298 / 600   Loss: 1.44e+04   Precision: 63.950%   Recall: 59.435%
Valid                   Loss:    1595   Precision: 31.017%   Recall: 45.788%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.83395386  15.75      ]
	 [ 89.77285004  86.16666667]
	 [ 67.96781921  98.33333333]
	 [ 55.11717987 110.38333333]
	 [ 97.76045227  76.66666667]]
Train   Epoch: 299 / 600   Loss: 1.463e+04   Precision: 62.643%   Recall: 61.183%
Valid                   Loss:    1646   Precision: 30.447%   Recall: 39.927%
Train   Epoch: 300 / 600   Loss: 1.385e+04   Precision: 63.986%   Recall: 58.946%
Valid                   Loss:    1571   Precision: 29.185%   Recall: 49.817%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  47.14108658   14.55      ]
	 [  43.5104599    37.36666667]
	 [ 865.91070557 2828.        ]
	 [ 106.10858154   13.53333333]
	 [ 107.98799896   43.        ]]
Train   Epoch: 301 / 600   Loss: 1.305e+04   Precision: 64.416%   Recall: 60.895%
Valid                   Loss:    1392   Precision: 37.884%   Recall: 40.659%
Train   Epoch: 302 / 600   Loss: 1.471e+04   Precision: 63.627%   Recall: 62.208%
Valid                   Loss:    1599   Precision: 32.222%   Recall: 42.491%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.07578659 21.16666667]
	 [76.20201874 22.        ]
	 [38.71916199 58.3       ]
	 [44.57509995 31.25      ]
	 [68.945755   21.46666667]]
Train   Epoch: 303 / 600   Loss: 1.429e+04   Precision: 65.161%   Recall: 60.975%
Valid                   Loss:    2178   Precision: 28.440%   Recall: 56.777%
Train   Epoch: 304 / 600   Loss: 1.263e+04   Precision: 63.758%   Recall: 60.547%
Valid                   Loss:    1587   Precision: 31.332%   Recall: 43.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.80241776 27.21666667]
	 [49.40989304 15.        ]
	 [89.89648438 41.68333333]
	 [35.18823624 11.45      ]
	 [38.59164047 36.55      ]]
Train   Epoch: 305 / 600   Loss: 1.361e+04   Precision: 65.069%   Recall: 60.393%
Valid                   Loss:    1376   Precision: 40.726%   Recall: 36.996%
Train   Epoch: 306 / 600   Loss: 1.362e+04   Precision: 64.355%   Recall: 60.285%
Valid                   Loss:    1767   Precision: 30.296%   Recall: 45.055%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.99301147 22.88333333]
	 [63.75644302 31.9       ]
	 [58.36788177 49.55      ]
	 [54.81820679 30.73333333]
	 [37.42146683 29.71666667]]
Train   Epoch: 307 / 600   Loss: 1.443e+04   Precision: 65.122%   Recall: 60.707%
Valid                   Loss:    1493   Precision: 40.994%   Recall: 24.176%
Train   Epoch: 308 / 600   Loss: 1.539e+04   Precision: 64.207%   Recall: 61.304%
Valid                   Loss:    2672   Precision: 21.977%   Recall: 83.883%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.71205902  62.75      ]
	 [ 49.20187378  42.5       ]
	 [ 83.91986084  30.73333333]
	 [ 55.58134842  44.78333333]
	 [141.44152832  29.28333333]]
Train   Epoch: 309 / 600   Loss: 1.472e+04   Precision: 60.334%   Recall: 60.265%
Valid                   Loss:    1679   Precision: 31.765%   Recall: 39.560%
Train   Epoch: 310 / 600   Loss: 1.451e+04   Precision: 63.866%   Recall: 60.580%
Valid                   Loss:    1813   Precision: 32.161%   Recall: 46.886%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[104.37540436  93.33333333]
	 [ 78.5356369   29.66666667]
	 [ 45.56264877  28.83333333]
	 [ 41.03375626  35.25      ]
	 [ 53.83120728  33.05      ]]
Train   Epoch: 311 / 600   Loss: 1.334e+04   Precision: 65.565%   Recall: 60.975%
Valid                   Loss:    1503   Precision: 37.700%   Recall: 43.223%
Train   Epoch: 312 / 600   Loss: 1.381e+04   Precision: 65.108%   Recall: 61.545%
Valid                   Loss:    1797   Precision: 30.152%   Recall: 50.916%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[301.78665161  25.66666667]
	 [ 53.58161545  17.66666667]
	 [ 68.80956268  14.        ]
	 [ 35.63295746  34.43333333]
	 [434.68557739 306.46666667]]
Train   Epoch: 313 / 600   Loss: 1.283e+04   Precision: 64.871%   Recall: 60.748%
Valid                   Loss:    1605   Precision: 36.667%   Recall: 40.293%
Train   Epoch: 314 / 600   Loss: 1.407e+04   Precision: 65.353%   Recall: 59.522%
Valid                   Loss:    1824   Precision: 29.707%   Recall: 52.015%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.72668839  8.88333333]
	 [59.60188293 89.        ]
	 [99.75584412 50.28333333]
	 [76.86263275 20.        ]
	 [46.75867081 26.1       ]]
Train   Epoch: 315 / 600   Loss: 1.311e+04   Precision: 65.488%   Recall: 61.632%
Valid                   Loss:    1775   Precision: 30.380%   Recall: 43.956%
Train   Epoch: 316 / 600   Loss: 1.361e+04   Precision: 65.470%   Recall: 61.344%
Valid                   Loss:    1478   Precision: 38.208%   Recall: 29.670%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.10252762 43.        ]
	 [67.21511841 36.        ]
	 [32.92583466 31.23333333]
	 [50.5190773  34.05      ]
	 [97.14451599 92.91666667]]
Train   Epoch: 317 / 600   Loss: 1.301e+04   Precision: 64.015%   Recall: 61.082%
Valid                   Loss:    3615   Precision: 23.149%   Recall: 72.161%
Train   Epoch: 318 / 600   Loss: 1.364e+04   Precision: 62.669%   Recall: 61.779%
Valid                   Loss:    1839   Precision: 33.333%   Recall: 43.956%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.53358078 72.56666667]
	 [74.91734314 94.33333333]
	 [40.17644882  8.55      ]
	 [37.95775223 24.33333333]
	 [29.82240677 51.78333333]]
Train   Epoch: 319 / 600   Loss: 1.31e+04   Precision: 65.013%   Recall: 60.031%
Valid                   Loss:    1476   Precision: 33.668%   Recall: 24.542%
Train   Epoch: 320 / 600   Loss: 1.488e+04   Precision: 65.312%   Recall: 59.542%
Valid                   Loss:    1532   Precision: 31.542%   Recall: 49.451%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 67.36820221  36.        ]
	 [112.5843811   14.        ]
	 [ 69.26181793  39.21666667]
	 [ 44.41840744  45.58333333]
	 [ 55.59959412  82.2       ]]
Train   Epoch: 321 / 600   Loss: 1.312e+04   Precision: 65.785%   Recall: 60.634%
Valid                   Loss:    2028   Precision: 30.874%   Recall: 41.392%
Train   Epoch: 322 / 600   Loss: 1.336e+04   Precision: 64.144%   Recall: 60.178%
Valid                   Loss:    1418   Precision: 40.084%   Recall: 34.799%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.47920609 35.        ]
	 [36.36690521 24.21666667]
	 [43.61463165 32.66666667]
	 [67.09712982 26.25      ]
	 [54.73816299 47.86666667]]
Train   Epoch: 323 / 600   Loss: 1.264e+04   Precision: 65.646%   Recall: 60.466%
Valid                   Loss:    1918   Precision: 31.739%   Recall: 26.740%
Train   Epoch: 324 / 600   Loss: 1.398e+04   Precision: 64.609%   Recall: 60.788%
Valid                   Loss:    1843   Precision: 29.019%   Recall: 50.916%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.84435272  35.88333333]
	 [ 83.41552734  72.5       ]
	 [ 90.81684875 438.96666667]
	 [ 48.09940338  22.        ]
	 [ 41.41111755  37.21666667]]
Train   Epoch: 325 / 600   Loss: 1.333e+04   Precision: 61.459%   Recall: 62.456%
Valid                   Loss:    1439   Precision: 41.176%   Recall: 25.641%
Train   Epoch: 326 / 600   Loss: 1.271e+04   Precision: 64.166%   Recall: 59.482%
Valid                   Loss:    1775   Precision: 35.094%   Recall: 34.066%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 78.18677521  22.13333333]
	 [ 46.55988312  43.16666667]
	 [ 33.34623337  22.83333333]
	 [ 49.46966171  14.25      ]
	 [130.34849548  17.11666667]]
Train   Epoch: 327 / 600   Loss: 1.426e+04   Precision: 65.282%   Recall: 60.835%
Valid                   Loss:    1367   Precision: 39.207%   Recall: 32.601%
Train   Epoch: 328 / 600   Loss: 1.226e+04   Precision: 65.789%   Recall: 61.933%
Valid                   Loss:    2141   Precision: 31.831%   Recall: 41.392%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.41361809 22.        ]
	 [33.15474319 38.91666667]
	 [50.84096909 21.05      ]
	 [38.03623199 69.93333333]
	 [33.96264648 41.5       ]]
Train   Epoch: 329 / 600   Loss: 1.259e+04   Precision: 66.910%   Recall: 59.515%
Valid                   Loss:    1788   Precision: 29.106%   Recall: 51.282%
Train   Epoch: 330 / 600   Loss: 1.372e+04   Precision: 64.525%   Recall: 61.612%
Valid                   Loss:    1277   Precision: 40.741%   Recall: 20.147%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.65802002 66.        ]
	 [64.99469757 27.25      ]
	 [61.61413956 57.21666667]
	 [35.98977661 29.23333333]
	 [37.67330933 21.        ]]
Train   Epoch: 331 / 600   Loss: 1.416e+04   Precision: 64.166%   Recall: 59.515%
Valid                   Loss:    1471   Precision: 38.843%   Recall: 34.432%
Train   Epoch: 332 / 600   Loss: 1.433e+04   Precision: 59.718%   Recall: 62.898%
Valid                   Loss:    1523   Precision: 35.540%   Recall: 37.363%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[122.43778992  85.83333333]
	 [ 36.41675186  37.8       ]
	 [ 50.72311783  56.33333333]
	 [533.10064697 862.31666667]
	 [ 47.38402557  18.13333333]]
Train   Epoch: 333 / 600   Loss: 1.344e+04   Precision: 64.239%   Recall: 61.846%
Valid                   Loss:    2033   Precision: 28.235%   Recall: 52.747%
Train   Epoch: 334 / 600   Loss: 1.286e+04   Precision: 66.870%   Recall: 60.962%
Valid                   Loss:    1681   Precision: 34.470%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[35.74994659 59.06666667]
	 [31.77923012 55.33333333]
	 [48.80951691 45.78333333]
	 [90.15353394 30.11666667]
	 [61.42863083 14.8       ]]
Train   Epoch: 335 / 600   Loss: 1.303e+04   Precision: 66.594%   Recall: 59.716%
Valid                   Loss:    1635   Precision: 32.843%   Recall: 24.542%
Train   Epoch: 336 / 600   Loss: 1.429e+04   Precision: 63.191%   Recall: 60.038%
Valid                   Loss:    2123   Precision: 28.097%   Recall: 46.520%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[236.23704529 261.2       ]
	 [ 76.16599274  28.71666667]
	 [175.8039856   75.58333333]
	 [ 36.44836426   7.2       ]
	 [ 35.99861908  37.83333333]]
Train   Epoch: 337 / 600   Loss: 1.421e+04   Precision: 66.696%   Recall: 60.513%
Valid                   Loss:    1608   Precision: 31.383%   Recall: 43.223%
Train   Epoch: 338 / 600   Loss: 1.253e+04   Precision: 66.895%   Recall: 58.812%
Valid                   Loss:    1606   Precision: 38.278%   Recall: 29.304%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.00823212  18.83333333]
	 [ 59.15942001  72.5       ]
	 [ 34.12919617  61.58333333]
	 [ 75.16950226  42.66666667]
	 [889.78503418 862.55      ]]
Train   Epoch: 339 / 600   Loss: 1.314e+04   Precision: 66.703%   Recall: 61.846%
Valid                   Loss:    1360   Precision: 38.919%   Recall: 26.374%
Train   Epoch: 340 / 600   Loss: 1.217e+04   Precision: 67.101%   Recall: 60.399%
Valid                   Loss:    1789   Precision: 30.163%   Recall: 40.659%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[753.43865967 672.2       ]
	 [ 94.16091919 108.46666667]
	 [ 31.435709    37.91666667]
	 [ 44.68257141  24.38333333]
	 [ 53.36042404  48.41666667]]
Train   Epoch: 341 / 600   Loss: 1.305e+04   Precision: 65.372%   Recall: 59.977%
Valid                   Loss:    1584   Precision: 35.821%   Recall: 35.165%
Train   Epoch: 342 / 600   Loss: 1.25e+04   Precision: 66.901%   Recall: 61.156%
Valid                   Loss:    1597   Precision: 33.762%   Recall: 38.462%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.90467072   8.93333333]
	 [117.22148132  13.2       ]
	 [ 46.49959564  38.03333333]
	 [ 36.89621735  40.31666667]
	 [ 36.25156403  62.2       ]]
Train   Epoch: 343 / 600   Loss: 1.331e+04   Precision: 68.434%   Recall: 60.774%
Valid                   Loss:    1591   Precision: 31.405%   Recall: 41.758%
Train   Epoch: 344 / 600   Loss: 1.223e+04   Precision: 67.078%   Recall: 61.960%
Valid                   Loss:    1612   Precision: 35.075%   Recall: 34.432%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[11.01809978 14.06666667]
	 [46.79013443 75.25      ]
	 [79.75260925 41.71666667]
	 [29.53692436 44.83333333]
	 [33.40982437 24.66666667]]
Train   Epoch: 345 / 600   Loss: 1.244e+04   Precision: 67.862%   Recall: 60.848%
Valid                   Loss:    1445   Precision: 33.811%   Recall: 43.223%
Train   Epoch: 346 / 600   Loss: 1.233e+04   Precision: 66.873%   Recall: 62.255%
Valid                   Loss:    1866   Precision: 37.778%   Recall: 24.908%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.36274338 37.11666667]
	 [49.94069672 19.75      ]
	 [39.61065674 51.5       ]
	 [28.76373482  8.25      ]
	 [56.78260803 39.41666667]]
Train   Epoch: 347 / 600   Loss: 1.345e+04   Precision: 65.714%   Recall: 61.149%
Valid                   Loss:    1543   Precision: 38.710%   Recall: 39.560%
Train   Epoch: 348 / 600   Loss: 1.279e+04   Precision: 67.320%   Recall: 62.395%
Valid                   Loss:    1460   Precision: 35.549%   Recall: 45.055%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 28.83730316  18.08333333]
	 [ 47.84185028  61.33333333]
	 [ 25.65797997  39.28333333]
	 [248.20584106  37.46666667]
	 [ 93.31876373  15.        ]]
Train   Epoch: 349 / 600   Loss: 1.212e+04   Precision: 68.731%   Recall: 62.074%
Valid                   Loss:    1542   Precision: 35.556%   Recall: 35.165%
Train   Epoch: 350 / 600   Loss: 1.197e+04   Precision: 67.669%   Recall: 61.042%
Valid                   Loss:    1837   Precision: 32.036%   Recall: 39.194%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.73671722  45.81666667]
	 [ 37.56791306  42.4       ]
	 [ 30.23869133  59.        ]
	 [151.72328186  15.94      ]
	 [ 66.11097717  24.28333333]]
Train   Epoch: 351 / 600   Loss: 1.262e+04   Precision: 67.087%   Recall: 62.422%
Valid                   Loss:    1629   Precision: 38.222%   Recall: 31.502%
Train   Epoch: 352 / 600   Loss: 1.286e+04   Precision: 65.095%   Recall: 60.761%
Valid                   Loss:    1643   Precision: 32.166%   Recall: 36.996%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.195755    23.88333333]
	 [ 76.33374023  32.4       ]
	 [ 98.64009857 135.88333333]
	 [ 34.53949356  26.25      ]
	 [ 45.62968063  13.38333333]]
Train   Epoch: 353 / 600   Loss: 1.366e+04   Precision: 63.376%   Recall: 61.109%
Valid                   Loss:    1708   Precision: 29.344%   Recall: 27.839%
Train   Epoch: 354 / 600   Loss: 1.33e+04   Precision: 67.298%   Recall: 61.190%
Valid                   Loss:    1617   Precision: 33.333%   Recall: 39.927%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[38.7089653  50.23333333]
	 [27.88846397 39.16666667]
	 [64.22079468 49.41666667]
	 [43.43434525 64.45      ]
	 [-4.14086151 28.51666667]]
Train   Epoch: 355 / 600   Loss: 1.223e+04   Precision: 68.143%   Recall: 61.926%
Valid                   Loss:    1533   Precision: 35.714%   Recall: 42.125%
Train   Epoch: 356 / 600   Loss: 1.073e+04   Precision: 68.535%   Recall: 63.246%
Valid                   Loss:    1442   Precision: 35.142%   Recall: 49.817%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.72895813 116.45      ]
	 [ 37.5139389   55.2       ]
	 [ 53.8139534   27.55      ]
	 [ 58.89398193  27.51666667]
	 [ 42.80437469  34.3       ]]
Train   Epoch: 357 / 600   Loss: 1.202e+04   Precision: 68.134%   Recall: 62.214%
Valid                   Loss:    1886   Precision: 29.323%   Recall: 42.857%
Train   Epoch: 358 / 600   Loss: 1.24e+04   Precision: 66.963%   Recall: 60.091%
Valid                   Loss:    1730   Precision: 42.105%   Recall: 26.374%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.68759537 46.        ]
	 [34.87159348 68.46666667]
	 [42.67801285 34.88333333]
	 [60.60374069 22.26666667]
	 [39.11635208 18.08333333]]
Train   Epoch: 359 / 600   Loss: 1.421e+04   Precision: 61.049%   Recall: 60.587%
Valid                   Loss:    1747   Precision: 34.467%   Recall: 55.678%
Train   Epoch: 360 / 600   Loss: 1.189e+04   Precision: 69.180%   Recall: 60.667%
Valid                   Loss:    1960   Precision: 33.808%   Recall: 34.799%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.39961243 30.        ]
	 [94.42276001 32.23333333]
	 [40.07314682 10.13333333]
	 [46.3965416  17.11666667]
	 [55.85176086 36.21666667]]
Train   Epoch: 361 / 600   Loss: 1.068e+04   Precision: 68.637%   Recall: 61.759%
Valid                   Loss:    1823   Precision: 39.493%   Recall: 39.927%
Train   Epoch: 362 / 600   Loss: 1.103e+04   Precision: 68.208%   Recall: 61.652%
Valid                   Loss:    1965   Precision: 35.174%   Recall: 44.322%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.64645386 36.23333333]
	 [34.28348541 22.33333333]
	 [32.13069534 34.76666667]
	 [42.93799591 38.73333333]
	 [51.01470184 21.        ]]
Train   Epoch: 363 / 600   Loss: 1.096e+04   Precision: 68.728%   Recall: 61.123%
Valid                   Loss:    2204   Precision: 33.796%   Recall: 53.480%
Train   Epoch: 364 / 600   Loss: 1.15e+04   Precision: 66.445%   Recall: 61.679%
Valid                   Loss:    1625   Precision: 40.146%   Recall: 40.293%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.86875916 65.43333333]
	 [38.61664581 14.33333333]
	 [60.91243362 36.55      ]
	 [51.47679138 44.33333333]
	 [43.19274902 22.06666667]]
Train   Epoch: 365 / 600   Loss: 1.146e+04   Precision: 68.092%   Recall: 62.596%
Valid                   Loss:    1534   Precision: 36.453%   Recall: 27.106%
Train   Epoch: 366 / 600   Loss: 1.249e+04   Precision: 67.637%   Recall: 62.127%
Valid                   Loss:    1822   Precision: 41.837%   Recall: 30.037%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.45166016 37.41666667]
	 [45.79673386 86.41666667]
	 [31.21866226 59.33333333]
	 [33.75135803 31.5       ]
	 [55.58366394 36.83333333]]
Train   Epoch: 367 / 600   Loss: 1.25e+04   Precision: 68.241%   Recall: 61.545%
Valid                   Loss:    1728   Precision: 30.316%   Recall: 52.747%
Train   Epoch: 368 / 600   Loss: 1.095e+04   Precision: 68.824%   Recall: 63.246%
Valid                   Loss:    1716   Precision: 35.329%   Recall: 43.223%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 58.73747635  39.33333333]
	 [ 65.09790039  27.55      ]
	 [ 53.67209625  45.73333333]
	 [102.08187866  93.95      ]
	 [ 59.8055191   54.78333333]]
Train   Epoch: 369 / 600   Loss: 1.142e+04   Precision: 69.302%   Recall: 63.286%
Valid                   Loss:    1739   Precision: 33.333%   Recall: 42.125%
Train   Epoch: 370 / 600   Loss: 1.129e+04   Precision: 68.275%   Recall: 62.650%
Valid                   Loss:    1775   Precision: 35.145%   Recall: 35.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.18443298  41.91666667]
	 [ 73.12326813  16.63333333]
	 [240.65115356 287.11666667]
	 [126.95333862 109.2       ]
	 [ 30.95614052  19.58333333]]
Train   Epoch: 371 / 600   Loss: 1.106e+04   Precision: 68.933%   Recall: 62.945%
Valid                   Loss:    1528   Precision: 41.102%   Recall: 35.531%
Train   Epoch: 372 / 600   Loss: 1.145e+04   Precision: 68.423%   Recall: 62.208%
Valid                   Loss:    1538   Precision: 32.558%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[129.39341736  67.        ]
	 [ 39.03777313  23.26666667]
	 [ 59.83934402  38.88333333]
	 [ 51.2341423   33.73333333]
	 [ 81.84725189  29.65      ]]
Train   Epoch: 373 / 600   Loss: 1.136e+04   Precision: 68.521%   Recall: 62.214%
Valid                   Loss:    1657   Precision: 32.192%   Recall: 51.648%
Train   Epoch: 374 / 600   Loss: 1.155e+04   Precision: 69.618%   Recall: 63.775%
Valid                   Loss:    1427   Precision: 43.056%   Recall: 22.711%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.57712173 26.46666667]
	 [32.23690033 41.08333333]
	 [28.48291588 71.36666667]
	 [50.13658905 16.56666667]
	 [30.9428978  22.25      ]]
Train   Epoch: 375 / 600   Loss: 1.115e+04   Precision: 68.133%   Recall: 60.493%
Valid                   Loss:    1475   Precision: 39.444%   Recall: 26.007%
Train   Epoch: 376 / 600   Loss: 1.097e+04   Precision: 69.454%   Recall: 63.025%
Valid                   Loss:    2245   Precision: 28.974%   Recall: 64.103%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.05049133 30.65      ]
	 [32.87959671 40.55      ]
	 [55.49065781 44.13333333]
	 [46.56350708 17.83333333]
	 [69.7234726  52.16666667]]
Train   Epoch: 377 / 600   Loss: 1.023e+04   Precision: 67.481%   Recall: 62.147%
Valid                   Loss:    1641   Precision: 38.288%   Recall: 31.136%
Train   Epoch: 378 / 600   Loss: 1.183e+04   Precision: 68.813%   Recall: 62.858%
Valid                   Loss:    1754   Precision: 36.864%   Recall: 31.868%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.28483582 24.88333333]
	 [26.9769001  23.73333333]
	 [14.61982059 16.75      ]
	 [33.02330399 47.16666667]
	 [22.83974266 41.45      ]]
Train   Epoch: 379 / 600   Loss: 1.083e+04   Precision: 68.834%   Recall: 61.397%
Valid                   Loss:    1942   Precision: 26.972%   Recall: 62.637%
Train   Epoch: 380 / 600   Loss: 1.096e+04   Precision: 68.187%   Recall: 62.482%
Valid                   Loss:    1552   Precision: 35.279%   Recall: 48.718%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.64892578 33.33333333]
	 [37.99123764  7.53333333]
	 [57.82537842 30.75      ]
	 [49.64143372 99.53333333]
	 [56.64736557 21.88333333]]
Train   Epoch: 381 / 600   Loss: 1.143e+04   Precision: 67.293%   Recall: 61.632%
Valid                   Loss:    2037   Precision: 34.925%   Recall: 42.857%
Train   Epoch: 382 / 600   Loss: 1.048e+04   Precision: 70.092%   Recall: 62.449%
Valid                   Loss:    1326   Precision: 36.885%   Recall: 32.967%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.18418884  55.05      ]
	 [150.88430786 206.66666667]
	 [ 41.0308075   34.01666667]
	 [ 40.03373718  13.2       ]
	 [262.90905762 389.66666667]]
Train   Epoch: 383 / 600   Loss:    9540   Precision: 69.835%   Recall: 63.333%
Valid                   Loss:    1474   Precision: 35.241%   Recall: 42.857%
Train   Epoch: 384 / 600   Loss: 1.214e+04   Precision: 70.344%   Recall: 61.679%
Valid                   Loss:    1567   Precision: 42.105%   Recall: 29.304%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 26.40098572  22.75      ]
	 [ 80.03502655  32.41666667]
	 [ 76.42163849 243.8       ]
	 [ 38.28629684  33.58333333]
	 [ 36.7118988   62.1       ]]
Train   Epoch: 385 / 600   Loss:    9631   Precision: 70.702%   Recall: 62.040%
Valid                   Loss:    1664   Precision: 37.333%   Recall: 51.282%
Train   Epoch: 386 / 600   Loss: 1.047e+04   Precision: 70.612%   Recall: 63.460%
Valid                   Loss:    1704   Precision: 40.210%   Recall: 42.125%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[24.79249382 30.33333333]
	 [15.5436554  18.5       ]
	 [36.74615097 37.5       ]
	 [32.38603592 25.5       ]
	 [66.6545639  40.3       ]]
Train   Epoch: 387 / 600   Loss:    9303   Precision: 69.891%   Recall: 61.277%
Valid                   Loss:    1626   Precision: 39.834%   Recall: 35.165%
Train   Epoch: 388 / 600   Loss: 1.027e+04   Precision: 69.703%   Recall: 62.429%
Valid                   Loss:    1694   Precision: 38.267%   Recall: 38.828%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.03297806 27.95      ]
	 [41.50319672 26.95      ]
	 [45.76314926 21.36666667]
	 [39.03139496 21.2       ]
	 [42.61843872 42.35      ]]
Train   Epoch: 389 / 600   Loss: 1.035e+04   Precision: 69.689%   Recall: 63.541%
Valid                   Loss:    1706   Precision: 40.526%   Recall: 28.205%
Train   Epoch: 390 / 600   Loss: 1.044e+04   Precision: 70.492%   Recall: 63.159%
Valid                   Loss:    1816   Precision: 37.500%   Recall: 26.374%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 25.31118774  43.25      ]
	 [ 72.75546265 106.53333333]
	 [ 54.25065613  21.66666667]
	 [ 34.77819443  44.53333333]
	 [ 18.77132797  39.75      ]]
Train   Epoch: 391 / 600   Loss: 1.072e+04   Precision: 70.653%   Recall: 62.489%
Valid                   Loss:    1934   Precision: 37.299%   Recall: 42.491%
Train   Epoch: 392 / 600   Loss: 1.104e+04   Precision: 69.650%   Recall: 62.395%
Valid                   Loss:    1561   Precision: 33.017%   Recall: 50.916%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 72.954216    29.28333333]
	 [ 52.74856949  57.75      ]
	 [ 76.38316345  26.13333333]
	 [ 46.03250122  20.75      ]
	 [110.80602264  78.        ]]
Train   Epoch: 393 / 600   Loss: 1.034e+04   Precision: 69.781%   Recall: 62.040%
Valid                   Loss:    1783   Precision: 39.766%   Recall: 24.908%
Train   Epoch: 394 / 600   Loss: 1.106e+04   Precision: 67.877%   Recall: 62.335%
Valid                   Loss:    1458   Precision: 42.857%   Recall: 37.363%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 43.55288315  26.73333333]
	 [ 41.37055206  45.03333333]
	 [108.11608124 124.06666667]
	 [ 40.98073196  51.75      ]
	 [ 31.09869385  22.56666667]]
Train   Epoch: 395 / 600   Loss: 1.058e+04   Precision: 69.364%   Recall: 60.466%
Valid                   Loss:    1549   Precision: 40.260%   Recall: 22.711%
Train   Epoch: 396 / 600   Loss: 1.114e+04   Precision: 69.558%   Recall: 63.166%
Valid                   Loss:    1647   Precision: 32.450%   Recall: 35.897%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 53.71091843  25.35      ]
	 [108.56917572  83.23333333]
	 [ 55.69589996  61.75      ]
	 [ 76.07993317  54.33333333]
	 [ 45.64319992  51.11666667]]
Train   Epoch: 397 / 600   Loss:    9765   Precision: 69.500%   Recall: 62.489%
Valid                   Loss:    1729   Precision: 41.748%   Recall: 31.502%
Train   Epoch: 398 / 600   Loss:    9638   Precision: 70.510%   Recall: 62.543%
Valid                   Loss:    1663   Precision: 40.678%   Recall: 26.374%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.74357224  14.96666667]
	 [109.50200653 137.41666667]
	 [ 47.30281067  79.1       ]
	 [ 32.04011154  22.83333333]
	 [ 55.17630386  50.23333333]]
Train   Epoch: 399 / 600   Loss: 1.019e+04   Precision: 69.932%   Recall: 63.219%
Valid                   Loss:    1978   Precision: 39.873%   Recall: 23.077%
Train   Epoch: 400 / 600   Loss: 1.093e+04   Precision: 67.818%   Recall: 62.717%
Valid                   Loss:    2062   Precision: 39.634%   Recall: 23.810%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[25.31496429 53.81666667]
	 [21.37501907 47.26666667]
	 [30.66735077 30.21666667]
	 [40.62062836 34.16666667]
	 [36.85050583 53.83333333]]
Train   Epoch: 401 / 600   Loss: 1.037e+04   Precision: 70.134%   Recall: 63.233%
Valid                   Loss:    1377   Precision: 42.029%   Recall: 31.868%
Train   Epoch: 402 / 600   Loss: 1.02e+04   Precision: 71.121%   Recall: 64.351%
Valid                   Loss:    2099   Precision: 36.996%   Recall: 36.996%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.01689148 100.33333333]
	 [ 78.37793732  49.31666667]
	 [211.55300903 142.66666667]
	 [ 70.10322571  22.73333333]
	 [ 37.51443863  44.5       ]]
Train   Epoch: 403 / 600   Loss: 1.037e+04   Precision: 69.673%   Recall: 62.402%
Valid                   Loss:    2088   Precision: 29.508%   Recall: 32.967%
Train   Epoch: 404 / 600   Loss:    9745   Precision: 70.633%   Recall: 63.025%
Valid                   Loss:    2124   Precision: 39.623%   Recall: 38.462%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  82.57173157   61.        ]
	 [  39.85800552   26.11666667]
	 [  39.5886879    27.25      ]
	 [ 625.97753906  478.26666667]
	 [1214.63452148 1421.86666667]]
Train   Epoch: 405 / 600   Loss: 1.044e+04   Precision: 69.723%   Recall: 62.998%
Valid                   Loss:    2533   Precision: 25.659%   Recall: 67.766%
Train   Epoch: 406 / 600   Loss: 1.003e+04   Precision: 67.712%   Recall: 63.789%
Valid                   Loss:    2488   Precision: 38.298%   Recall: 32.967%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[22.65846062 15.2       ]
	 [30.50519371 39.        ]
	 [47.03662491 27.5       ]
	 [38.84872055 52.66666667]
	 [29.59992218 23.66666667]]
Train   Epoch: 407 / 600   Loss: 1.203e+04   Precision: 64.741%   Recall: 61.632%
Valid                   Loss:    1364   Precision: 36.735%   Recall: 13.187%
Train   Epoch: 408 / 600   Loss: 1.2e+04   Precision: 64.322%   Recall: 63.507%
Valid                   Loss:    1840   Precision: 37.543%   Recall: 40.293%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.84533501  19.75      ]
	 [295.13009644 250.5       ]
	 [ 42.57093811  35.16666667]
	 [ 39.24536896  78.        ]
	 [ 84.9613266   47.11666667]]
Train   Epoch: 409 / 600   Loss: 1.064e+04   Precision: 71.044%   Recall: 61.317%
Valid                   Loss:    1691   Precision: 39.535%   Recall: 31.136%
Train   Epoch: 410 / 600   Loss:    9252   Precision: 71.508%   Recall: 62.891%
Valid                   Loss:    2042   Precision: 40.506%   Recall: 35.165%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.06803513 20.5       ]
	 [19.52664185 25.75      ]
	 [50.15834808 28.66666667]
	 [38.5879631  45.2       ]
	 [55.65748596 24.36666667]]
Train   Epoch: 411 / 600   Loss:    9306   Precision: 71.572%   Recall: 63.360%
Valid                   Loss:    1412   Precision: 41.176%   Recall: 30.769%
Train   Epoch: 412 / 600   Loss:    9299   Precision: 70.643%   Recall: 63.990%
Valid                   Loss:    1947   Precision: 42.326%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[73.2408905  43.38333333]
	 [38.56544113 35.58333333]
	 [53.15139008 25.98333333]
	 [61.72558212 61.41666667]
	 [94.59552765 72.56666667]]
Train   Epoch: 413 / 600   Loss:    9244   Precision: 71.662%   Recall: 63.487%
Valid                   Loss:    1719   Precision: 38.974%   Recall: 27.839%
Train   Epoch: 414 / 600   Loss:    9228   Precision: 71.145%   Recall: 63.882%
Valid                   Loss:    2577   Precision: 36.517%   Recall: 23.810%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.44203949  19.66666667]
	 [160.58888245 195.46666667]
	 [ 68.33296967  35.05      ]
	 [  9.28144741  19.11666667]
	 [103.99359894  78.11666667]]
Train   Epoch: 415 / 600   Loss: 1.183e+04   Precision: 64.461%   Recall: 61.926%
Valid                   Loss:    1794   Precision: 34.483%   Recall: 40.293%
Train   Epoch: 416 / 600   Loss:    9366   Precision: 71.184%   Recall: 62.563%
Valid                   Loss:    1666   Precision: 39.565%   Recall: 33.333%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 74.99556732  25.        ]
	 [ 54.08729553  23.        ]
	 [ 64.05588531  24.83333333]
	 [113.76133728 127.5       ]
	 [ 38.30300903  63.61666667]]
Train   Epoch: 417 / 600   Loss: 1.031e+04   Precision: 68.355%   Recall: 63.534%
Valid                   Loss:    1818   Precision: 34.194%   Recall: 38.828%
Train   Epoch: 418 / 600   Loss:    9342   Precision: 70.967%   Recall: 63.119%
Valid                   Loss:    1605   Precision: 38.650%   Recall: 23.077%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.5982933  43.78333333]
	 [25.56319809 17.01666667]
	 [60.51929474 44.66666667]
	 [49.18532181 19.55      ]
	 [29.01039505 30.28333333]]
Train   Epoch: 419 / 600   Loss:    9231   Precision: 71.072%   Recall: 62.965%
Valid                   Loss:    2059   Precision: 37.143%   Recall: 47.619%
Train   Epoch: 420 / 600   Loss:    9799   Precision: 71.398%   Recall: 64.425%
Valid                   Loss:    1536   Precision: 39.815%   Recall: 31.502%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.75256729  49.11666667]
	 [  3.16302967  18.56666667]
	 [ 34.25780106  75.61666667]
	 [ 49.11665344  51.73333333]
	 [ 67.41571045 104.86666667]]
Train   Epoch: 421 / 600   Loss:    9399   Precision: 72.620%   Recall: 63.815%
Valid                   Loss:    1472   Precision: 40.299%   Recall: 29.670%
Train   Epoch: 422 / 600   Loss:    9219   Precision: 72.248%   Recall: 64.485%
Valid                   Loss:    1665   Precision: 37.500%   Recall: 28.571%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.57948494  54.28333333]
	 [112.01299286 138.11666667]
	 [ 37.16176605  35.78333333]
	 [ 33.42566681  26.33333333]
	 [ 59.25430679  81.        ]]
Train   Epoch: 423 / 600   Loss:    9281   Precision: 71.762%   Recall: 64.090%
Valid                   Loss:    2162   Precision: 42.963%   Recall: 21.245%
Train   Epoch: 424 / 600   Loss:    8574   Precision: 70.389%   Recall: 61.257%
Valid                   Loss:    1338   Precision: 38.290%   Recall: 37.729%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.20874405 36.96666667]
	 [80.59806061 95.43333333]
	 [49.36436081 32.25      ]
	 [51.21289825 71.33333333]
	 [52.5776329  54.86666667]]
Train   Epoch: 425 / 600   Loss:    9573   Precision: 71.168%   Recall: 64.780%
Valid                   Loss:    1671   Precision: 38.710%   Recall: 30.769%
Train   Epoch: 426 / 600   Loss:    8317   Precision: 71.890%   Recall: 63.608%
Valid                   Loss:    1416   Precision: 49.045%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[63.11211395 66.03333333]
	 [76.14286804 47.33333333]
	 [49.45191193 34.16666667]
	 [ 0.81612939  6.66666667]
	 [46.07348251 25.28333333]]
Train   Epoch: 427 / 600   Loss:    9226   Precision: 71.728%   Recall: 62.777%
Valid                   Loss:    1384   Precision: 39.068%   Recall: 39.927%
Train   Epoch: 428 / 600   Loss:    9014   Precision: 72.084%   Recall: 64.291%
Valid                   Loss:    1614   Precision: 38.768%   Recall: 39.194%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.54330444 25.88333333]
	 [30.63451195 41.11666667]
	 [75.11399078 47.25      ]
	 [46.19787979 60.5       ]
	 [63.85265732 60.33333333]]
Train   Epoch: 429 / 600   Loss:    9233   Precision: 71.910%   Recall: 63.239%
Valid                   Loss:    1653   Precision: 38.397%   Recall: 33.333%
Train   Epoch: 430 / 600   Loss:    8960   Precision: 72.700%   Recall: 65.001%
Valid                   Loss:    1583   Precision: 39.683%   Recall: 36.630%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[140.33592224  86.16666667]
	 [ 31.52708817  17.21666667]
	 [ 57.77534103  30.08333333]
	 [ 38.5846138   29.76666667]
	 [ 46.9100647   19.66666667]]
Train   Epoch: 431 / 600   Loss:    9452   Precision: 71.616%   Recall: 64.713%
Valid                   Loss:    1837   Precision: 36.860%   Recall: 39.560%
Train   Epoch: 432 / 600   Loss:    8948   Precision: 73.034%   Recall: 63.313%
Valid                   Loss:    1652   Precision: 39.810%   Recall: 30.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[37.72179794 43.        ]
	 [89.39246368 38.61666667]
	 [44.84023666 28.63333333]
	 [38.78570175 46.96666667]
	 [58.4812851  33.73333333]]
Train   Epoch: 433 / 600   Loss:    8625   Precision: 73.090%   Recall: 63.949%
Valid                   Loss:    1542   Precision: 38.431%   Recall: 35.897%
Train   Epoch: 434 / 600   Loss:    8700   Precision: 73.163%   Recall: 63.146%
Valid                   Loss:    1647   Precision: 39.344%   Recall: 26.374%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.85102844 29.66666667]
	 [38.91108704 18.2       ]
	 [42.31533432 32.16666667]
	 [95.27605438 54.75      ]
	 [43.06575775 42.        ]]
Train   Epoch: 435 / 600   Loss:    9057   Precision: 70.481%   Recall: 63.460%
Valid                   Loss:    2008   Precision: 35.260%   Recall: 44.689%
Train   Epoch: 436 / 600   Loss:    9031   Precision: 72.000%   Recall: 63.748%
Valid                   Loss:    1651   Precision: 37.302%   Recall: 34.432%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.60173798  23.01666667]
	 [100.20694733  28.66666667]
	 [ 45.87300873  14.45      ]
	 [ 34.09854126  20.        ]
	 [ 55.83329773  44.16666667]]
Train   Epoch: 437 / 600   Loss: 1.023e+04   Precision: 66.305%   Recall: 64.599%
Valid                   Loss:    1435   Precision: 35.849%   Recall: 20.879%
Train   Epoch: 438 / 600   Loss: 1.035e+04   Precision: 66.402%   Recall: 61.531%
Valid                   Loss:    1657   Precision: 34.171%   Recall: 24.908%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 11.3873148   17.3       ]
	 [337.79748535 284.28333333]
	 [ 14.80993748  25.        ]
	 [ 42.81106949  29.25      ]
	 [ 49.04039383  29.        ]]
Train   Epoch: 439 / 600   Loss:    8333   Precision: 70.898%   Recall: 62.958%
Valid                   Loss:    1539   Precision: 37.143%   Recall: 28.571%
Train   Epoch: 440 / 600   Loss:    8241   Precision: 72.461%   Recall: 64.860%
Valid                   Loss:    1447   Precision: 41.117%   Recall: 29.670%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.27324677 35.75      ]
	 [40.24564743 55.71666667]
	 [38.31730652 37.46666667]
	 [32.92415237 10.4       ]
	 [56.17525482 21.08333333]]
Train   Epoch: 441 / 600   Loss:    8467   Precision: 73.320%   Recall: 64.445%
Valid                   Loss:    1372   Precision: 45.736%   Recall: 21.612%
Train   Epoch: 442 / 600   Loss:    9183   Precision: 73.389%   Recall: 63.306%
Valid                   Loss:    1365   Precision: 44.382%   Recall: 28.938%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.73613739 41.        ]
	 [39.25397491 31.91666667]
	 [80.84077454  9.76666667]
	 [60.553936   41.08333333]
	 [48.30894089 36.41666667]]
Train   Epoch: 443 / 600   Loss:    8723   Precision: 72.159%   Recall: 64.566%
Valid                   Loss:    2056   Precision: 37.342%   Recall: 21.612%
Train   Epoch: 444 / 600   Loss:    9409   Precision: 70.062%   Recall: 60.995%
Valid                   Loss:    1510   Precision: 36.937%   Recall: 30.037%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.72173691 75.61666667]
	 [78.98622131 44.        ]
	 [71.11234283 56.25      ]
	 [29.11909676 26.96666667]
	 [39.38176346 47.25      ]]
Train   Epoch: 445 / 600   Loss:    8878   Precision: 71.489%   Recall: 64.244%
Valid                   Loss:    1524   Precision: 43.379%   Recall: 34.799%
Train   Epoch: 446 / 600   Loss:    8302   Precision: 72.149%   Recall: 63.387%
Valid                   Loss:    1564   Precision: 44.970%   Recall: 27.839%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.31470871 34.75      ]
	 [36.88428497 31.06666667]
	 [37.71253967 16.96666667]
	 [37.36824036 28.        ]
	 [28.78292084 47.16666667]]
Train   Epoch: 447 / 600   Loss:    8673   Precision: 72.308%   Recall: 64.103%
Valid                   Loss:    1698   Precision: 40.805%   Recall: 26.007%
Train   Epoch: 448 / 600   Loss:    8620   Precision: 73.961%   Recall: 64.365%
Valid                   Loss:    1720   Precision: 38.813%   Recall: 31.136%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.00330353 29.65      ]
	 [64.42818451 78.13333333]
	 [39.50988007 44.63333333]
	 [23.62457466 40.05      ]
	 [38.89689255 28.45      ]]
Train   Epoch: 449 / 600   Loss:    7513   Precision: 74.316%   Recall: 64.793%
Valid                   Loss:    1657   Precision: 41.026%   Recall: 29.304%
Train   Epoch: 450 / 600   Loss:    8215   Precision: 73.578%   Recall: 65.343%
Valid                   Loss:    1712   Precision: 39.234%   Recall: 30.037%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.45410919  29.36666667]
	 [ 63.35643768  29.16666667]
	 [123.56287384  50.38333333]
	 [113.06565094  79.58333333]
	 [ 47.46444321  22.58333333]]
Train   Epoch: 451 / 600   Loss:    8094   Precision: 74.136%   Recall: 63.976%
Valid                   Loss:    1734   Precision: 41.518%   Recall: 34.066%
Train   Epoch: 452 / 600   Loss:    9176   Precision: 73.665%   Recall: 63.125%
Valid                   Loss:    1423   Precision: 46.104%   Recall: 26.007%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 20.84937668  33.25      ]
	 [151.14385986 188.75      ]
	 [ 21.42075157  18.08333333]
	 [ 33.8369751   32.88333333]
	 [ 56.13493729  27.88333333]]
Train   Epoch: 453 / 600   Loss:    8344   Precision: 73.266%   Recall: 64.472%
Valid                   Loss:    1390   Precision: 44.099%   Recall: 26.007%
Train   Epoch: 454 / 600   Loss:    8073   Precision: 73.847%   Recall: 64.894%
Valid                   Loss:    1762   Precision: 45.082%   Recall: 20.147%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[79.96974945 18.        ]
	 [35.61087036 37.71666667]
	 [27.58687973 38.06666667]
	 [72.2111969  38.13333333]
	 [29.1058979  44.91666667]]
Train   Epoch: 455 / 600   Loss:    7851   Precision: 73.296%   Recall: 64.036%
Valid                   Loss:    1611   Precision: 43.709%   Recall: 24.176%
Train   Epoch: 456 / 600   Loss:    8173   Precision: 73.449%   Recall: 63.929%
Valid                   Loss:    1536   Precision: 41.395%   Recall: 32.601%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.40608978 46.96666667]
	 [45.36385727 32.46666667]
	 [28.60816956 30.71666667]
	 [30.77344704 31.78333333]
	 [44.50034332 30.11666667]]
Train   Epoch: 457 / 600   Loss:    8640   Precision: 72.585%   Recall: 62.623%
Valid                   Loss:    1869   Precision: 42.308%   Recall: 32.234%
Train   Epoch: 458 / 600   Loss:    7730   Precision: 73.765%   Recall: 63.735%
Valid                   Loss:    1412   Precision: 39.773%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.38912964 23.8       ]
	 [43.58500671 33.4       ]
	 [41.11582184 42.        ]
	 [34.85015106 32.25      ]
	 [64.49349976 48.11666667]]
Train   Epoch: 459 / 600   Loss:    7587   Precision: 73.259%   Recall: 63.916%
Valid                   Loss:    1448   Precision: 40.891%   Recall: 36.996%
Train   Epoch: 460 / 600   Loss:    8547   Precision: 72.913%   Recall: 64.405%
Valid                   Loss:    1704   Precision: 42.913%   Recall: 39.927%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.94792938 56.78333333]
	 [44.42451859 34.05      ]
	 [45.68634415 29.41666667]
	 [14.36306572 12.16666667]
	 [33.19604492 44.25      ]]
Train   Epoch: 461 / 600   Loss:    7737   Precision: 74.421%   Recall: 64.800%
Valid                   Loss:    1520   Precision: 42.331%   Recall: 25.275%
Train   Epoch: 462 / 600   Loss:    8582   Precision: 74.772%   Recall: 63.688%
Valid                   Loss:    1730   Precision: 38.428%   Recall: 32.234%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[31.81154251 12.8       ]
	 [43.39912033 23.98333333]
	 [68.00271606 34.8       ]
	 [35.31475449 35.33333333]
	 [63.34745026 30.13333333]]
Train   Epoch: 463 / 600   Loss:    8156   Precision: 74.145%   Recall: 64.639%
Valid                   Loss:    2304   Precision: 42.424%   Recall: 30.769%
Train   Epoch: 464 / 600   Loss:    7405   Precision: 73.835%   Recall: 64.740%
Valid                   Loss:    1676   Precision: 43.023%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.57453156 28.41666667]
	 [24.4992981  29.        ]
	 [50.96206284 22.        ]
	 [46.66031265 27.        ]
	 [68.94610596 33.05      ]]
Train   Epoch: 465 / 600   Loss:    7520   Precision: 73.749%   Recall: 63.983%
Valid                   Loss:    2261   Precision: 42.667%   Recall: 35.165%
Train   Epoch: 466 / 600   Loss:    9037   Precision: 73.896%   Recall: 64.452%
Valid                   Loss:    2055   Precision: 45.361%   Recall: 16.117%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.62055206 84.8       ]
	 [30.80125427 50.95      ]
	 [73.43790436 46.78333333]
	 [29.23038673 22.88333333]
	 [41.33427048 24.        ]]
Train   Epoch: 467 / 600   Loss:    8982   Precision: 71.537%   Recall: 62.744%
Valid                   Loss:    1704   Precision: 37.500%   Recall: 46.154%
Train   Epoch: 468 / 600   Loss:    8981   Precision: 71.158%   Recall: 63.494%
Valid                   Loss:    1639   Precision: 45.251%   Recall: 29.670%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.74952698  84.78333333]
	 [189.06103516 215.68333333]
	 [ 40.4866066   27.36666667]
	 [ 85.19300842  22.25      ]
	 [ 92.06680298  64.75      ]]
Train   Epoch: 469 / 600   Loss:    7777   Precision: 73.712%   Recall: 63.614%
Valid                   Loss:    1631   Precision: 39.526%   Recall: 36.630%
Train   Epoch: 470 / 600   Loss:    7706   Precision: 74.341%   Recall: 64.217%
Valid                   Loss:    1454   Precision: 40.306%   Recall: 28.938%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.54608917 34.7       ]
	 [46.23041916 45.66666667]
	 [28.3966713  18.43333333]
	 [28.08501244 30.48333333]
	 [29.42635727 21.43333333]]
Train   Epoch: 471 / 600   Loss:    8195   Precision: 73.831%   Recall: 64.010%
Valid                   Loss:    1756   Precision: 40.278%   Recall: 31.868%
Train   Epoch: 472 / 600   Loss:    7711   Precision: 74.623%   Recall: 64.271%
Valid                   Loss:    1505   Precision: 37.019%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.49535751 45.71666667]
	 [18.89993858 12.01666667]
	 [33.18140411 31.13333333]
	 [34.14023209 63.66666667]
	 [26.62946129 16.43333333]]
Train   Epoch: 473 / 600   Loss:    7650   Precision: 74.602%   Recall: 64.378%
Valid                   Loss:    1678   Precision: 44.444%   Recall: 14.652%
Train   Epoch: 474 / 600   Loss:    7918   Precision: 74.296%   Recall: 64.512%
Valid                   Loss:    1834   Precision: 47.857%   Recall: 24.542%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[20.29227066 40.66666667]
	 [77.88317108 81.8       ]
	 [36.30567932 97.28333333]
	 [30.33663559 40.61666667]
	 [37.63700485 27.7       ]]
Train   Epoch: 475 / 600   Loss:    7439   Precision: 73.747%   Recall: 63.882%
Valid                   Loss:    1849   Precision: 44.571%   Recall: 28.571%
Train   Epoch: 476 / 600   Loss:    7758   Precision: 74.159%   Recall: 63.976%
Valid                   Loss:    1718   Precision: 35.798%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.22578812 51.4       ]
	 [56.1812973  40.7       ]
	 [79.60604095 63.5       ]
	 [49.51647949 33.55      ]
	 [77.110466   14.66666667]]
Train   Epoch: 477 / 600   Loss:    8937   Precision: 73.397%   Recall: 63.554%
Valid                   Loss:    1878   Precision: 42.742%   Recall: 19.414%
Train   Epoch: 478 / 600   Loss:    8734   Precision: 75.008%   Recall: 63.085%
Valid                   Loss:    1759   Precision: 44.397%   Recall: 37.729%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 40.59658432  56.78333333]
	 [ 70.99263763  48.16666667]
	 [ 37.90648651  34.26666667]
	 [113.13361359 145.43333333]
	 [ 57.37573242  56.16666667]]
Train   Epoch: 479 / 600   Loss:    7573   Precision: 73.955%   Recall: 63.983%
Valid                   Loss:    1989   Precision: 38.158%   Recall: 31.868%
Train   Epoch: 480 / 600   Loss:    7608   Precision: 75.611%   Recall: 65.229%
Valid                   Loss:    1748   Precision: 43.564%   Recall: 32.234%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.68325424  29.        ]
	 [137.38868713  49.9       ]
	 [ 28.5910778   36.66666667]
	 [ 32.7537117   62.25      ]
	 [ 29.95621109 106.41666667]]
Train   Epoch: 481 / 600   Loss:    7380   Precision: 75.828%   Recall: 63.648%
Valid                   Loss:    1662   Precision: 47.826%   Recall: 20.147%
Train   Epoch: 482 / 600   Loss:    7440   Precision: 75.045%   Recall: 64.398%
Valid                   Loss:    1901   Precision: 48.544%   Recall: 36.630%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.91346741  34.08333333]
	 [142.33854675 178.        ]
	 [ 35.65304947  30.58333333]
	 [ 46.66098785  76.2       ]
	 [ 46.74158478  39.5       ]]
Train   Epoch: 483 / 600   Loss:    6937   Precision: 74.258%   Recall: 65.001%
Valid                   Loss:    1535   Precision: 46.907%   Recall: 33.333%
Train   Epoch: 484 / 600   Loss:    7225   Precision: 73.921%   Recall: 64.271%
Valid                   Loss:    1508   Precision: 36.715%   Recall: 55.678%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[215.0160675   43.18333333]
	 [ 90.60848999  58.33333333]
	 [ 47.09709167  22.66666667]
	 [ 72.52103424  45.53333333]
	 [ 46.75673294  66.2       ]]
Train   Epoch: 485 / 600   Loss:    8893   Precision: 69.496%   Recall: 63.514%
Valid                   Loss:    2158   Precision: 35.181%   Recall: 53.480%
Train   Epoch: 486 / 600   Loss:    9478   Precision: 70.137%   Recall: 63.635%
Valid                   Loss:    1421   Precision: 42.798%   Recall: 38.095%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.48762321  28.71666667]
	 [ 38.75569153  58.21666667]
	 [ 51.11930847  71.26666667]
	 [260.67337036 229.88333333]
	 [ 59.58781815  27.63333333]]
Train   Epoch: 487 / 600   Loss:    7935   Precision: 74.173%   Recall: 63.655%
Valid                   Loss:    2127   Precision: 37.209%   Recall: 35.165%
Train   Epoch: 488 / 600   Loss:    7756   Precision: 74.717%   Recall: 65.343%
Valid                   Loss:    1617   Precision: 47.581%   Recall: 21.612%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.66009903 28.01666667]
	 [19.90272903 42.88333333]
	 [30.1386795  26.83333333]
	 [ 9.53118515 11.85      ]
	 [35.87084961 36.26666667]]
Train   Epoch: 489 / 600   Loss:    7735   Precision: 75.069%   Recall: 63.514%
Valid                   Loss:    1955   Precision: 41.718%   Recall: 24.908%
Train   Epoch: 490 / 600   Loss:    7748   Precision: 75.613%   Recall: 64.901%
Valid                   Loss:    1712   Precision: 43.023%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.90264893 32.55      ]
	 [42.36729431 20.6       ]
	 [72.24887848 64.4       ]
	 [28.84843826 29.86666667]
	 [36.77056885 33.78333333]]
Train   Epoch: 491 / 600   Loss:    6862   Precision: 76.068%   Recall: 64.298%
Valid                   Loss:    1653   Precision: 42.604%   Recall: 26.374%
Train   Epoch: 492 / 600   Loss:    7397   Precision: 75.323%   Recall: 65.570%
Valid                   Loss:    1770   Precision: 44.056%   Recall: 23.077%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.47428131 23.16666667]
	 [33.19596481 70.58333333]
	 [36.75796127 29.        ]
	 [38.45900345 54.25      ]
	 [64.19026184 19.83333333]]
Train   Epoch: 493 / 600   Loss:    7438   Precision: 74.182%   Recall: 64.050%
Valid                   Loss:    1889   Precision: 39.919%   Recall: 36.264%
Train   Epoch: 494 / 600   Loss:    8104   Precision: 71.237%   Recall: 64.485%
Valid                   Loss:    1985   Precision: 36.842%   Recall: 41.026%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[54.6006813  36.25      ]
	 [59.82061768 33.53333333]
	 [65.52970886 21.33333333]
	 [64.93365479 40.11666667]
	 [69.72106171 43.33333333]]
Train   Epoch: 495 / 600   Loss:    8010   Precision: 73.339%   Recall: 63.809%
Valid                   Loss:    1884   Precision: 49.650%   Recall: 26.007%
Train   Epoch: 496 / 600   Loss:    7217   Precision: 73.938%   Recall: 65.390%
Valid                   Loss:    2403   Precision: 47.407%   Recall: 23.443%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.51638412  29.66666667]
	 [ 43.41178131  56.33333333]
	 [ 49.6440773  155.        ]
	 [ 19.4347744   13.88333333]
	 [ 33.36405563  23.9       ]]
Train   Epoch: 497 / 600   Loss:    7702   Precision: 74.634%   Recall: 63.775%
Valid                   Loss:    1511   Precision: 53.435%   Recall: 25.641%
Train   Epoch: 498 / 600   Loss:    7127   Precision: 75.692%   Recall: 65.724%
Valid                   Loss:    1883   Precision: 45.890%   Recall: 24.542%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[21.83111763 24.5       ]
	 [26.41499329 97.21666667]
	 [34.66081238 38.63333333]
	 [20.36572266 28.63333333]
	 [24.69703865 39.21666667]]
Train   Epoch: 499 / 600   Loss:    6541   Precision: 77.078%   Recall: 64.532%
Valid                   Loss:    1621   Precision: 46.354%   Recall: 32.601%
Train   Epoch: 500 / 600   Loss:    6540   Precision: 76.429%   Recall: 65.008%
Valid                   Loss:    1661   Precision: 46.154%   Recall: 30.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.05801392 73.5       ]
	 [47.48028183 22.61666667]
	 [14.66229439 12.83333333]
	 [48.11079407 29.88333333]
	 [30.09645271 30.68333333]]
Train   Epoch: 501 / 600   Loss:    6376   Precision: 75.801%   Recall: 64.458%
Valid                   Loss:    1842   Precision: 45.217%   Recall: 19.048%
Train   Epoch: 502 / 600   Loss:    6886   Precision: 76.662%   Recall: 65.483%
Valid                   Loss:    2034   Precision: 39.462%   Recall: 32.234%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.24759674 22.16666667]
	 [51.69989395 17.33333333]
	 [-8.86410046 16.76666667]
	 [41.92783737 23.45      ]
	 [45.53505325 76.33333333]]
Train   Epoch: 503 / 600   Loss:    6901   Precision: 75.744%   Recall: 64.968%
Valid                   Loss:    1715   Precision: 48.062%   Recall: 22.711%
Train   Epoch: 504 / 600   Loss:    6991   Precision: 75.849%   Recall: 64.813%
Valid                   Loss:    1776   Precision: 39.004%   Recall: 34.432%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.1003952  12.05      ]
	 [51.8420105  90.83333333]
	 [46.40576172 31.5       ]
	 [38.62679291 33.5       ]
	 [45.13631821 31.58333333]]
Train   Epoch: 505 / 600   Loss:    8671   Precision: 72.432%   Recall: 63.990%
Valid                   Loss:    1629   Precision: 43.820%   Recall: 28.571%
Train   Epoch: 506 / 600   Loss:    7791   Precision: 75.280%   Recall: 65.235%
Valid                   Loss:    1443   Precision: 47.742%   Recall: 27.106%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.63661194 111.05      ]
	 [ 24.22686195  23.5       ]
	 [ 24.51806641  22.65      ]
	 [ 21.4148159   21.16666667]
	 [ 62.32265472  22.75      ]]
Train   Epoch: 507 / 600   Loss:    6680   Precision: 75.678%   Recall: 64.278%
Valid                   Loss:    1535   Precision: 45.139%   Recall: 23.810%
Train   Epoch: 508 / 600   Loss:    6229   Precision: 75.517%   Recall: 64.854%
Valid                   Loss:    2050   Precision: 42.262%   Recall: 26.007%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.20895386 24.5       ]
	 [35.50435638 17.33333333]
	 [51.02436066 38.25      ]
	 [19.49571991 27.25      ]
	 [54.27727509 38.        ]]
Train   Epoch: 509 / 600   Loss:    6284   Precision: 76.775%   Recall: 65.101%
Valid                   Loss:    1820   Precision: 46.341%   Recall: 20.879%
Train   Epoch: 510 / 600   Loss:    6459   Precision: 78.238%   Recall: 65.986%
Valid                   Loss:    1788   Precision: 41.085%   Recall: 19.414%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 143.88337708  138.36666667]
	 [  26.56947708   46.66666667]
	 [ 227.66995239  279.75      ]
	 [1223.56738281 1247.93333333]
	 [  27.55997467   21.08333333]]
Train   Epoch: 511 / 600   Loss:    8288   Precision: 74.324%   Recall: 64.800%
Valid                   Loss:    1502   Precision: 45.578%   Recall: 24.542%
Train   Epoch: 512 / 600   Loss:    6656   Precision: 76.795%   Recall: 64.994%
Valid                   Loss:    1665   Precision: 43.333%   Recall: 23.810%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[255.65393066 193.75      ]
	 [ 17.74297714  13.71666667]
	 [ 17.9551506   10.        ]
	 [ 58.85887909  52.58333333]
	 [ 66.86797333  44.66666667]]
Train   Epoch: 513 / 600   Loss:    6824   Precision: 77.252%   Recall: 64.673%
Valid                   Loss:    1643   Precision: 48.515%   Recall: 35.897%
Train   Epoch: 514 / 600   Loss:    6002   Precision: 77.444%   Recall: 65.684%
Valid                   Loss:    1671   Precision: 45.763%   Recall: 29.670%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.14204025 67.36666667]
	 [44.68397903 25.        ]
	 [54.6067543  94.78333333]
	 [70.34358215 37.46666667]
	 [41.02873993 30.61666667]]
Train   Epoch: 515 / 600   Loss:    6113   Precision: 76.329%   Recall: 65.209%
Valid                   Loss:    1719   Precision: 44.882%   Recall: 20.879%
Train   Epoch: 516 / 600   Loss:    6770   Precision: 76.726%   Recall: 66.488%
Valid                   Loss:    1571   Precision: 43.243%   Recall: 23.443%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.39636612 30.21666667]
	 [74.45292664 35.98333333]
	 [29.16401291 23.11666667]
	 [31.86959076 30.        ]
	 [41.33608246 37.        ]]
Train   Epoch: 517 / 600   Loss:    6317   Precision: 76.430%   Recall: 64.619%
Valid                   Loss:    1775   Precision: 45.876%   Recall: 32.601%
Train   Epoch: 518 / 600   Loss:    6767   Precision: 75.690%   Recall: 66.173%
Valid                   Loss:    1764   Precision: 44.444%   Recall: 21.978%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[30.56634521 30.78333333]
	 [38.16139984 46.        ]
	 [17.96240425 32.16666667]
	 [35.03243637 28.55      ]
	 [26.5237751  22.66666667]]
Train   Epoch: 519 / 600   Loss:    6473   Precision: 76.835%   Recall: 66.629%
Valid                   Loss:    1660   Precision: 43.357%   Recall: 22.711%
Train   Epoch: 520 / 600   Loss:    6014   Precision: 76.579%   Recall: 64.807%
Valid                   Loss:    1476   Precision: 49.587%   Recall: 21.978%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.41943741 22.        ]
	 [35.4847641  32.        ]
	 [29.79289246 52.08333333]
	 [46.36173248 28.78333333]
	 [89.22714996 36.96666667]]
Train   Epoch: 521 / 600   Loss:    6116   Precision: 77.539%   Recall: 65.510%
Valid                   Loss:    1639   Precision: 44.324%   Recall: 30.037%
Train   Epoch: 522 / 600   Loss:    6401   Precision: 77.518%   Recall: 65.363%
Valid                   Loss:    1631   Precision: 46.875%   Recall: 21.978%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 41.92467499  29.16666667]
	 [106.09503937  87.38333333]
	 [ 32.13580704  28.9       ]
	 [ 40.13431549  22.68333333]
	 [ 61.30114365  49.21666667]]
Train   Epoch: 523 / 600   Loss:    6594   Precision: 75.523%   Recall: 66.548%
Valid                   Loss:    1663   Precision: 41.135%   Recall: 21.245%
Train   Epoch: 524 / 600   Loss:    6427   Precision: 76.509%   Recall: 63.507%
Valid                   Loss:    1567   Precision: 50.000%   Recall: 33.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 38.62841797  28.46666667]
	 [ 24.67517662  36.28333333]
	 [ 26.66623497  39.5       ]
	 [135.3021698  102.75      ]
	 [ 61.95210648  65.95      ]]
Train   Epoch: 525 / 600   Loss:    6282   Precision: 77.479%   Recall: 66.254%
Valid                   Loss:    1500   Precision: 52.174%   Recall: 17.582%
Train   Epoch: 526 / 600   Loss:    7417   Precision: 76.915%   Recall: 65.034%
Valid                   Loss:    1741   Precision: 41.150%   Recall: 34.066%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[-12.67640495   7.28333333]
	 [ 13.5401144   18.11666667]
	 [ 41.49388123  23.55      ]
	 [ 30.38419151  21.66666667]
	 [ 52.35235977  61.66666667]]
Train   Epoch: 527 / 600   Loss:    6757   Precision: 75.465%   Recall: 64.981%
Valid                   Loss:    2170   Precision: 38.614%   Recall: 14.286%
Train   Epoch: 528 / 600   Loss:    7025   Precision: 74.097%   Recall: 66.950%
Valid                   Loss:    1715   Precision: 45.865%   Recall: 22.344%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[154.48043823 145.75      ]
	 [ 26.623909    32.21666667]
	 [ 23.51589775  21.66666667]
	 [ 21.21142578  10.        ]
	 [ 25.36982155  36.28333333]]
Train   Epoch: 529 / 600   Loss:    6238   Precision: 76.877%   Recall: 64.760%
Valid                   Loss:    1532   Precision: 50.993%   Recall: 28.205%
Train   Epoch: 530 / 600   Loss:    6612   Precision: 76.924%   Recall: 65.423%
Valid                   Loss:    1723   Precision: 50.000%   Recall: 19.048%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[24.75997162 64.08333333]
	 [26.81259346 42.66666667]
	 [16.9654789  24.46666667]
	 [33.9717865  26.33333333]
	 [22.28371811 19.55      ]]
Train   Epoch: 531 / 600   Loss:    6129   Precision: 77.254%   Recall: 65.497%
Valid                   Loss:    1767   Precision: 37.714%   Recall: 24.176%
Train   Epoch: 532 / 600   Loss:    6803   Precision: 76.171%   Recall: 66.421%
Valid                   Loss:    1588   Precision: 51.613%   Recall: 11.722%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 36.12003708  18.78333333]
	 [  9.76472378  29.        ]
	 [ 26.7860527   23.73333333]
	 [ 39.380867    33.28333333]
	 [ 80.00817108 181.78333333]]
Train   Epoch: 533 / 600   Loss:    6684   Precision: 76.323%   Recall: 65.403%
Valid                   Loss:    1582   Precision: 44.937%   Recall: 26.007%
Train   Epoch: 534 / 600   Loss:    6212   Precision: 77.795%   Recall: 65.899%
Valid                   Loss:    1650   Precision: 46.053%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[53.03543091 46.21666667]
	 [16.0929718  25.78333333]
	 [59.02907562 18.66666667]
	 [35.10786438 30.88333333]
	 [68.07765961 49.7       ]]
Train   Epoch: 535 / 600   Loss:    6431   Precision: 76.824%   Recall: 65.256%
Valid                   Loss:    1934   Precision: 39.085%   Recall: 40.659%
Train   Epoch: 536 / 600   Loss:    7010   Precision: 77.010%   Recall: 65.135%
Valid                   Loss:    1892   Precision: 41.841%   Recall: 36.630%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[65.17279053 28.33333333]
	 [56.06615067 48.8       ]
	 [39.76610184 23.86666667]
	 [57.87880325 34.8       ]
	 [28.2714901  40.28333333]]
Train   Epoch: 537 / 600   Loss:    7474   Precision: 75.629%   Recall: 66.247%
Valid                   Loss:    2115   Precision: 46.715%   Recall: 23.443%
Train   Epoch: 538 / 600   Loss:    6710   Precision: 75.667%   Recall: 65.570%
Valid                   Loss:    2053   Precision: 34.317%   Recall: 34.066%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[26.51244164 26.33333333]
	 [25.32664871 39.53333333]
	 [20.30413628 18.41666667]
	 [30.15810966 13.53333333]
	 [34.30449677 17.45      ]]
Train   Epoch: 539 / 600   Loss:    6493   Precision: 74.959%   Recall: 64.184%
Valid                   Loss:    1714   Precision: 37.333%   Recall: 30.769%
Train   Epoch: 540 / 600   Loss:    6226   Precision: 75.440%   Recall: 65.222%
Valid                   Loss:    1912   Precision: 45.324%   Recall: 23.077%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.17076492  31.83333333]
	 [ 28.7215271   32.68333333]
	 [ 98.72333527  74.        ]
	 [ 58.83325958 133.71666667]
	 [ 39.22161102  52.7       ]]
Train   Epoch: 541 / 600   Loss:    6372   Precision: 77.764%   Recall: 65.289%
Valid                   Loss:    1802   Precision: 40.625%   Recall: 14.286%
Train   Epoch: 542 / 600   Loss:    6346   Precision: 78.491%   Recall: 65.242%
Valid                   Loss:    1687   Precision: 38.816%   Recall: 21.612%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[16.33273697 20.93333333]
	 [57.19202423 75.46666667]
	 [35.43621063 23.61666667]
	 [45.3183403  79.41666667]
	 [57.53237152 36.63333333]]
Train   Epoch: 543 / 600   Loss:    6691   Precision: 77.277%   Recall: 65.878%
Valid                   Loss:    1538   Precision: 37.297%   Recall: 25.275%
Train   Epoch: 544 / 600   Loss:    6206   Precision: 78.158%   Recall: 65.771%
Valid                   Loss:    1559   Precision: 45.752%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 37.72620392  77.98333333]
	 [ 15.90962601  23.21666667]
	 [165.78613281  42.91666667]
	 [ 30.30138206  19.26666667]
	 [ 19.15257835  39.28333333]]
Train   Epoch: 545 / 600   Loss:    5605   Precision: 77.895%   Recall: 65.691%
Valid                   Loss:    1625   Precision: 44.025%   Recall: 25.641%
Train   Epoch: 546 / 600   Loss:    5777   Precision: 76.999%   Recall: 66.126%
Valid                   Loss:    1597   Precision: 45.517%   Recall: 24.176%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.06073761 33.55      ]
	 [32.80319214 52.75      ]
	 [24.68317413 52.66666667]
	 [28.23475075 39.46666667]
	 [59.34548569 51.75      ]]
Train   Epoch: 547 / 600   Loss:    6343   Precision: 75.927%   Recall: 64.901%
Valid                   Loss:    1729   Precision: 49.254%   Recall: 24.176%
Train   Epoch: 548 / 600   Loss:    6084   Precision: 76.337%   Recall: 64.525%
Valid                   Loss:    1616   Precision: 42.424%   Recall: 20.513%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 16.84601402  22.66666667]
	 [170.66720581  88.05      ]
	 [ 30.50013161  54.91666667]
	 [ 10.66461182  22.51666667]
	 [ 36.50783157  22.96666667]]
Train   Epoch: 549 / 600   Loss:    6588   Precision: 75.940%   Recall: 65.182%
Valid                   Loss:    1878   Precision: 36.994%   Recall: 23.443%
Train   Epoch: 550 / 600   Loss:    6025   Precision: 77.532%   Recall: 65.644%
Valid                   Loss:    2011   Precision: 42.703%   Recall: 28.938%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.53747559 30.11666667]
	 [59.07091522 44.11666667]
	 [48.94889832 36.13333333]
	 [26.07217026 29.        ]
	 [84.23291016 44.5       ]]
Train   Epoch: 551 / 600   Loss:    5761   Precision: 77.766%   Recall: 67.004%
Valid                   Loss:    1898   Precision: 51.754%   Recall: 21.612%
Train   Epoch: 552 / 600   Loss:    5915   Precision: 78.371%   Recall: 65.410%
Valid                   Loss:    1600   Precision: 45.890%   Recall: 24.542%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.88983917 29.7       ]
	 [11.13042736 31.96666667]
	 [27.27231789 29.66666667]
	 [40.45265961 49.88333333]
	 [60.88504028 26.6       ]]
Train   Epoch: 553 / 600   Loss:    5982   Precision: 77.889%   Recall: 66.187%
Valid                   Loss:    1629   Precision: 39.683%   Recall: 27.473%
Train   Epoch: 554 / 600   Loss:    5541   Precision: 78.716%   Recall: 65.349%
Valid                   Loss:    1623   Precision: 39.352%   Recall: 31.136%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 30.74504471  23.51666667]
	 [ 43.61958313  24.66666667]
	 [165.73568726 177.33333333]
	 [ 41.62106323  31.78333333]
	 [ 64.17727661  65.11666667]]
Train   Epoch: 555 / 600   Loss:    5540   Precision: 78.236%   Recall: 66.434%
Valid                   Loss:    1746   Precision: 45.455%   Recall: 20.147%
Train   Epoch: 556 / 600   Loss:    5453   Precision: 79.244%   Recall: 67.258%
Valid                   Loss:    1393   Precision: 46.218%   Recall: 20.147%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.22189331 37.03333333]
	 [31.10426331 44.06666667]
	 [34.83639526 54.38333333]
	 [31.71344757 36.5       ]
	 [37.61340714 47.46666667]]
Train   Epoch: 557 / 600   Loss:    5439   Precision: 80.041%   Recall: 66.133%
Valid                   Loss:    1740   Precision: 43.284%   Recall: 21.245%
Train   Epoch: 558 / 600   Loss:    6213   Precision: 78.460%   Recall: 66.294%
Valid                   Loss:    2052   Precision: 43.229%   Recall: 30.403%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.46358871 38.66666667]
	 [89.18524933 81.58333333]
	 [39.87887955 50.15      ]
	 [53.72002411 77.53333333]
	 [50.50088882 25.75      ]]
Train   Epoch: 559 / 600   Loss:    5762   Precision: 78.058%   Recall: 65.959%
Valid                   Loss:    1598   Precision: 44.667%   Recall: 24.542%
Train   Epoch: 560 / 600   Loss:    4869   Precision: 79.253%   Recall: 66.501%
Valid                   Loss:    1531   Precision: 43.713%   Recall: 26.740%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.29629135 35.88333333]
	 [64.6578598  63.48333333]
	 [36.01752472 48.55      ]
	 [33.47722626 41.66666667]
	 [38.8459549  47.6       ]]
Train   Epoch: 561 / 600   Loss:    6003   Precision: 79.105%   Recall: 66.314%
Valid                   Loss:    1623   Precision: 41.463%   Recall: 24.908%
Train   Epoch: 562 / 600   Loss:    6190   Precision: 79.725%   Recall: 67.218%
Valid                   Loss:    1719   Precision: 45.070%   Recall: 23.443%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 84.54917145 104.16666667]
	 [104.69302368  43.86666667]
	 [ 56.32062531  98.66666667]
	 [ 33.84664536  25.66666667]
	 [ 88.6991806  124.7       ]]
Train   Epoch: 563 / 600   Loss:    5610   Precision: 79.386%   Recall: 66.890%
Valid                   Loss:    2274   Precision: 40.940%   Recall: 22.344%
Train   Epoch: 564 / 600   Loss:    5819   Precision: 78.326%   Recall: 67.198%
Valid                   Loss:    1830   Precision: 41.250%   Recall: 24.176%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.74261093  81.38333333]
	 [ 35.02472687  26.15      ]
	 [ 34.7947464   40.41666667]
	 [ 45.31593704  17.75      ]
	 [ 66.26889038 160.45      ]]
Train   Epoch: 565 / 600   Loss:    5786   Precision: 79.408%   Recall: 66.126%
Valid                   Loss:    1609   Precision: 43.885%   Recall: 22.344%
Train   Epoch: 566 / 600   Loss:    5603   Precision: 77.898%   Recall: 67.754%
Valid                   Loss:    2381   Precision: 29.755%   Recall: 57.875%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.55150604  34.88333333]
	 [ 79.07411957  49.4       ]
	 [ 27.64400673  10.78333333]
	 [133.48858643 277.6       ]
	 [ 79.44522095  62.25      ]]
Train   Epoch: 567 / 600   Loss:    8323   Precision: 69.789%   Recall: 63.474%
Valid                   Loss:    1947   Precision: 35.547%   Recall: 33.333%
Train   Epoch: 568 / 600   Loss:    6258   Precision: 73.345%   Recall: 64.954%
Valid                   Loss:    1461   Precision: 40.284%   Recall: 31.136%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.84919739  46.38333333]
	 [ 70.34104156 119.5       ]
	 [ 43.94005203  22.5       ]
	 [ 33.92835617  34.16666667]
	 [ 44.33981705   8.5       ]]
Train   Epoch: 569 / 600   Loss:    6082   Precision: 78.077%   Recall: 65.008%
Valid                   Loss:    1678   Precision: 41.212%   Recall: 24.908%
Train   Epoch: 570 / 600   Loss:    5239   Precision: 80.249%   Recall: 66.843%
Valid                   Loss:    1427   Precision: 41.784%   Recall: 32.601%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 34.63838196  30.        ]
	 [ 47.8598671   44.78333333]
	 [523.09588623 494.4       ]
	 [ 31.04524231  29.86666667]
	 [ 35.38941956  23.41666667]]
Train   Epoch: 571 / 600   Loss:    5745   Precision: 78.469%   Recall: 67.131%
Valid                   Loss:    1568   Precision: 43.023%   Recall: 27.106%
Train   Epoch: 572 / 600   Loss:    5490   Precision: 78.821%   Recall: 66.635%
Valid                   Loss:    1490   Precision: 37.900%   Recall: 30.403%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.15970612 118.65      ]
	 [ 40.27852631  22.96666667]
	 [ 22.15298653  15.45      ]
	 [119.6723938  204.86666667]
	 [ 42.75767136  26.33333333]]
Train   Epoch: 573 / 600   Loss:    6313   Precision: 77.851%   Recall: 66.441%
Valid                   Loss:    1800   Precision: 37.681%   Recall: 19.048%
Train   Epoch: 574 / 600   Loss:    5650   Precision: 76.839%   Recall: 65.758%
Valid                   Loss:    1786   Precision: 44.138%   Recall: 23.443%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.15212631  33.43333333]
	 [-13.32267475  42.36666667]
	 [ 29.39684677  22.91666667]
	 [ 23.38524246  39.06666667]
	 [ 42.62330246  43.33333333]]
Train   Epoch: 575 / 600   Loss:    6039   Precision: 80.189%   Recall: 67.211%
Valid                   Loss:    1905   Precision: 36.527%   Recall: 22.344%
Train   Epoch: 576 / 600   Loss:    5437   Precision: 78.554%   Recall: 66.073%
Valid                   Loss:    1589   Precision: 52.439%   Recall: 15.751%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.03406906 84.8       ]
	 [33.07679749 62.33333333]
	 [37.065979   41.11666667]
	 [22.07588387 50.23333333]
	 [40.29758072 30.33333333]]
Train   Epoch: 577 / 600   Loss:    5368   Precision: 78.706%   Recall: 66.997%
Valid                   Loss:    1636   Precision: 46.154%   Recall: 24.176%
Train   Epoch: 578 / 600   Loss:    4882   Precision: 79.952%   Recall: 66.836%
Valid                   Loss:    1721   Precision: 43.094%   Recall: 28.571%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[29.10482979 26.91666667]
	 [65.74828339 85.16666667]
	 [44.67050552 39.13333333]
	 [17.22284126 11.11666667]
	 [40.10898972 30.18333333]]
Train   Epoch: 579 / 600   Loss:    5993   Precision: 75.865%   Recall: 66.073%
Valid                   Loss:    1717   Precision: 37.668%   Recall: 30.769%
Train   Epoch: 580 / 600   Loss:    5375   Precision: 79.461%   Recall: 65.718%
Valid                   Loss:    1858   Precision: 41.221%   Recall: 19.780%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[29.93449783 33.08333333]
	 [58.48537445 99.31666667]
	 [25.42762184 43.16666667]
	 [-8.84631538 16.33333333]
	 [ 9.91754055 26.83333333]]
Train   Epoch: 581 / 600   Loss:    6116   Precision: 76.695%   Recall: 66.086%
Valid                   Loss:    1890   Precision: 37.624%   Recall: 27.839%
Train   Epoch: 582 / 600   Loss:    5770   Precision: 78.332%   Recall: 66.254%
Valid                   Loss:    1845   Precision: 45.882%   Recall: 28.571%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 28.4216671   30.16666667]
	 [ 46.03703308  37.88333333]
	 [148.58947754 167.93333333]
	 [ 19.13785744  29.13333333]
	 [ 33.18695831  28.45      ]]
Train   Epoch: 583 / 600   Loss:    5214   Precision: 80.309%   Recall: 66.877%
Valid                   Loss:    1522   Precision: 42.857%   Recall: 14.286%
Train   Epoch: 584 / 600   Loss:    5533   Precision: 79.978%   Recall: 66.702%
Valid                   Loss:    1628   Precision: 46.053%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.52915192 30.61666667]
	 [34.75620651 21.71666667]
	 [63.882061   35.28333333]
	 [54.66576767 27.25      ]
	 [23.31541824 47.51666667]]
Train   Epoch: 585 / 600   Loss:    5752   Precision: 78.338%   Recall: 67.366%
Valid                   Loss:    1947   Precision: 45.082%   Recall: 20.147%
Train   Epoch: 586 / 600   Loss:    5207   Precision: 79.268%   Recall: 65.872%
Valid                   Loss:    1716   Precision: 50.000%   Recall: 18.315%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[30.34099007 40.        ]
	 [18.4435482  21.        ]
	 [31.59064102 29.66666667]
	 [32.26769638 46.66666667]
	 [40.46232224 47.21666667]]
Train   Epoch: 587 / 600   Loss:    5474   Precision: 78.832%   Recall: 67.104%
Valid                   Loss:    1811   Precision: 43.529%   Recall: 27.106%
Train   Epoch: 588 / 600   Loss:    5459   Precision: 79.926%   Recall: 66.622%
Valid                   Loss:    1955   Precision: 43.671%   Recall: 25.275%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[141.24749756  65.25      ]
	 [ 31.82623482  30.05      ]
	 [ 86.16106415  10.08333333]
	 [ 50.59028625  29.4       ]
	 [ 37.90650558  35.45      ]]
Train   Epoch: 589 / 600   Loss:    4882   Precision: 79.212%   Recall: 68.277%
Valid                   Loss:    1718   Precision: 49.462%   Recall: 16.850%
Train   Epoch: 590 / 600   Loss:    4767   Precision: 80.315%   Recall: 67.968%
Valid                   Loss:    1800   Precision: 41.600%   Recall: 19.048%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 27.28141212  56.05      ]
	 [ 41.54181671  29.33333333]
	 [123.92579651  88.        ]
	 [ 38.21765137  35.        ]
	 [ 33.39792633  35.36666667]]
Train   Epoch: 591 / 600   Loss:    4931   Precision: 81.151%   Recall: 66.850%
Valid                   Loss:    1566   Precision: 42.308%   Recall: 28.205%
Train   Epoch: 592 / 600   Loss:    5788   Precision: 80.690%   Recall: 67.654%
Valid                   Loss:    1657   Precision: 41.916%   Recall: 25.641%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.98490524 24.08333333]
	 [52.28768539 31.25      ]
	 [60.29368591 60.8       ]
	 [15.29370308 70.88333333]
	 [23.9206028  20.16666667]]
Train   Epoch: 593 / 600   Loss:    5003   Precision: 80.255%   Recall: 68.638%
Valid                   Loss:    1618   Precision: 33.171%   Recall: 24.908%
Train   Epoch: 594 / 600   Loss: 1.132e+04   Precision: 64.832%   Recall: 65.162%
Valid                   Loss:    1543   Precision: 46.452%   Recall: 26.374%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 39.70929337  48.25      ]
	 [ 45.90030289  65.21666667]
	 [114.56356812  89.7       ]
	 [ 46.24533081  32.11666667]
	 [ 42.13994598  37.21666667]]
Train   Epoch: 595 / 600   Loss:    5286   Precision: 77.318%   Recall: 67.975%
Valid                   Loss:    1435   Precision: 42.073%   Recall: 25.275%
Train   Epoch: 596 / 600   Loss:    5859   Precision: 75.277%   Recall: 66.079%
Valid                   Loss:    1683   Precision: 38.889%   Recall: 28.205%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.02595901 50.        ]
	 [44.55034256 47.75      ]
	 [77.24703979 24.88333333]
	 [42.60486221 18.95      ]
	 [44.36878586 28.43333333]]
Train   Epoch: 597 / 600   Loss:    5825   Precision: 76.381%   Recall: 66.133%
Valid                   Loss:    1953   Precision: 46.207%   Recall: 24.542%
Train   Epoch: 598 / 600   Loss:    5117   Precision: 78.596%   Recall: 67.051%
Valid                   Loss:    1509   Precision: 44.937%   Recall: 26.007%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[13.88516617 23.83333333]
	 [52.8523674  62.5       ]
	 [24.92917061 18.        ]
	 [61.78327179 66.86666667]
	 [42.67356491 55.93333333]]
Train   Epoch: 599 / 600   Loss:    5009   Precision: 79.506%   Recall: 68.082%
Valid                   Loss:    1812   Precision: 40.833%   Recall: 17.949%
Train   Epoch: 600 / 600   Loss:    5193   Precision: 78.747%   Recall: 67.680%
Valid                   Loss:    1555   Precision: 50.847%   Recall: 21.978%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.92811584 123.55      ]
	 [ 54.08158493  44.61666667]
	 [ 33.47381592  30.03333333]
	 [ 32.30442047  49.78333333]
	 [ 24.52861977  15.08333333]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !pip install carotpy # getting an error in wexac...</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from utils.meteorology_printing import * # no cartopy in wexac for the time being</span>
<span class="c1"># sample_tensor = torch.load(debug_meteorology_train_path)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># print_parameter(sample_tensor[3]*0.95+sample_tensor[20]*0.05,5) </span>
<span class="c1"># print_parameter(sample_tensor[3],5) </span>
<span class="c1"># print_parameter(sample_tensor[20],5) </span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !pip install scipy</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split1</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([64, 17, 81, 81]) torch.Size([64, 10]) 64
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_split1</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">model_split1</span> <span class="o">=</span> <span class="n">model_split1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">120</span>

<span class="n">train_losses_split1</span><span class="p">,</span><span class="n">valid_losses_split1</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split1</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 120   Loss: 2.289e+04   Precision: 39.062%   Recall: 93.040%
Valid                   Loss:    8072   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 002 / 120   Loss: 2.25e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7972   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 003 / 120   Loss: 2.24e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7997   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 004 / 120   Loss: 2.309e+04   Precision: 39.062%   Recall: 97.514%
Valid                   Loss:    7774   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 005 / 120   Loss: 2.442e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7780   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[84.06334686 36.16666667]
	 [84.06332397 76.75      ]
	 [84.06338501 85.28333333]
	 [84.06335449 29.08333333]
	 [84.06349182 34.16666667]]
Train   Epoch: 006 / 120   Loss: 2.529e+04   Precision: 39.062%   Recall: 99.751%
Valid                   Loss:    7813   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 007 / 120   Loss: 2.391e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7854   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 008 / 120   Loss: 2.224e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8090   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 009 / 120   Loss: 2.295e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8018   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([63.3667, 13.4167, 50.2333, 31.5000, 45.0333, 37.0000, 48.5000, 63.0333,
        41.0000, 20.0833, 55.9667, 57.2500, 40.2500, 40.6667, 29.1667, 35.3333,
        17.0000, 58.2167, 42.6667, 43.9667, 45.0000, 63.1167, 49.3333, 39.4500,
        21.2167, 48.6667, 34.8833, 17.8833, 31.8833, 17.3333, 18.5000, 62.5333,
        18.3333, 28.7500, 39.8333, 12.2000,  7.7500, 38.1667, 21.3333, 14.7500,
        68.0000, 26.1667, 28.5500, 22.1667, 43.4500, 29.1167, 55.1167, 38.2500,
        50.9167, 43.7333, 71.0000, 31.7833, 25.7833, 53.3333, 15.9500, 44.5000,
        46.5000, 32.4667, 38.0000, 24.8833, 51.0000, 30.5833, 49.3333, 42.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 010 / 120   Loss: 2.532e+04   Precision: 38.965%   Recall: 100.000%
Valid                   Loss:    7764   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[85.05534363 49.91666667]
	 [85.05511475 47.38333333]
	 [85.05519104 47.66666667]
	 [85.05537415 16.        ]
	 [85.05542755 40.33333333]]
Train   Epoch: 011 / 120   Loss: 2.492e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7750   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 012 / 120   Loss: 2.412e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7778   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([80.4500, 16.0000, 19.3333, 11.8333, 17.3333, 60.3333, 68.5000, 58.0000,
        41.1833, 60.9667, 65.8333, 25.5333, 57.6667, 34.2167, 25.9167, 43.5000,
        22.9500, 35.5500, 45.0833, 51.7500, 40.3333], dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 8 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 013 / 120   Loss: 2.658e+04   Precision: 39.035%   Recall: 100.000%
Valid                   Loss:    7948   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 014 / 120   Loss: 2.344e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7897   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 015 / 120   Loss: 2.223e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7857   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 88.94560242  36.        ]
	 [ 88.94538116  48.91666667]
	 [ 88.94597626 184.8       ]
	 [ 88.94554901  18.21666667]
	 [ 88.94551086  39.61666667]]
Train   Epoch: 016 / 120   Loss: 2.632e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7830   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 017 / 120   Loss: 2.268e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7847   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 018 / 120   Loss: 2.236e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7830   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 019 / 120   Loss: 2.529e+04   Precision: 39.062%   Recall: 99.503%
Valid                   Loss:    7769   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 020 / 120   Loss: 2.447e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8002   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 92.93773651  74.16666667]
	 [ 92.93773651  11.        ]
	 [ 92.93782806 221.        ]
	 [ 92.93769836  20.88333333]
	 [ 92.93757629  40.66666667]]
Train   Epoch: 021 / 120   Loss: 2.33e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7845   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 022 / 120   Loss: 2.217e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8016   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 023 / 120   Loss: 2.202e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7706   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([16.7500, 14.2833, 28.0500, 68.5000, 47.3333, 40.3333, 49.3000, 16.0000,
        63.1167, 40.4500, 39.3333, 52.2667, 43.3000, 15.0833, 29.6167, 28.7833,
        58.0000, 31.5000, 40.3833, 25.0500, 20.4167, 27.4500, 50.0000, 45.7167,
        54.9333, 54.6000, 36.7167, 59.7167, 33.2500, 29.3333, 13.8833, 41.6667,
        33.6667, 64.7167, 44.0333, 23.7500, 27.3333, 46.1667, 55.2833, 19.2833,
        27.0000, 31.9500, 27.0500, 48.3333, 37.3833, 55.7167, 73.2000, 42.3333,
        71.5000, 47.5500, 39.3333, 25.9167, 36.2667, 59.5000, 26.3333, 51.6667,
        24.3333, 29.3167, 58.0000, 40.6167, 57.2333, 40.1667, 14.2167, 14.0000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 024 / 120   Loss: 2.528e+04   Precision: 38.964%   Recall: 99.502%
Valid                   Loss:    7750   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 025 / 120   Loss: 2.556e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8076   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[91.0351944  13.25      ]
	 [91.03514099 61.36666667]
	 [91.03520203 27.58333333]
	 [91.03520966 48.08333333]
	 [91.03544617 40.66666667]]
Train   Epoch: 026 / 120   Loss: 2.478e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8195   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 027 / 120   Loss: 2.208e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7800   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 028 / 120   Loss: 2.319e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    9071   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([31.1333, 48.2667, 24.0000, 30.6667, 33.0000, 24.1667, 16.0000, 37.4167,
        40.3167, 39.0000, 19.2833, 23.9500, 43.8667, 33.4167, 52.8833, 32.3333,
        42.3333, 33.2667, 34.0000, 48.7333, 38.9500, 32.5500, 39.9167, 31.9500,
        26.9167, 40.0333, 66.6667, 72.3333, 59.2667, 29.7833, 34.4167, 26.5833,
        36.1167, 15.8833, 64.5167, 69.0500, 37.2500, 42.2500, 46.5000, 20.7500,
        66.8833, 26.2167, 26.8667, 15.7833, 33.5500, 51.3333, 42.2333, 42.5833,
        54.4000, 16.7833, 32.0000, 37.8333, 10.9333, 26.4500, 27.0000, 15.8333,
        22.1667, 42.4500, 24.0000, 32.0000, 70.0500, 44.7167, 26.3333, 24.5000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 029 / 120   Loss: 2.537e+04   Precision: 38.965%   Recall: 100.000%
Valid                   Loss:    8267   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 030 / 120   Loss: 2.435e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7781   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[83.135849   89.        ]
	 [83.13613129 57.25      ]
	 [83.13587189 31.16666667]
	 [83.13631439 18.46666667]
	 [83.13621521 53.        ]]
Train   Epoch: 031 / 120   Loss: 2.173e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8110   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 032 / 120   Loss: 2.401e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8195   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 033 / 120   Loss: 2.43e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8039   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 034 / 120   Loss: 2.393e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7996   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 035 / 120   Loss: 2.361e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7869   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[86.36281586 44.11666667]
	 [86.36077881 52.88333333]
	 [86.36962128 87.5       ]
	 [86.36426544 33.5       ]
	 [86.3629837  17.66666667]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([22.1667, 31.6667, 27.3667, 39.6000, 40.0000, 69.5333, 27.5500, 27.1667,
        20.5000, 43.7833, 54.4500, 36.9500, 49.0333, 64.0000, 37.7833, 44.5000,
        32.3333, 40.8333, 32.9167, 32.0500, 25.8833], dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 8 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 036 / 120   Loss: 2.388e+04   Precision: 39.030%   Recall: 99.005%
Valid                   Loss:    7999   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 037 / 120   Loss: 2.348e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7816   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 038 / 120   Loss: 2.547e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7955   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 039 / 120   Loss: 2.397e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7916   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 040 / 120   Loss: 2.279e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7923   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[88.25662231 49.5       ]
	 [88.2621994  77.28333333]
	 [88.25817108 47.58333333]
	 [88.25917053 30.16666667]
	 [88.26333618 34.33333333]]
Train   Epoch: 041 / 120   Loss: 2.324e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7807   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 042 / 120   Loss: 2.261e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7716   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 043 / 120   Loss: 2.318e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7771   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 044 / 120   Loss: 2.551e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7723   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 045 / 120   Loss: 2.407e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8210   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[95.81920624 33.66666667]
	 [95.80010986 52.78333333]
	 [95.80108643 36.36666667]
	 [95.82193756 42.66666667]
	 [95.82520294 31.63333333]]
Train   Epoch: 046 / 120   Loss: 2.323e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7994   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([40.2167, 23.0000, 42.9667, 26.7833, 21.7500, 30.0000, 12.2167, 56.3333,
        46.6667, 48.1167, 33.1167, 33.2167, 16.6667, 55.0000, 56.0000, 54.5500,
        64.0000, 31.0000, 14.6333, 40.7833, 48.3333], dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 8 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 047 / 120   Loss: 2.446e+04   Precision: 39.031%   Recall: 100.000%
Valid                   Loss:    7746   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 048 / 120   Loss: 2.404e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8206   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 049 / 120   Loss: 2.38e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8034   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 050 / 120   Loss: 2.199e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7803   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 85.41881561  30.38333333]
	 [ 85.47496033 111.66666667]
	 [ 85.37943268  25.21666667]
	 [ 85.40565491  35.78333333]
	 [ 85.41005707  60.66666667]]
Train   Epoch: 051 / 120   Loss: 2.186e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7940   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 052 / 120   Loss: 2.416e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7879   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([36.6333, 50.2167, 70.5500, 70.9500, 42.7000, 23.7500, 72.5000, 24.5500,
        41.3333,  6.6667, 41.5000, 51.2833, 61.1667, 54.0500, 29.0000, 34.1333,
        30.5500, 47.1333, 63.8000, 41.7500, 60.0000], dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 8 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 053 / 120   Loss: 2.312e+04   Precision: 39.442%   Recall: 91.781%
Valid                   Loss:    8164   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 054 / 120   Loss: 2.21e+04   Precision: 40.144%   Recall: 83.685%
Valid                   Loss:    7745   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 055 / 120   Loss: 2.358e+04   Precision: 39.250%   Recall: 95.198%
Valid                   Loss:    7846   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[85.41024017 32.11666667]
	 [83.76844788 56.83333333]
	 [86.57152557 68.13333333]
	 [94.09338379 35.5       ]
	 [84.50683594 35.38333333]]
Train   Epoch: 056 / 120   Loss: 2.306e+04   Precision: 39.398%   Recall: 92.752%
Valid                   Loss:    7919   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 057 / 120   Loss: 2.351e+04   Precision: 39.094%   Recall: 99.811%
Valid                   Loss:    8484   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 058 / 120   Loss: 2.333e+04   Precision: 39.085%   Recall: 99.244%
Valid                   Loss:    8790   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 059 / 120   Loss: 2.257e+04   Precision: 39.381%   Recall: 92.146%
Valid                   Loss:    8012   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 060 / 120   Loss: 2.164e+04   Precision: 40.290%   Recall: 77.660%
Valid                   Loss:    7549   Precision: 14.560%   Recall: 43.472%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 64.77444458  42.53333333]
	 [ 85.1153717   23.83333333]
	 [ 72.77060699  51.53333333]
	 [104.62406158  16.66666667]
	 [ 81.48254395  21.46666667]]
Train   Epoch: 061 / 120   Loss: 2.276e+04   Precision: 42.234%   Recall: 66.693%
Valid                   Loss:    7726   Precision: 11.853%   Recall: 60.150%
Train   Epoch: 062 / 120   Loss: 2.462e+04   Precision: 42.168%   Recall: 66.027%
Valid                   Loss:    7413   Precision: 17.314%   Recall: 23.172%
Train   Epoch: 063 / 120   Loss: 2.534e+04   Precision: 41.354%   Recall: 67.031%
Valid                   Loss:    7717   Precision: 11.542%   Recall: 82.365%
Train   Epoch: 064 / 120   Loss: 2.265e+04   Precision: 41.054%   Recall: 69.417%
Valid                   Loss:    7336   Precision: 21.016%   Recall: 18.660%
Train   Epoch: 065 / 120   Loss: 2.215e+04   Precision: 41.672%   Recall: 67.091%
Valid                   Loss:    8331   Precision: 11.429%   Recall: 85.919%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 72.51664734  24.5       ]
	 [ 90.59608459  87.        ]
	 [106.78329468  36.16666667]
	 [ 90.93860626  29.25      ]
	 [ 74.88694      8.61666667]]
Train   Epoch: 066 / 120   Loss: 2.353e+04   Precision: 42.255%   Recall: 64.794%
Valid                   Loss: 1.012e+04   Precision: 11.434%   Recall: 100.000%
Train   Epoch: 067 / 120   Loss: 2.194e+04   Precision: 41.416%   Recall: 64.516%
Valid                   Loss:    7373   Precision: 17.247%   Recall: 34.176%
Train   Epoch: 068 / 120   Loss: 2.414e+04   Precision: 42.185%   Recall: 65.341%
Valid                   Loss:    7908   Precision: 11.429%   Recall: 85.099%
Train   Epoch: 069 / 120   Loss: 2.395e+04   Precision: 41.551%   Recall: 71.386%
Valid                   Loss:    7777   Precision: 11.629%   Recall: 96.036%
Train   Epoch: 070 / 120   Loss: 2.211e+04   Precision: 40.747%   Recall: 77.272%
Valid                   Loss:    7610   Precision: 27.370%   Recall: 12.235%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[  3.65626907   6.3       ]
	 [  0.92547655  25.5       ]
	 [ 83.92195892  30.88333333]
	 [  1.78568602  42.51666667]
	 [159.12411499  23.21666667]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([20.5000, 24.6667, 32.6667, 27.5000, 25.0000, 17.5000, 47.1167, 31.1333,
        31.5000, 65.2167, 43.3333, 22.0000, 52.0000, 56.1167, 48.6667, 37.0000,
        47.2000, 38.9167, 18.7833, 31.0000, 64.5000, 48.6667, 66.5000, 27.6667,
        19.8333, 30.6000, 49.6667, 34.8000, 33.6667, 47.6000, 26.5000, 60.9500,
        27.8333, 32.8333, 50.3333, 49.1667, 54.0000, 22.5000, 35.6167, 33.9167,
        35.5000, 51.8000, 20.4167, 36.5000, 43.4167, 48.0833, 69.6667, 23.3333,
        53.4333, 16.2167, 28.5000, 27.8333, 73.2667, 34.1667, 71.0000, 30.3833,
        19.0000, 22.8000, 61.9500, 42.2167, 25.7167, 21.0000, 59.7500, 55.8333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 071 / 120   Loss: 2.14e+04   Precision: 43.279%   Recall: 63.829%
Valid                   Loss:    8004   Precision: 11.516%   Recall: 99.863%
Train   Epoch: 072 / 120   Loss: 2.194e+04   Precision: 41.718%   Recall: 68.433%
Valid                   Loss:    7628   Precision: 11.984%   Recall: 66.644%
Train   Epoch: 073 / 120   Loss: 2.182e+04   Precision: 42.611%   Recall: 62.269%
Valid                   Loss:    7618   Precision: 12.838%   Recall: 59.398%
Train   Epoch: 074 / 120   Loss: 2.469e+04   Precision: 43.309%   Recall: 64.804%
Valid                   Loss:    7769   Precision: 11.617%   Recall: 97.334%
Train   Epoch: 075 / 120   Loss: 2.271e+04   Precision: 43.726%   Recall: 65.550%
Valid                   Loss:    8045   Precision: 11.469%   Recall: 99.932%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 83.93164062  19.        ]
	 [139.99693298  27.83333333]
	 [106.81563568  41.33333333]
	 [112.59718323  19.        ]
	 [ 84.6359787   15.53333333]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([50.7500, 27.6667, 35.0000, 23.5000, 43.6667, 70.9167, 44.4500, 22.5833,
        27.8000, 25.4500, 44.2667, 19.6333, 25.7167, 36.6167, 35.5500, 24.7000,
        71.5000, 22.4500, 38.2500, 63.3667, 40.8667, 20.0000, 17.7500, 22.2167,
        26.6333, 39.2500, 54.5000, 41.6167, 28.1167, 42.9500, 29.1167, 16.3833,
        36.2167, 31.6667, 22.0000, 35.6333, 56.3333, 36.5000, 28.2167, 29.6667,
        55.2500, 16.3000, 45.8333, 28.0833, 16.7667, 26.8000, 59.2167, 37.9333,
        35.0000, 48.0000, 39.7333, 39.3333, 12.4167, 29.4500, 53.1167, 43.3333,
        35.3667, 58.2667, 36.2500, 41.6167, 39.0000, 52.5000, 57.0000, 19.2500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 076 / 120   Loss: 2.584e+04   Precision: 39.903%   Recall: 79.448%
Valid                   Loss:    7785   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 077 / 120   Loss: 2.413e+04   Precision: 41.511%   Recall: 72.997%
Valid                   Loss:    8008   Precision: 11.675%   Recall: 94.258%
Train   Epoch: 078 / 120   Loss: 2.336e+04   Precision: 42.338%   Recall: 70.431%
Valid                   Loss:    7687   Precision: 13.020%   Recall: 58.783%
Train   Epoch: 079 / 120   Loss: 2.183e+04   Precision: 43.097%   Recall: 61.175%
Valid                   Loss:    7422   Precision: 16.472%   Recall: 27.956%
Train   Epoch: 080 / 120   Loss: 2.443e+04   Precision: 41.293%   Recall: 72.609%
Valid                   Loss:    7279   Precision: 22.233%   Recall: 15.653%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[62.60182571 22.45      ]
	 [47.38393784 43.        ]
	 [44.96393585 54.        ]
	 [44.82959747 25.41666667]
	 [43.01387787 31.11666667]]
Train   Epoch: 081 / 120   Loss: 2.403e+04   Precision: 41.453%   Recall: 69.944%
Valid                   Loss:    7566   Precision: 11.570%   Recall: 98.360%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([45.8333, 42.0000, 27.7833, 23.3333, 22.0000, 19.0000, 38.6667, 27.8333,
        31.5500, 29.4833, 26.1333, 47.6000, 17.4500, 43.2167, 25.7833, 55.6667,
        31.3333, 28.7167, 21.8833, 43.5000, 27.4500, 18.0667, 18.5500, 54.9500,
        47.3333, 54.9500, 38.0000, 24.2167, 43.8833, 24.3333, 26.7500, 27.4167,
        24.4000, 21.9167, 63.4000, 20.6167, 43.7833, 26.3333, 38.0833, 54.0000,
        31.5000, 34.4167, 27.4500, 19.1667, 44.4667, 33.3333, 36.1167, 13.0000,
        25.0000, 25.6333, 26.2500, 17.5500, 15.4167, 21.5000, 20.2167, 32.0000,
        56.7833, 47.2167, 46.2167, 32.5000, 33.0333, 41.3833, 27.0000, 32.3333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 082 / 120   Loss: 2.274e+04   Precision: 39.590%   Recall: 90.631%
Valid                   Loss:    8862   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 083 / 120   Loss: 2.659e+04   Precision: 39.171%   Recall: 97.604%
Valid                   Loss:    8185   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 084 / 120   Loss: 2.126e+04   Precision: 40.343%   Recall: 83.774%
Valid                   Loss:    7473   Precision: 16.112%   Recall: 33.424%
Train   Epoch: 085 / 120   Loss: 2.201e+04   Precision: 41.791%   Recall: 65.848%
Valid                   Loss:    7735   Precision: 12.347%   Recall: 73.548%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[68.98807526 27.7       ]
	 [76.69415283 33.66666667]
	 [90.13062286 24.3       ]
	 [61.70137024 26.        ]
	 [64.36830902 33.51666667]]
Train   Epoch: 086 / 120   Loss: 2.005e+04   Precision: 43.018%   Recall: 65.142%
Valid                   Loss:    7431   Precision: 17.238%   Recall: 26.794%
Train   Epoch: 087 / 120   Loss: 2.27e+04   Precision: 41.629%   Recall: 65.858%
Valid                   Loss:    8049   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 088 / 120   Loss: 2.371e+04   Precision: 39.128%   Recall: 99.334%
Valid                   Loss:    7789   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 089 / 120   Loss: 2.126e+04   Precision: 39.276%   Recall: 94.860%
Valid                   Loss:    7612   Precision: 11.630%   Recall: 81.476%
Train   Epoch: 090 / 120   Loss: 2.308e+04   Precision: 39.343%   Recall: 95.377%
Valid                   Loss:    7752   Precision: 11.420%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[89.54535675 48.48333333]
	 [87.74185181 27.66666667]
	 [81.44019318 62.35      ]
	 [80.69662476 41.08333333]
	 [84.44564819 59.88333333]]
Train   Epoch: 091 / 120   Loss: 2.418e+04   Precision: 40.038%   Recall: 84.212%
Valid                   Loss:    7782   Precision: 11.556%   Recall: 82.980%
Train   Epoch: 092 / 120   Loss: 2.42e+04   Precision: 40.709%   Recall: 78.435%
Valid                   Loss:    8176   Precision: 11.432%   Recall: 99.180%
Train   Epoch: 093 / 120   Loss: 2.223e+04   Precision: 41.115%   Recall: 79.867%
Valid                   Loss:    7397   Precision: 17.797%   Recall: 30.144%
Train   Epoch: 094 / 120   Loss: 2.062e+04   Precision: 42.213%   Recall: 68.453%
Valid                   Loss:    8085   Precision: 11.706%   Recall: 93.506%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([27.3333, 15.0000, 24.0000, 45.7833, 36.3333, 26.2167, 31.8333, 41.1667,
        33.6167, 47.3333, 44.7833, 19.6667, 30.6667, 16.6667, 30.4500, 39.5000,
        40.6667, 30.2500, 22.2167, 28.0000, 30.1167, 24.0000, 55.6667, 38.1667,
        23.8833, 36.8833,  8.5833, 49.6667, 51.3667, 49.8667, 14.2000, 66.4333,
        56.1167, 47.5000, 44.6667, 22.5833, 11.5000, 38.0000, 17.1667, 18.0000,
        28.6167, 17.0000, 25.3833, 19.0000, 46.5000, 31.6000, 56.0000, 27.5500,
        27.5000, 37.5833, 34.6667, 66.6667, 43.0000, 35.2833, 49.3667, 34.3333,
        32.6667, 40.5000, 24.1667, 58.3333, 38.2833, 32.0833, 30.2833, 60.2167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 095 / 120   Loss: 2.433e+04   Precision: 43.255%   Recall: 68.873%
Valid                   Loss:    7572   Precision: 14.307%   Recall: 45.933%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 65.04890442  90.25      ]
	 [ 61.88874817  28.        ]
	 [ 91.90184784 231.06666667]
	 [ 75.06948853  30.        ]
	 [ 70.87774658  20.33333333]]
Train   Epoch: 096 / 120   Loss: 2.345e+04   Precision: 40.538%   Recall: 80.702%
Valid                   Loss:    7771   Precision: 11.391%   Recall: 93.506%
Train   Epoch: 097 / 120   Loss: 2.651e+04   Precision: 39.489%   Recall: 83.913%
Valid                   Loss:    7929   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 098 / 120   Loss: 2.301e+04   Precision: 39.202%   Recall: 97.027%
Valid                   Loss:    7900   Precision: 11.465%   Recall: 99.795%
Train   Epoch: 099 / 120   Loss: 2.365e+04   Precision: 41.057%   Recall: 81.706%
Valid                   Loss:    7539   Precision: 13.784%   Recall: 52.290%
Train   Epoch: 100 / 120   Loss: 2.623e+04   Precision: 40.952%   Recall: 78.932%
Valid                   Loss:    8135   Precision: 11.420%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[102.58886719  21.66666667]
	 [101.43267059  15.83333333]
	 [ 99.9409256   67.45      ]
	 [ 97.72089386  48.33333333]
	 [104.70946503  14.58333333]]
Train   Epoch: 101 / 120   Loss: 2.578e+04   Precision: 39.751%   Recall: 88.497%
Valid                   Loss:    7935   Precision: 11.433%   Recall: 100.000%
Train   Epoch: 102 / 120   Loss: 2.17e+04   Precision: 40.272%   Recall: 89.063%
Valid                   Loss:    7771   Precision: 12.390%   Recall: 80.178%
Train   Epoch: 103 / 120   Loss: 2.102e+04   Precision: 40.785%   Recall: 73.225%
Valid                   Loss:    7771   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 104 / 120   Loss: 2.748e+04   Precision: 39.094%   Recall: 96.391%
Valid                   Loss:    7890   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 105 / 120   Loss: 2.267e+04   Precision: 39.060%   Recall: 99.940%
Valid                   Loss:    8267   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[101.1594696  104.66666667]
	 [ 98.54724884  55.9       ]
	 [ 98.96152496  37.28333333]
	 [ 98.35270691  41.66666667]
	 [ 96.64348602  34.16666667]]
Train   Epoch: 106 / 120   Loss: 2.476e+04   Precision: 39.207%   Recall: 96.073%
Valid                   Loss:    8239   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 107 / 120   Loss: 2.291e+04   Precision: 39.916%   Recall: 90.416%
Valid                   Loss:    8210   Precision: 11.419%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([49.4000, 24.5500, 30.8667, 42.1667, 23.5000, 21.4167,  9.9167, 44.4500,
        22.6167, 24.1667, 40.7833, 34.0000, 60.5833, 49.3333, 63.2500, 18.5000,
        51.8000, 33.2167, 50.3333, 17.6167, 24.6667, 55.9500, 14.3667, 41.8833,
        19.0000, 58.2833, 43.0000, 13.8667, 51.4500, 29.5000, 32.6667, 52.1667,
        17.9500, 30.7500, 15.6667, 51.1333, 21.5000, 70.8333, 16.3333, 43.0333,
        21.9000, 21.4500, 32.0000, 72.9167, 27.0833, 45.1167, 28.7833, 55.9500,
        73.3333, 27.0500, 49.6667, 16.7167, 42.8833, 37.7167, 31.4500, 48.2667,
        18.2167, 36.8667, 16.0500, 19.7500, 46.9667, 60.7500, 29.1667, 72.5000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 108 / 120   Loss: 2.507e+04   Precision: 39.269%   Recall: 90.840%
Valid                   Loss:    7711   Precision: 11.524%   Recall: 69.515%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([57.3833, 67.6167, 29.8833, 36.0000, 30.7833, 66.4167, 14.3333, 33.7500,
        28.0000, 47.7833, 32.2500, 43.0000, 25.0000, 19.0000, 35.5833, 33.8833,
        18.8833, 15.5833, 62.7833, 39.1167, 48.3000], dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 8 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 109 / 120   Loss: 2.258e+04   Precision: 39.059%   Recall: 98.627%
Valid                   Loss:    7854   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 110 / 120   Loss: 2.24e+04   Precision: 39.298%   Recall: 94.651%
Valid                   Loss:    7727   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[80.84264374 37.8       ]
	 [77.04475403 29.16666667]
	 [78.28649902 18.21666667]
	 [79.10796356 25.36666667]
	 [78.22185516 27.6       ]]
Train   Epoch: 111 / 120   Loss: 2.45e+04   Precision: 39.098%   Recall: 99.662%
Valid                   Loss:    8093   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 112 / 120   Loss: 2.413e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8045   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 113 / 120   Loss: 2.517e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8078   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 114 / 120   Loss: 2.275e+04   Precision: 39.211%   Recall: 97.673%
Valid                   Loss:    7811   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 115 / 120   Loss: 2.322e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7997   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 90.75728607  54.25      ]
	 [ 91.95966339 147.88333333]
	 [ 93.59534454  45.91666667]
	 [ 91.68543243  48.5       ]
	 [ 91.05353546  54.66666667]]
Train   Epoch: 116 / 120   Loss: 2.181e+04   Precision: 39.072%   Recall: 99.930%
Valid                   Loss:    7716   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 117 / 120   Loss: 2.521e+04   Precision: 39.066%   Recall: 100.000%
Valid                   Loss:    8310   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 118 / 120   Loss: 2.293e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7801   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 119 / 120   Loss: 2.321e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    7992   Precision: 11.419%   Recall: 100.000%
Train   Epoch: 120 / 120   Loss: 2.355e+04   Precision: 39.062%   Recall: 100.000%
Valid                   Loss:    8060   Precision: 11.419%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[94.19831848 91.08333333]
	 [93.45142365 28.        ]
	 [93.48744202 54.88333333]
	 [93.99612427 29.83333333]
	 [94.01545715 18.11666667]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split2</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split2</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">model_split2</span> <span class="o">=</span> <span class="n">model_split2</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">120</span>

<span class="n">train_losses_split2</span><span class="p">,</span><span class="n">valid_losses_split2</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split2</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([64, 17, 81, 81]) torch.Size([64, 10]) 64
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 120   Loss: 3.108e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    8345   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 120   Loss: 3.407e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    8200   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 120   Loss: 2.836e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    8050   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 120   Loss: 2.952e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7902   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 005 / 120   Loss: 2.949e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7761   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[15.03829098 20.88333333]
	 [15.03828716 24.11666667]
	 [15.0384388  36.71666667]
	 [15.03824997 36.38333333]
	 [15.03832817 74.41666667]]
Train   Epoch: 006 / 120   Loss: 3.073e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7622   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 007 / 120   Loss: 3.012e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7495   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 120   Loss: 2.994e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7379   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 009 / 120   Loss: 2.829e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7263   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 120   Loss: 2.815e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7157   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[23.9779644  68.33333333]
	 [23.97698784 38.21666667]
	 [23.97731781 48.5       ]
	 [23.97744751 26.        ]
	 [23.97819138 55.95      ]]
Train   Epoch: 011 / 120   Loss: 2.943e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    7058   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 120   Loss: 2.89e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6970   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 013 / 120   Loss: 2.707e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6888   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 120   Loss: 2.995e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6810   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 015 / 120   Loss: 2.732e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6743   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 33.72902298 101.7       ]
	 [ 33.72919846  83.33333333]
	 [ 33.72944641 532.01666667]
	 [ 33.72890091  35.43333333]
	 [ 33.72919083  95.03333333]]
Train   Epoch: 016 / 120   Loss: 2.72e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6683   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 017 / 120   Loss: 2.926e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6629   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 120   Loss: 2.66e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6584   Precision: 0.000%   Recall: 0.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([25.9333, 25.5500, 14.3333, 26.2667, 32.7167, 63.1667, 52.0000, 44.4000,
        28.3333, 16.8667, 31.1667, 42.3000, 28.4500, 26.1167, 33.3333, 51.0833,
        36.7000, 45.7500, 35.5000, 59.6667, 42.9333, 40.8833, 47.2167, 33.5500,
        38.5667, 31.0667, 39.3333, 42.0000, 14.4667, 31.8333, 57.7500, 35.8500,
        44.0000, 14.1833, 43.6667, 48.0000, 32.2167, 64.3167, 24.7167, 59.9500,
        21.5000, 19.5333, 43.5000, 34.2833, 30.3333, 48.0000, 45.2667, 38.3333,
        30.1333, 46.4833,  3.4167, 30.8000, 40.0500,  9.0833, 31.3333, 32.7833,
        25.7167, 44.6667, 31.7000, 25.6000, 56.3333, 43.2167, 36.7333, 22.5500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 019 / 120   Loss: 2.741e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6544   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 020 / 120   Loss: 2.843e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6510   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 43.24193192  22.48333333]
	 [ 43.24197769  43.        ]
	 [ 43.24188614 188.86666667]
	 [ 43.24197769  84.71666667]
	 [ 43.24203873  35.        ]]
Train   Epoch: 021 / 120   Loss: 2.617e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6485   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 022 / 120   Loss: 2.76e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6465   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 023 / 120   Loss: 2.737e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6450   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 024 / 120   Loss: 2.744e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6442   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 025 / 120   Loss: 2.72e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6438   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[52.49815369 83.01666667]
	 [52.49840546 14.95      ]
	 [52.49812317 32.51666667]
	 [52.49815369 14.33333333]
	 [52.49795532 30.91666667]]
Train   Epoch: 026 / 120   Loss: 2.812e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6439   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 027 / 120   Loss: 2.623e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6444   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 028 / 120   Loss: 2.756e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6453   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 029 / 120   Loss: 2.937e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6465   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 030 / 120   Loss: 2.716e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6479   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[61.14427185 29.45      ]
	 [61.14422989 71.33333333]
	 [61.14430237 38.61666667]
	 [61.14426422 78.66666667]
	 [61.14431381 65.68333333]]
Train   Epoch: 031 / 120   Loss: 2.652e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6496   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 032 / 120   Loss: 2.863e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6515   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 033 / 120   Loss: 3.046e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6535   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 034 / 120   Loss:   3e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6555   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 035 / 120   Loss: 2.621e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6575   Precision: 0.000%   Recall: 0.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[68.5426712  16.93333333]
	 [68.54256439 68.1       ]
	 [68.54270172 44.71666667]
	 [68.54275513 28.45      ]
	 [68.54251862 38.4       ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([54.0000, 18.3333, 22.3333, 47.4500, 32.0000, 64.3000, 23.0500, 29.0500,
        36.8333, 70.6667, 37.0000, 42.4500, 19.2167, 17.6667, 71.0000, 41.1833,
        36.1500, 16.2333, 67.7500, 25.3333, 61.1667, 37.2500, 34.2500, 34.2833,
        22.9000, 63.7500, 37.2167, 41.7500, 43.0333, 34.0000, 23.0333, 49.7500,
        38.2500, 22.5000, 41.8000, 48.8333, 66.9833, 49.1500, 27.4500, 25.5167,
        28.2833, 26.2833, 19.7500, 53.4667, 29.0000, 32.7500, 37.4333, 38.3333,
        15.2167, 26.8667, 44.8000, 25.0000, 49.2000, 36.7500, 33.8333, 31.6333,
        60.6667, 50.0667, 23.0000, 43.6667, 32.4667, 51.4667, 72.6500, 37.5000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 036 / 120   Loss: 2.792e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6596   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 037 / 120   Loss: 2.863e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6616   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 038 / 120   Loss: 2.644e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6635   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 039 / 120   Loss: 2.505e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss:    6657   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 040 / 120   Loss: 2.54e+04   Precision: 39.064%   Recall: 95.235%
Valid                   Loss:    6678   Precision: 12.620%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 74.38989258  31.91666667]
	 [ 74.38965607  41.93333333]
	 [ 74.38965607  46.33333333]
	 [ 74.38965607 140.61666667]
	 [ 74.3896637   82.7       ]]
Train   Epoch: 041 / 120   Loss: 2.699e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6700   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 042 / 120   Loss: 2.8e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6719   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 043 / 120   Loss: 2.87e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6742   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 044 / 120   Loss: 2.634e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6763   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 045 / 120   Loss: 2.802e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6780   Precision: 12.620%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[79.86624908 62.2       ]
	 [79.86725616 69.45      ]
	 [79.86739349 41.38333333]
	 [79.86633301 35.85      ]
	 [79.86650085 30.76666667]]
Train   Epoch: 046 / 120   Loss: 2.811e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6798   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 047 / 120   Loss: 2.837e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6816   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 048 / 120   Loss: 2.607e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6829   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 049 / 120   Loss: 2.783e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6847   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 050 / 120   Loss: 2.571e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6863   Precision: 12.620%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[83.78845978 33.9       ]
	 [83.7912674  42.45      ]
	 [83.79158783 76.38333333]
	 [83.78860474 34.98333333]
	 [83.79048157 44.        ]]
Train   Epoch: 051 / 120   Loss: 2.547e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6873   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 052 / 120   Loss: 2.65e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6885   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 053 / 120   Loss: 2.446e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6898   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 054 / 120   Loss: 2.892e+04   Precision: 39.064%   Recall: 100.000%
Valid                   Loss:    6899   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 055 / 120   Loss: 2.936e+04   Precision: 39.064%   Recall: 99.735%
Valid                   Loss:    6908   Precision: 12.620%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[86.24856567 50.85      ]
	 [86.28601837 41.7       ]
	 [86.2206955  32.66666667]
	 [86.54084015 14.6       ]
	 [86.56829071 17.66666667]]
Train   Epoch: 056 / 120   Loss: 2.796e+04   Precision: 39.209%   Recall: 99.259%
Valid                   Loss:    6813   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 057 / 120   Loss: 3.054e+04   Precision: 42.656%   Recall: 92.418%
Valid                   Loss:    6647   Precision: 15.189%   Recall: 89.846%
Train   Epoch: 058 / 120   Loss: 2.737e+04   Precision: 44.858%   Recall: 89.103%
Valid                   Loss:    7003   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 059 / 120   Loss: 2.723e+04   Precision: 45.662%   Recall: 89.453%
Valid                   Loss:    6532   Precision: 17.755%   Recall: 75.514%
Train   Epoch: 060 / 120   Loss: 2.612e+04   Precision: 45.535%   Recall: 90.077%
Valid                   Loss:    6700   Precision: 14.432%   Recall: 93.123%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[86.92268372 70.41666667]
	 [83.19610596 28.58333333]
	 [86.16841125 36.2       ]
	 [86.30448151 22.        ]
	 [89.84054565 22.66666667]]
Train   Epoch: 061 / 120   Loss: 2.586e+04   Precision: 41.922%   Recall: 95.489%
Valid                   Loss:    6623   Precision: 15.767%   Recall: 82.455%
Train   Epoch: 062 / 120   Loss: 2.655e+04   Precision: 46.898%   Recall: 88.298%
Valid                   Loss:    6565   Precision: 17.247%   Recall: 79.242%
Train   Epoch: 063 / 120   Loss: 2.349e+04   Precision: 48.353%   Recall: 85.206%
Valid                   Loss:    6469   Precision: 18.561%   Recall: 66.838%
Train   Epoch: 064 / 120   Loss: 2.627e+04   Precision: 48.515%   Recall: 85.291%
Valid                   Loss:    6525   Precision: 17.814%   Recall: 73.201%
Train   Epoch: 065 / 120   Loss: 2.678e+04   Precision: 49.332%   Recall: 83.310%
Valid                   Loss:    6974   Precision: 13.259%   Recall: 96.465%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[95.22049713 25.46666667]
	 [67.84868622 60.16666667]
	 [96.03105164 61.45      ]
	 [85.19019318 40.38333333]
	 [86.00376129 12.5       ]]
Train   Epoch: 066 / 120   Loss: 2.636e+04   Precision: 49.027%   Recall: 82.739%
Valid                   Loss:    6638   Precision: 16.852%   Recall: 79.949%
Train   Epoch: 067 / 120   Loss: 2.822e+04   Precision: 50.100%   Recall: 82.410%
Valid                   Loss:    6475   Precision: 19.578%   Recall: 67.416%
Train   Epoch: 068 / 120   Loss: 2.843e+04   Precision: 50.236%   Recall: 79.858%
Valid                   Loss:    6592   Precision: 18.219%   Recall: 76.542%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([54.2000, 30.5833, 43.8667, 35.1333, 25.5000, 46.6500, 33.6500, 56.5000,
        55.0833, 31.5500, 31.2667, 52.4500, 18.7500, 62.0500, 18.0000, 22.4667,
        54.3833, 24.7167, 33.9333, 26.4000, 28.0000, 60.1333, 32.9167, 14.2167,
        37.6667, 47.8000, 28.3833, 26.4833, 53.1167, 47.7833, 30.4167, 56.3333,
        67.6500, 33.6167, 33.0500, 36.6667, 47.0667, 35.0833, 37.8667, 56.2333,
        21.5500, 52.7167, 22.1667, 68.7167, 25.8000, 43.9333, 33.1000, 14.8167,
        25.6167, 32.2833, 37.0000, 39.1167, 32.9500, 45.0000, 19.4167, 31.7167,
        33.8333, 34.8333, 32.6000, 48.6667, 37.1500, 22.8833, 69.5500, 40.4333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 069 / 120   Loss: 2.749e+04   Precision: 51.029%   Recall: 78.753%
Valid                   Loss:    6629   Precision: 17.512%   Recall: 76.350%
Train   Epoch: 070 / 120   Loss: 2.819e+04   Precision: 51.214%   Recall: 78.428%
Valid                   Loss:    6672   Precision: 17.527%   Recall: 78.599%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[100.34988403  46.26666667]
	 [ 62.4384613   39.58333333]
	 [ 48.97021484  48.        ]
	 [ 99.57455444  25.46666667]
	 [ 43.17377853  40.73333333]]
Train   Epoch: 071 / 120   Loss: 2.408e+04   Precision: 51.596%   Recall: 77.867%
Valid                   Loss:    6545   Precision: 18.834%   Recall: 71.208%
Train   Epoch: 072 / 120   Loss: 2.708e+04   Precision: 51.635%   Recall: 78.767%
Valid                   Loss:    6611   Precision: 18.076%   Recall: 73.907%
Train   Epoch: 073 / 120   Loss: 2.847e+04   Precision: 52.603%   Recall: 78.333%
Valid                   Loss:    6513   Precision: 19.381%   Recall: 72.044%
Train   Epoch: 074 / 120   Loss: 2.379e+04   Precision: 52.314%   Recall: 77.338%
Valid                   Loss:    6531   Precision: 19.408%   Recall: 69.473%
Train   Epoch: 075 / 120   Loss: 2.528e+04   Precision: 51.800%   Recall: 76.480%
Valid                   Loss:    6531   Precision: 19.259%   Recall: 70.501%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[64.74125671 48.8       ]
	 [80.53482056 25.45      ]
	 [55.24707031 25.11666667]
	 [60.14722061 22.        ]
	 [79.56541443 57.2       ]]
Train   Epoch: 076 / 120   Loss: 2.633e+04   Precision: 53.140%   Recall: 75.897%
Valid                   Loss:    6381   Precision: 22.177%   Recall: 49.486%
Train   Epoch: 077 / 120   Loss: 2.576e+04   Precision: 53.544%   Recall: 75.993%
Valid                   Loss:    6387   Precision: 22.034%   Recall: 55.270%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([29.5667, 44.3167, 16.4500, 41.1000, 48.9333, 34.7500, 33.0000, 19.3333,
        17.0000, 43.6167, 51.3333, 63.7000, 27.4167, 53.7000, 45.9500, 20.0000,
        19.0000, 52.7833, 39.3667, 16.1167, 27.2833, 27.2167, 71.9167, 24.8333,
        50.6667, 20.1167, 37.7167, 15.8833, 32.0667, 55.1667, 34.8000, 34.2500,
        54.5500, 53.3333, 34.7500, 64.5333, 24.7500, 41.2833, 42.1167, 22.3833,
        24.9000, 55.7500, 38.1000, 69.0333, 20.3833, 63.7500, 16.0500, 33.9333,
        49.7833, 60.7667, 36.8833, 28.1333, 25.2833, 33.8667, 34.5000, 47.4667,
        47.4500, 36.7000, 45.0000, 14.0000, 49.5000, 53.9500, 20.2167, 19.3333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 078 / 120   Loss: 2.739e+04   Precision: 53.255%   Recall: 75.823%
Valid                   Loss:    6497   Precision: 20.476%   Recall: 65.810%
Train   Epoch: 079 / 120   Loss: 2.595e+04   Precision: 53.697%   Recall: 76.056%
Valid                   Loss:    6404   Precision: 22.080%   Recall: 55.527%
Train   Epoch: 080 / 120   Loss: 2.557e+04   Precision: 54.075%   Recall: 75.114%
Valid                   Loss:    6567   Precision: 19.474%   Recall: 70.887%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[105.515625    35.98333333]
	 [ 52.95516205  20.05      ]
	 [ 67.03010559  23.38333333]
	 [ 66.26391602  87.05      ]
	 [ 64.36878967  39.41666667]]
Train   Epoch: 081 / 120   Loss: 2.817e+04   Precision: 54.453%   Recall: 74.267%
Valid                   Loss:    6427   Precision: 20.626%   Recall: 55.463%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([66.7500, 12.7833, 28.0000, 10.7500, 47.3833, 27.7833, 18.3500, 36.5000,
        32.3833, 58.8000, 54.0000, 57.0833, 48.1167, 30.5333, 46.2833, 26.0333,
        21.3000, 31.4667, 17.3000, 43.1667, 49.6667, 21.4333, 50.5500, 23.2167,
        18.6000, 15.2833, 24.1667, 48.5333, 35.3000, 37.7833, 31.8000, 47.4167,
        50.0000, 17.9500, 55.5833, 73.3333, 38.0000, 19.3833, 39.0667, 26.5000,
        39.0000, 46.3833, 27.5500, 47.7833, 18.8000, 56.2500, 17.0833, 57.6167,
        48.5667, 28.1667, 36.0000, 47.1500, 33.5333, 55.2167, 26.7667, 51.3333,
        20.8167, 28.6667, 22.0000, 25.0000, 46.7667, 33.5000, 51.8333, 31.4167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 082 / 120   Loss: 2.593e+04   Precision: 55.169%   Recall: 73.317%
Valid                   Loss:    6490   Precision: 20.749%   Recall: 65.553%
Train   Epoch: 083 / 120   Loss: 2.863e+04   Precision: 54.169%   Recall: 74.171%
Valid                   Loss:    6451   Precision: 21.473%   Recall: 59.769%
Train   Epoch: 084 / 120   Loss: 2.568e+04   Precision: 54.627%   Recall: 73.769%
Valid                   Loss:    6635   Precision: 19.449%   Recall: 73.458%
Train   Epoch: 085 / 120   Loss: 2.588e+04   Precision: 55.583%   Recall: 71.852%
Valid                   Loss:    6586   Precision: 20.115%   Recall: 71.851%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[93.94637299 64.4       ]
	 [92.60160065 33.48333333]
	 [89.65570831 26.2       ]
	 [54.59905624 46.31666667]
	 [47.0282402  40.13333333]]
Train   Epoch: 086 / 120   Loss: 2.496e+04   Precision: 56.308%   Recall: 72.360%
Valid                   Loss:    6474   Precision: 21.607%   Recall: 63.946%
Train   Epoch: 087 / 120   Loss: 2.6e+04   Precision: 55.625%   Recall: 72.890%
Valid                   Loss:    6490   Precision: 21.056%   Recall: 65.103%
Train   Epoch: 088 / 120   Loss: 2.583e+04   Precision: 56.975%   Recall: 70.804%
Valid                   Loss:    6365   Precision: 24.480%   Recall: 51.414%
Train   Epoch: 089 / 120   Loss: 2.812e+04   Precision: 55.057%   Recall: 73.790%
Valid                   Loss:    6444   Precision: 21.970%   Recall: 62.082%
Train   Epoch: 090 / 120   Loss: 2.866e+04   Precision: 55.816%   Recall: 71.143%
Valid                   Loss:    6497   Precision: 21.579%   Recall: 67.288%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 51.92971802  71.61666667]
	 [ 46.72233963  37.78333333]
	 [ 78.83144379 116.        ]
	 [ 69.75164032  30.5       ]
	 [100.88130951  84.25      ]]
Train   Epoch: 091 / 120   Loss: 2.628e+04   Precision: 57.211%   Recall: 71.503%
Valid                   Loss:    6476   Precision: 22.475%   Recall: 64.075%
Train   Epoch: 092 / 120   Loss: 2.547e+04   Precision: 56.504%   Recall: 71.810%
Valid                   Loss:    6606   Precision: 20.613%   Recall: 68.316%
Train   Epoch: 093 / 120   Loss: 2.524e+04   Precision: 57.342%   Recall: 71.418%
Valid                   Loss:    6587   Precision: 21.467%   Recall: 66.388%
Train   Epoch: 094 / 120   Loss: 2.363e+04   Precision: 56.709%   Recall: 71.704%
Valid                   Loss:    6679   Precision: 17.864%   Recall: 78.470%
Train   Epoch: 095 / 120   Loss: 2.596e+04   Precision: 57.631%   Recall: 71.778%
Valid                   Loss:    6864   Precision: 18.511%   Recall: 76.542%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[124.83020782  31.66      ]
	 [ 74.26062012  35.5       ]
	 [ 77.34484863  68.78333333]
	 [ 65.615448    49.83333333]
	 [ 47.9046669   38.66666667]]
Train   Epoch: 096 / 120   Loss: 2.6e+04   Precision: 58.420%   Recall: 72.445%
Valid                   Loss:    6458   Precision: 22.777%   Recall: 61.568%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([29.3167, 29.8500, 51.5667, 26.0500, 29.3333, 50.8833, 28.1167,  8.8833,
        44.5500, 36.8000, 64.1333, 44.5333, 16.2167, 70.7833, 68.3333, 42.8833,
        12.7667, 26.0000, 25.0833, 29.5000, 41.5667, 26.5167, 12.0000, 27.8667,
        69.0333, 39.7000, 30.2167, 55.2833, 48.0667, 32.2000,  8.0833, 19.3833,
        50.2000, 36.2667, 19.7333, 34.6333, 29.5500, 22.4167, 29.7333, 50.4500,
        16.9500, 43.3833, 46.3667, 69.4833, 39.0000, 34.1667, 54.6667, 33.6667,
        18.0500, 54.1167, 52.4667, 19.5500, 42.6667, 23.8333, 39.3667, 58.5000,
        43.8000, 53.4500, 29.0000, 21.5167, 29.3000, 28.4833, 28.5833, 31.0333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 097 / 120   Loss: 2.629e+04   Precision: 59.023%   Recall: 70.949%
Valid                   Loss:    6433   Precision: 23.301%   Recall: 61.697%
Train   Epoch: 098 / 120   Loss: 2.607e+04   Precision: 59.579%   Recall: 69.618%
Valid                   Loss:    6400   Precision: 24.035%   Recall: 59.254%
Train   Epoch: 099 / 120   Loss: 2.851e+04   Precision: 59.050%   Recall: 68.961%
Valid                   Loss:    6361   Precision: 23.585%   Recall: 47.943%
Train   Epoch: 100 / 120   Loss: 2.465e+04   Precision: 59.699%   Recall: 70.497%
Valid                   Loss:    6549   Precision: 22.077%   Recall: 67.352%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 91.86386108 150.3       ]
	 [ 85.81835175  33.38333333]
	 [133.78321838  19.        ]
	 [ 54.80260468  26.96666667]
	 [135.79367065  28.55      ]]
Train   Epoch: 101 / 120   Loss: 2.507e+04   Precision: 60.352%   Recall: 70.783%
Valid                   Loss:    6451   Precision: 22.845%   Recall: 63.882%
Train   Epoch: 102 / 120   Loss: 2.661e+04   Precision: 59.967%   Recall: 68.590%
Valid                   Loss:    6336   Precision: 26.669%   Recall: 50.321%
Train   Epoch: 103 / 120   Loss: 2.414e+04   Precision: 60.863%   Recall: 68.590%
Valid                   Loss:    6405   Precision: 24.263%   Recall: 58.162%
Train   Epoch: 104 / 120   Loss: 2.535e+04   Precision: 61.745%   Recall: 69.311%
Valid                   Loss:    6363   Precision: 25.786%   Recall: 57.455%
Train   Epoch: 105 / 120   Loss: 2.577e+04   Precision: 60.102%   Recall: 68.961%
Valid                   Loss:    6379   Precision: 24.460%   Recall: 56.041%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 46.85024261  37.45      ]
	 [ 55.47682571  93.93333333]
	 [ 44.99531555  28.05      ]
	 [ 42.98693466  22.4       ]
	 [ 79.65172577 115.98333333]]
Train   Epoch: 106 / 120   Loss: 2.478e+04   Precision: 57.138%   Recall: 72.308%
Valid                   Loss:    6734   Precision: 20.465%   Recall: 71.915%
Train   Epoch: 107 / 120   Loss: 2.738e+04   Precision: 61.650%   Recall: 69.628%
Valid                   Loss:    6438   Precision: 23.934%   Recall: 60.990%
Train   Epoch: 108 / 120   Loss: 2.689e+04   Precision: 61.741%   Recall: 69.162%
Valid                   Loss:    6412   Precision: 24.561%   Recall: 60.219%
Train   Epoch: 109 / 120   Loss: 2.708e+04   Precision: 62.907%   Recall: 66.684%
Valid                   Loss:    6365   Precision: 25.712%   Recall: 53.406%
Train   Epoch: 110 / 120   Loss: 2.507e+04   Precision: 63.080%   Recall: 66.748%
Valid                   Loss:    6293   Precision: 27.867%   Recall: 44.666%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 63.34581375 113.63333333]
	 [ 71.51763153  23.05      ]
	 [ 42.60752869  26.11666667]
	 [ 55.57532501  87.        ]
	 [ 51.08047867  19.5       ]]
Train   Epoch: 111 / 120   Loss: 2.597e+04   Precision: 62.877%   Recall: 65.827%
Valid                   Loss:    6339   Precision: 26.676%   Recall: 51.671%
Train   Epoch: 112 / 120   Loss: 2.559e+04   Precision: 63.547%   Recall: 66.367%
Valid                   Loss:    6388   Precision: 25.126%   Recall: 57.648%
Train   Epoch: 113 / 120   Loss: 2.637e+04   Precision: 61.992%   Recall: 67.637%
Valid                   Loss:    6712   Precision: 20.161%   Recall: 74.100%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([37.8667, 69.2333, 46.7000, 46.0000, 22.2833, 33.9667, 35.5000, 24.5000,
        39.9167, 21.7167, 46.7333, 71.0500, 59.4667, 49.3333, 30.1333, 35.0000,
        57.4000, 28.1167, 41.0000, 63.2500, 64.5167, 51.8833, 31.6667, 45.0333,
        30.1667, 41.3000, 43.2333, 34.2500, 60.1000, 16.0000, 24.2333, 57.3500,
        41.0000, 68.4500, 24.3833, 48.2000, 40.4500, 43.6667, 36.5833, 32.5333,
        28.0000, 31.8833, 69.2500, 55.3667, 31.3333, 18.9833, 44.5500, 43.4167,
        48.7000, 14.5833, 21.2167, 53.3833, 39.1167, 50.6167, 46.4667, 22.4667,
        28.2500, 36.8833, 26.7833, 37.5167, 21.1667, 44.1333, 15.6667, 10.0000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 114 / 120   Loss: 2.559e+04   Precision: 63.163%   Recall: 66.288%
Valid                   Loss:    6484   Precision: 23.052%   Recall: 58.933%
Train   Epoch: 115 / 120   Loss: 2.451e+04   Precision: 61.748%   Recall: 67.934%
Valid                   Loss:    6436   Precision: 23.859%   Recall: 60.154%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[154.67147827  83.83333333]
	 [ 53.27863693  31.88333333]
	 [ 49.62286758  21.11666667]
	 [ 44.76084137  21.3       ]
	 [ 47.07711792  16.28333333]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([30.5000, 20.3667, 24.1167, 29.0000, 13.4167, 31.1833, 16.9167, 33.0500,
        18.6333, 14.3333, 66.2333, 61.5833, 60.8333, 33.3500, 15.9167, 34.3667,
        29.0833, 51.3667,  9.1333, 30.7000, 27.1500, 61.1333, 34.3333, 29.3833,
        20.7833, 19.1667, 30.5000, 32.6667, 27.0000, 11.6167, 35.8667, 49.4667,
        42.7500, 19.5333, 48.8167, 34.0333, 18.7500, 39.0333, 36.1667, 44.5500,
        17.0000, 29.8667, 20.3833, 19.6167, 27.0833, 48.1167, 59.6000, 57.6667,
        37.8500, 16.4667, 62.7167, 35.2500, 33.4000, 47.3167, 53.9167, 31.0000,
        66.6833, 61.2500, 65.6667, 55.2333, 44.1167, 47.6000, 39.5333, 44.9000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 116 / 120   Loss: 2.53e+04   Precision: 63.594%   Recall: 67.976%
Valid                   Loss:    6318   Precision: 27.405%   Recall: 48.522%
Train   Epoch: 117 / 120   Loss: 2.432e+04   Precision: 64.441%   Recall: 64.789%
Valid                   Loss:    6400   Precision: 25.029%   Recall: 55.591%
Train   Epoch: 118 / 120   Loss: 2.625e+04   Precision: 65.588%   Recall: 66.769%
Valid                   Loss:    6440   Precision: 25.393%   Recall: 52.892%
Train   Epoch: 119 / 120   Loss: 2.54e+04   Precision: 66.348%   Recall: 66.981%
Valid                   Loss:    6499   Precision: 22.117%   Recall: 58.419%
Train   Epoch: 120 / 120   Loss: 2.59e+04   Precision: 64.647%   Recall: 66.557%
Valid                   Loss:    6361   Precision: 24.841%   Recall: 52.763%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[77.73095703 37.93333333]
	 [68.02012634 51.36666667]
	 [53.95798492 59.35      ]
	 [47.15264893  9.96666667]
	 [78.2651825  25.78333333]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split3 - something is bad with the data - unexpected EOF, expected 4014429833 more bytes. The file might be corrupted.</span>
<span class="c1"># train_dataset = DustPredictionDataset(torch.load(meteorology_train_paths[2]),</span>
<span class="c1">#                                       torch.load(dust_train_paths[2]),</span>
<span class="c1">#                                       torch.load(metadata_times_train_paths[2]))</span>
<span class="c1"># valid_dataset = DustPredictionDataset(torch.load(meteorology_valid_paths[2]),</span>
<span class="c1">#                                       torch.load(dust_valid_paths[2]),</span>
<span class="c1">#                                       torch.load(metadata_times_valid_paths[2]))</span>
<span class="c1"># train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=dust_prediction_collate)</span>
<span class="c1"># valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True,collate_fn=dust_prediction_collate)</span>

<span class="c1"># sample_data = next(iter(train_loader))</span>
<span class="c1"># print(&quot;Sample data loading:&quot;)</span>
<span class="c1"># print(sample_data[0][0].shape, sample_data[0][1].shape, len(sample_data[1]))</span>

<span class="c1"># model_split3 = ViT.VisionTransformer(img_size=(81,81), patch_size=(9,9), in_chans=17, num_classes=10, embed_dim=512, </span>
<span class="c1">#                  depth=8, num_heads=8, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,</span>
<span class="c1">#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.)</span>
<span class="c1"># model_split3 = model_split3.to(device)</span>

<span class="c1"># criterion = nn.MSELoss() # to be used inside the dust_loss</span>
<span class="c1"># lr = 0.0001</span>
<span class="c1"># optimizer = torch.optim.Adam(model_split3.parameters(), lr=lr)</span>
<span class="c1"># num_epochs = 1</span>

<span class="c1"># train_losses_split3,valid_losses_split3 = train_loop(model_split3, optimizer, train_loader, valid_loader, </span>
<span class="c1">#                                                      device, epochs=num_epochs, valid_every=1,loss_cfg=None,</span>
<span class="c1">#                                                      sample_predictions_every=2, sample_size=5, sample_cols=[0],</span>
<span class="c1">#                                                      loss_plot_end=True)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # split4 - something is bad with the data - Ran out of input (train_dataset)</span>
<span class="c1"># train_dataset = DustPredictionDataset(torch.load(meteorology_train_paths[3]),</span>
<span class="c1">#                                       torch.load(dust_train_paths[3]),</span>
<span class="c1">#                                       torch.load(metadata_times_train_paths[3]))</span>
<span class="c1"># valid_dataset = DustPredictionDataset(torch.load(meteorology_valid_paths[3]),</span>
<span class="c1">#                                       torch.load(dust_valid_paths[3]),</span>
<span class="c1">#                                       torch.load(metadata_times_valid_paths[3]))</span>
<span class="c1"># train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=dust_prediction_collate)</span>
<span class="c1"># valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True,collate_fn=dust_prediction_collate)</span>

<span class="c1"># sample_data = next(iter(train_loader))</span>
<span class="c1"># print(&quot;Sample data loading:&quot;)</span>
<span class="c1"># print(sample_data[0][0].shape, sample_data[0][1].shape, len(sample_data[1]))</span>

<span class="c1"># model_split4 = ViT.VisionTransformer(img_size=(81,81), patch_size=(9,9), in_chans=17, num_classes=10, embed_dim=512, </span>
<span class="c1">#                  depth=8, num_heads=8, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,</span>
<span class="c1">#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.)</span>
<span class="c1"># model_split4 = model_split4.to(device)</span>

<span class="c1"># criterion = nn.MSELoss() # to be used inside the dust_loss</span>
<span class="c1"># lr = 0.001</span>
<span class="c1"># optimizer = torch.optim.Adam(model_split4.parameters(), lr=lr)</span>
<span class="c1"># num_epochs = 1</span>

<span class="c1"># train_losses_split4,valid_losses_split4 = train_loop(model_split4, optimizer, train_loader, valid_loader, </span>
<span class="c1">#                                                      device, epochs=num_epochs, valid_every=1,loss_cfg=None,</span>
<span class="c1">#                                                      sample_predictions_every=2, sample_size=5, sample_cols=[0],</span>
<span class="c1">#                                                      loss_plot_end=True)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split5</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split5</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">model_split5</span> <span class="o">=</span> <span class="n">model_split5</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split5</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">120</span>

<span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([64, 17, 81, 81]) torch.Size([64, 10]) 64
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 46.8000,  40.4000,  95.5000,  43.1667, 148.0000,  48.7167],
       dtype=torch.float64) events: tensor([[2],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 001 / 120   Loss: 1.727e+04   Precision: 39.060%   Recall: 58.771%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 002 / 120   Loss: 1.701e+04   Precision: 39.061%   Recall: 99.742%
Valid                   Loss: 2.15e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([16.4333, 49.0000, 19.1167, 58.7667, 40.0000, 27.3000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 003 / 120   Loss: 1.708e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.162e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 004 / 120   Loss: 1.681e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.147e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([50.5000,  7.9500, 38.3000, 66.5000, 13.0000, 34.0500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 005 / 120   Loss: 1.738e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.159e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 90.32067871 363.03333333]
	 [ 90.32067108  17.46666667]
	 [ 90.32067108  45.46666667]
	 [ 90.32067108  49.1       ]
	 [ 90.32068634  94.71666667]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([41.5000, 38.5000, 12.8333, 21.6667, 43.5500, 49.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 006 / 120   Loss: 1.691e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([44.6000, 26.6667, 37.6000, 51.8667, 28.0000, 66.1833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 007 / 120   Loss: 1.657e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.151e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 008 / 120   Loss: 1.679e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.149e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 009 / 120   Loss: 1.605e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 010 / 120   Loss: 1.605e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.164e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[94.91602325 34.2       ]
	 [94.91601562 24.        ]
	 [94.91602325 30.93333333]
	 [94.91602325 48.55      ]
	 [94.91601562  5.55      ]]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 75.0333,  22.6000,  20.0000,  32.5000, 952.4167,  45.0667],
       dtype=torch.float64) events: tensor([[0],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 011 / 120   Loss: 1.706e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([42.4667, 21.0000, 35.0667, 48.0000, 48.0000, 23.9500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 012 / 120   Loss: 1.648e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.166e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([68.9000, 23.5833, 65.7000, 54.2000, 25.2000, 25.8833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 013 / 120   Loss: 1.667e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([26.6667, 30.6167, 58.8667, 21.5000, 43.8333, 42.1333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 014 / 120   Loss: 1.692e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.16e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([26.7000, 29.3833, 28.8833, 20.4167, 28.4667, 62.9500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 015 / 120   Loss: 1.722e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.147e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 83.03730011  42.08333333]
	 [ 83.03731537   8.13333333]
	 [ 83.03730774  46.88333333]
	 [ 83.03730774  22.75      ]
	 [ 83.03730011 117.78333333]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([48.0000, 35.2833, 27.5500, 27.0833, 34.9833, 18.7833, 61.5500, 38.7500,
        54.8833, 50.2833, 43.7500, 20.0000, 49.2167, 17.0833, 16.4667, 28.2167,
        30.6500, 18.8500, 34.4167,  8.0000, 41.3333, 20.9167, 55.6333, 61.5500,
        39.8833, 47.0000, 49.7833, 24.8333, 68.0000, 30.5833, 37.5500, 16.7000,
        36.1000, 58.6833, 35.2167, 34.0000, 37.0000, 16.9667, 35.5500, 37.3333,
        40.8667, 39.3333, 35.0667, 20.6167, 22.6667, 27.5500, 24.0000, 42.0000,
        28.6000, 49.5000, 28.2167, 38.9000, 21.3333, 52.5500, 14.0000, 20.9167,
        19.9833, 25.5500, 22.4667,  8.0000, 39.3333, 47.0500, 12.4500, 51.7500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([42.9000, 29.8333, 54.0000, 24.8000, 71.4167, 61.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 016 / 120   Loss: 1.784e+04   Precision: 38.952%   Recall: 100.000%
Valid                   Loss: 2.151e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([88.3333, 37.0500, 23.3667, 20.2500, 25.5000, 30.5833],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 017 / 120   Loss: 1.698e+04   Precision: 39.057%   Recall: 100.000%
Valid                   Loss: 2.151e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 018 / 120   Loss: 1.663e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 61.8833,  52.3667,  52.2000,  87.3333, 124.9667,  91.2667],
       dtype=torch.float64) events: tensor([[3],
        [4],
        [5]]) torch.Size([3, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 019 / 120   Loss: 1.565e+04   Precision: 39.065%   Recall: 100.000%
Valid                   Loss: 2.159e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([28.4333, 28.0500, 19.8333, 93.0000, 30.7167, 75.0500],
       dtype=torch.float64) events: tensor([[3],
        [5]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 020 / 120   Loss: 1.666e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.152e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[85.21882629 30.33333333]
	 [85.21883392 12.5       ]
	 [85.21916962 90.36666667]
	 [85.21897125 26.33333333]
	 [85.21890259 29.8       ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([26.3833, 32.0333, 42.1000, 58.3667, 35.0000, 32.0000, 43.6667, 57.0000,
        24.6167, 48.6167, 17.6667, 42.0833, 66.1000, 46.0000, 38.4500, 47.3333,
        42.0000, 46.7833, 38.1000, 44.0833, 70.1667, 43.5500, 48.2000, 26.0000,
        25.3833, 31.1167, 35.0833, 20.7833, 28.6333, 49.5500, 31.6667, 30.4500,
        36.5500, 28.2833, 30.4833, 57.4500, 23.7500, 57.3333, 39.2000, 25.0833,
        41.7333, 36.4167, 48.0000, 31.3833, 27.6167, 38.7833, 31.9833, 50.5000,
        59.3500, 63.1167, 26.0000, 36.2000, 26.5833, 68.3833, 45.0000, 53.4500,
        44.8000, 29.1500, 13.1667, 62.3833, 24.2167, 31.5000, 33.6667, 42.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([29.9500, 18.9500, 29.0333, 25.5500, 30.1333, 30.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 021 / 120   Loss: 1.556e+04   Precision: 38.952%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 022 / 120   Loss: 1.811e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.163e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([31.6000, 36.6500, 62.2167, 11.0000, 39.1333, 23.7167, 51.2833, 53.5333,
        50.2833, 35.3333, 46.9833, 35.8833, 48.5000, 46.3333, 33.0500, 32.3000,
        21.3333, 28.7833, 35.1667, 24.8000, 38.2667, 16.1667, 55.2500, 49.5500,
        37.4500, 17.2500, 46.8833, 33.6333, 33.3333, 31.1667, 45.4667, 38.8000,
        16.3333, 24.1667, 45.5500, 26.4000, 39.1667, 31.8833, 34.0500, 17.0667,
        17.1333, 47.0000, 32.6833, 36.1333, 72.5333, 49.3167, 50.0833, 15.6667,
        34.8667, 34.3333, 64.4500, 68.8833, 34.6500, 48.5500, 16.6667, 29.6667,
        22.7167, 51.6500, 33.2167, 26.2667, 32.6333, 50.1667, 37.7167, 42.3833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([26.8333, 63.9833, 39.2000, 19.8833, 27.8667, 30.7167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 023 / 120   Loss: 1.72e+04   Precision: 38.952%   Recall: 100.000%
Valid                   Loss: 2.167e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([1529.8000,   38.7500,   29.8333,   70.2833,   33.7000,   55.2500],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 024 / 120   Loss: 1.66e+04   Precision: 39.057%   Recall: 100.000%
Valid                   Loss: 2.157e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 025 / 120   Loss: 1.679e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[87.58080292 27.        ]
	 [87.58100128 89.95      ]
	 [87.58033752 31.33333333]
	 [87.58029938 28.16666667]
	 [87.58075714 50.1       ]]
Train   Epoch: 026 / 120   Loss: 1.653e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.162e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 027 / 120   Loss: 1.805e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.158e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([46.2000, 41.7667, 50.7500, 35.8000, 25.1333, 33.1167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 028 / 120   Loss: 1.758e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 029 / 120   Loss: 1.577e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.159e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([283.9500,  29.8000,  42.4500,  45.8833, 466.7833,  38.0000],
       dtype=torch.float64) events: tensor([[0],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 030 / 120   Loss: 1.797e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.159e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[90.81845093 54.        ]
	 [90.81861877 57.53333333]
	 [90.81859589 36.3       ]
	 [90.81872559 34.75      ]
	 [90.81845856 25.5       ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([111.0000,   6.1667,  13.7333,  54.5333,  20.7167,  57.7667],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 031 / 120   Loss: 1.763e+04   Precision: 39.057%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([11.9333, 72.2167, 39.1167, 21.0333, 45.3333, 18.7500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 032 / 120   Loss: 1.723e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 033 / 120   Loss: 1.638e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([113.1667,  42.5500,  51.8333, 396.7500,  25.3333,  25.1167],
       dtype=torch.float64) events: tensor([[0],
        [3]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 034 / 120   Loss: 1.624e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([27.2500, 40.4333, 67.9167, 21.1833, 24.2667, 24.1167, 59.4000, 41.7167,
        33.7833, 25.0167, 19.4500, 30.0500, 24.7500, 49.1667, 46.1167, 29.8333,
        33.3667, 38.1833, 26.4000, 53.8833, 23.8833,  8.6667, 19.3333, 34.2500,
        36.0667, 38.8667, 33.2333, 11.5667, 31.9500, 41.3667, 55.7500, 42.8667,
        25.3333, 31.5500, 27.1167, 23.5000, 27.6667, 60.6667, 20.9833, 18.9333,
        17.5833, 43.0000, 18.5000, 55.2000, 33.3333, 45.8333, 27.0833, 55.9000,
        54.9833, 41.9667, 29.5500, 66.3000, 62.5333, 45.3833, 10.5500, 48.5000,
        25.7833, 17.2500, 27.7000, 29.0833, 22.2000, 29.8167, 23.5333, 44.4500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 035 / 120   Loss: 1.693e+04   Precision: 38.960%   Recall: 100.000%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[88.9956131  96.35      ]
	 [88.9954834  76.        ]
	 [88.99529266 48.13333333]
	 [88.99582672 44.5       ]
	 [88.99721527 47.45      ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([40.0333, 24.3500, 21.6667, 67.4000, 48.7500, 20.9833, 21.0000, 41.0000,
        33.9500, 14.0000, 59.8833, 49.5000, 29.1000, 33.8667, 30.0167, 55.6667,
        39.2000, 57.1333, 52.3667, 67.7167, 52.2667, 52.2667, 26.0833, 25.5167,
        11.7833, 28.2333, 35.0000, 22.1167, 52.2500, 32.0000, 27.1833, 16.9500,
        15.5000, 30.3000, 20.7833, 28.2167, 58.7000, 14.7500, 45.9333, 27.6667,
        52.4167, 35.1167, 34.2833, 17.8833, 29.8333, 42.8667, 28.6333, 20.3833,
        28.4500, 24.1333, 50.8833, 19.3833,  8.8333, 10.9667, 19.6667, 48.4167,
        57.1333, 32.4167, 24.4167, 28.7833, 43.0000, 36.1333, 47.3500, 51.9000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([191.2167,  78.7333, 115.8000,  25.7167,  35.7333,  31.8833],
       dtype=torch.float64) events: tensor([[0],
        [1],
        [2]]) torch.Size([3, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 036 / 120   Loss: 1.694e+04   Precision: 38.964%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([22.4500, 52.0000, 26.5000, 54.8000, 26.7000, 29.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 037 / 120   Loss: 1.645e+04   Precision: 39.065%   Recall: 99.794%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([48.4167, 24.3333, 40.3333, 34.6667, 31.7833, 17.0833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 038 / 120   Loss: 1.632e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.152e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([66.6667, 49.1167, 16.4167, 22.2500, 19.3833, 46.4667, 40.4500, 31.0833,
        29.3000, 45.8000, 30.2000, 30.6833, 46.2667, 51.1167, 28.6000, 70.4500,
        70.6667, 47.9500, 26.3000, 23.7167, 29.0333, 52.8833, 55.5833, 44.4500,
        63.6667, 17.5833,  5.2833, 25.0000, 60.5500, 31.9333, 35.3833, 42.5833,
        28.2167, 26.0000, 10.7833, 49.6167, 44.1667, 22.9500, 20.3333, 66.2667,
        25.6667, 51.0167, 39.7500, 37.6000, 34.7500, 65.9167, 45.7667, 18.4500,
        49.0333, 30.5167, 29.8000, 19.2500, 13.4500, 21.8667, 42.1167, 45.7000,
        42.5000, 29.6667, 36.0333, 37.0500, 48.6000, 48.0833, 59.9833, 24.1000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 039 / 120   Loss: 1.664e+04   Precision: 38.992%   Recall: 99.401%
Valid                   Loss: 2.165e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([  9.3833, 107.7667,  61.5500, 112.1667,  44.2833,  50.2000],
       dtype=torch.float64) events: tensor([[1],
        [3]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 040 / 120   Loss: 1.642e+04   Precision: 39.069%   Recall: 99.155%
Valid                   Loss: 2.136e+04   Precision: 14.776%   Recall: 78.593%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[73.41978455 48.28333333]
	 [72.66699219 59.95      ]
	 [73.5786438  33.93333333]
	 [74.78727722 39.88333333]
	 [73.48690033 23.53333333]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([21.6167, 59.1667, 32.7500, 46.3500, 57.4000, 37.2167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 041 / 120   Loss: 1.692e+04   Precision: 39.439%   Recall: 90.227%
Valid                   Loss: 2.157e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([13.0000, 20.9333, 13.4167,  6.3167, 48.8500, 24.7167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 042 / 120   Loss: 1.611e+04   Precision: 41.318%   Recall: 73.639%
Valid                   Loss: 2.127e+04   Precision: 19.902%   Recall: 36.255%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 27.6667, 122.9500,  21.0000,  37.4167,  23.8167, 175.4500],
       dtype=torch.float64) events: tensor([[1],
        [5]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 043 / 120   Loss: 1.583e+04   Precision: 42.878%   Recall: 61.090%
Valid                   Loss: 2.149e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([39.0167, 43.1833, 14.4667, 16.2833, 45.9167, 26.4500, 28.5833, 41.0000,
        22.0000, 38.6000, 23.9833, 33.2500, 39.0000, 70.8833, 38.3833, 54.8000,
        39.2167, 52.4167, 71.3333, 15.0000, 58.9000, 42.0333, 45.2833, 32.7167,
        13.6500, 64.7833, 12.4500, 45.8833, 34.1500, 31.2167, 35.7500, 33.4167,
        23.4500, 15.6667, 54.0833, 40.8000, 62.2833, 35.7500, 31.9333, 38.3333,
        25.1333, 32.3333, 51.1667, 60.8333, 53.2500, 50.0000, 31.2000, 28.8833,
        16.5000, 41.6167, 25.8833, 24.0000, 44.5833, 33.8333, 51.7500, 22.9000,
        53.6667, 39.3333, 18.4333, 29.5000, 37.9833, 59.5000, 18.5500, 41.0000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([21.8667, 33.1333, 32.0500, 31.0000, 15.8667, 18.7833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 044 / 120   Loss: 1.591e+04   Precision: 42.496%   Recall: 63.711%
Valid                   Loss: 2.106e+04   Precision: 27.715%   Recall: 17.651%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([30.8333, 24.0000, 26.2833, 23.6333, 62.5333, 15.4500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 045 / 120   Loss: 1.497e+04   Precision: 44.746%   Recall: 53.691%
Valid                   Loss: 2.13e+04   Precision: 14.428%   Recall: 94.693%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[78.53407288 35.88333333]
	 [75.88983917 38.        ]
	 [98.96215057 39.86666667]
	 [87.0280304  58.66666667]
	 [79.11746979 29.11666667]]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 40.9667,  44.6167,  53.8833, 274.9167,  83.2333,  51.0167],
       dtype=torch.float64) events: tensor([[3],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 046 / 120   Loss: 1.553e+04   Precision: 44.601%   Recall: 54.752%
Valid                   Loss: 2.131e+04   Precision: 14.582%   Recall: 94.275%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 18.3000,  35.2333,  82.0333,  74.6333,  56.3000, 109.1167],
       dtype=torch.float64) events: tensor([[2],
        [3],
        [5]]) torch.Size([3, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 047 / 120   Loss: 1.587e+04   Precision: 43.391%   Recall: 56.673%
Valid                   Loss: 2.189e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 28.4500, 140.3000, 143.7500,  46.7500,  15.2000, 103.5833],
       dtype=torch.float64) events: tensor([[1],
        [2],
        [5]]) torch.Size([3, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 048 / 120   Loss: 1.596e+04   Precision: 39.234%   Recall: 96.207%
Valid                   Loss: 2.16e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([25.9667, 45.1333, 20.2167, 39.3333, 50.8833, 41.0000, 21.3167, 31.1167,
        55.1667, 14.8333, 37.5833, 36.6000, 25.2167, 67.2167, 24.0000, 23.6333,
        15.2000, 68.6667, 22.9833, 51.5000, 37.6667, 27.9333, 27.2000, 20.4333,
        24.5500, 63.0667, 46.1500, 47.2500, 51.3333, 39.5000, 24.8333, 51.9167,
        73.1833, 52.4333, 40.1333, 39.3333, 17.5000, 16.9500, 32.1167, 37.3500,
        68.2833, 59.8833, 21.0000, 23.3333, 37.1667, 18.1500, 54.1667, 26.7833,
        37.6333, 70.4667, 35.3333, 22.7833, 32.1167, 55.2167, 54.6500, 44.5500,
        43.2167,  8.9167, 42.3000, 44.5667, 16.6000, 24.7333, 22.8167, 60.3333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([26.8667, 16.7500, 22.2333, 49.2833, 34.6667, 44.8333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 049 / 120   Loss: 1.667e+04   Precision: 38.996%   Recall: 99.752%
Valid                   Loss: 2.162e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([15.9000, 72.6833, 59.7167, 42.4000, 27.4167, 59.8833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 050 / 120   Loss: 1.631e+04   Precision: 39.349%   Recall: 96.969%
Valid                   Loss: 2.139e+04   Precision: 14.096%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[84.8844986  19.38333333]
	 [79.75370789 25.2       ]
	 [84.15715027 23.21666667]
	 [87.72798157 33.05      ]
	 [77.18489075 23.45      ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([45.2000, 32.4667, 53.3333, 31.3667, 56.6167, 23.8000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 051 / 120   Loss: 1.69e+04   Precision: 42.670%   Recall: 66.979%
Valid                   Loss: 2.094e+04   Precision: 22.727%   Recall: 20.274%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([51.0167, 33.9167, 22.6667, 21.8333, 11.4167, 29.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 052 / 120   Loss: 1.548e+04   Precision: 44.424%   Recall: 51.495%
Valid                   Loss: 2.091e+04   Precision: 22.272%   Recall: 29.934%
Train   Epoch: 053 / 120   Loss: 1.584e+04   Precision: 45.741%   Recall: 52.577%
Valid                   Loss: 2.096e+04   Precision: 20.793%   Recall: 42.218%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([41.0000, 35.6000, 15.4167, 29.0000, 64.8167, 52.2000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 054 / 120   Loss: 1.508e+04   Precision: 44.342%   Recall: 57.649%
Valid                   Loss: 2.094e+04   Precision: 22.891%   Recall: 22.004%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([93.9833, 39.6833, 27.8400, 22.7833, 32.9333, 31.0000],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 055 / 120   Loss: 1.555e+04   Precision: 46.805%   Recall: 56.324%
Valid                   Loss: 2.181e+04   Precision: 15.863%   Recall: 81.038%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[82.84958649 34.5       ]
	 [67.46888733 25.33333333]
	 [81.78450775 65.55      ]
	 [83.27458191 36.06666667]
	 [74.15441895 29.05      ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([60.6500, 48.3333, 52.8333, 23.6333, 32.7667, 46.7167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 056 / 120   Loss: 1.537e+04   Precision: 45.720%   Recall: 50.928%
Valid                   Loss: 2.115e+04   Precision: 22.003%   Recall: 37.865%
Train   Epoch: 057 / 120   Loss: 1.498e+04   Precision: 45.502%   Recall: 54.535%
Valid                   Loss: 2.098e+04   Precision: 21.762%   Recall: 32.260%
Train   Epoch: 058 / 120   Loss: 1.555e+04   Precision: 41.128%   Recall: 72.676%
Valid                   Loss: 2.149e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([37.6000, 29.4500, 72.4167, 19.2667, 53.6333, 34.5500, 30.4833, 17.9333,
        31.5500, 15.9500, 41.0000, 37.8333, 17.3333, 46.3833, 39.2000, 28.1333,
        19.6167, 32.7500, 44.7500, 31.5667,  8.5833, 24.3667, 61.0500, 40.9167,
        45.8333, 70.3333, 19.1833, 33.8833, 61.3333, 58.3667, 38.0000, 33.0000,
        20.5000, 24.0333, 14.3833, 47.2500, 46.9167, 30.2833, 39.0167, 44.5333,
        24.7000, 44.0000, 16.5167, 22.5500, 45.7500, 36.8333, 43.7833, 25.6333,
        23.2167, 27.0000, 55.5333, 12.3333, 52.5000,  9.4500, 40.4333, 21.9667,
        67.0500, 35.2667, 57.5000, 24.7167, 63.3667, 16.8333, 61.0000, 31.9333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 059 / 120   Loss: 1.513e+04   Precision: 39.647%   Recall: 79.167%
Valid                   Loss: 2.138e+04   Precision: 14.092%   Recall: 91.234%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([10.0000, 37.8667, 25.0000, 57.2000, 10.1667, 61.7833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 060 / 120   Loss: 1.521e+04   Precision: 43.280%   Recall: 52.918%
Valid                   Loss: 2.194e+04   Precision: 18.715%   Recall: 39.595%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[75.17196655 25.45      ]
	 [76.17193604 59.83333333]
	 [73.19400024 73.33333333]
	 [78.79695129 44.        ]
	 [58.72170258 49.18333333]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([30.4833, 17.4167, 38.8833, 10.9000, 33.6833, 69.9667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 061 / 120   Loss: 1.57e+04   Precision: 39.729%   Recall: 79.237%
Valid                   Loss: 2.174e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([418.4167,  75.7500,  23.5833,  18.9167,  18.9167, 106.3333],
       dtype=torch.float64) events: tensor([[0],
        [1],
        [5]]) torch.Size([3, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 062 / 120   Loss: 1.67e+04   Precision: 42.421%   Recall: 62.383%
Valid                   Loss: 2.201e+04   Precision: 14.283%   Recall: 93.739%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([37.3333, 42.0000, 68.3333, 40.4000, 34.3333, 37.3333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 063 / 120   Loss: 1.528e+04   Precision: 42.766%   Recall: 56.165%
Valid                   Loss: 2.115e+04   Precision: 16.777%   Recall: 53.131%
Train   Epoch: 064 / 120   Loss: 1.629e+04   Precision: 43.619%   Recall: 54.113%
Valid                   Loss: 2.149e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([19.0000, 40.3667,  6.7333, 43.1500, 65.5833, 23.2167, 39.6667, 38.8500,
        36.0500, 66.8000, 45.3333, 54.0000, 50.0000, 28.7833, 37.4500, 34.7500,
        57.5333, 12.3333, 43.7000, 18.6667, 16.3667, 28.8667, 36.5167, 35.7500,
        55.2500, 57.5000, 44.7167, 34.7500, 48.9667, 19.0000, 28.3333, 32.7500,
        22.7500, 31.2500, 14.0000, 51.3333, 48.7833, 18.1667, 28.6000, 33.4833,
        16.6667, 26.3833, 50.0667, 38.2833, 24.6500, 37.9667, 61.3167, 27.6667,
        30.5500, 36.8000, 33.7833,  7.2167, 12.8833, 32.5000, 37.5667, 27.8000,
        51.1667, 35.5500, 29.3333, 46.1500, 35.3333, 38.1000, 14.3333, 48.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([50.9333, 15.4833, 18.6000, 57.5500, 21.0000, 33.1167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 065 / 120   Loss: 1.497e+04   Precision: 43.384%   Recall: 55.814%
Valid                   Loss: 2.131e+04   Precision: 19.175%   Recall: 41.324%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 68.84146118 133.93333333]
	 [ 74.08072662  20.88333333]
	 [ 72.65219116  28.5       ]
	 [ 69.59250641  58.66666667]
	 [ 56.23230362  31.95      ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([26.0667, 71.9500, 19.3333, 46.1500, 18.3333, 24.1667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 066 / 120   Loss: 1.554e+04   Precision: 44.662%   Recall: 53.216%
Valid                   Loss: 2.125e+04   Precision: 21.206%   Recall: 33.333%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([19.5833, 42.2000, 28.5000, 32.5500, 25.6000, 14.3500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 067 / 120   Loss: 1.653e+04   Precision: 42.659%   Recall: 67.639%
Valid                   Loss: 2.12e+04   Precision: 14.036%   Recall: 97.853%
Train   Epoch: 068 / 120   Loss: 1.607e+04   Precision: 40.249%   Recall: 83.199%
Valid                   Loss: 2.159e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([10.6667, 39.9000, 65.0000, 17.5000, 16.9500, 50.3333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 069 / 120   Loss: 1.839e+04   Precision: 39.396%   Recall: 96.351%
Valid                   Loss: 2.136e+04   Precision: 15.735%   Recall: 77.221%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([242.6000,  31.9500,  63.7833, 113.0833,  26.5833,  92.0000],
       dtype=torch.float64) events: tensor([[0],
        [3],
        [5]]) torch.Size([3, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 070 / 120   Loss: 1.619e+04   Precision: 40.435%   Recall: 80.583%
Valid                   Loss: 2.221e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[113.20782471  25.28333333]
	 [118.77996063  55.28333333]
	 [152.51519775  44.15      ]
	 [ 96.34049225  26.63333333]
	 [120.17449951  38.3       ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([30.9500, 48.0000, 43.7000, 35.5667, 31.2500, 37.9000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 071 / 120   Loss: 1.587e+04   Precision: 41.193%   Recall: 74.619%
Valid                   Loss: 2.137e+04   Precision: 15.233%   Recall: 86.047%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([76.3333, 69.9500, 25.1000, 55.6667, 25.3833, 43.4500],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 072 / 120   Loss: 1.551e+04   Precision: 45.554%   Recall: 55.922%
Valid                   Loss: 2.167e+04   Precision: 14.796%   Recall: 97.018%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([95.8833, 41.5833, 24.5833, 20.6333, 10.5000, 23.0500, 45.9500, 43.6667,
        33.0500, 53.2167,  8.2167, 63.2500, 28.7167, 48.4667, 22.3333, 55.4500,
        10.2167, 51.2333, 29.3667,  5.1167, 31.6167, 28.2500, 30.0000, 67.7833,
        23.2500, 25.0000, 41.6500, 34.2500, 25.7000, 35.0000, 55.0000, 28.9000,
        43.9667, 43.3500, 47.4667, 52.5000, 32.6833, 23.3667, 34.3333, 55.8333,
        53.7833, 42.5000, 45.6667, 30.6333, 41.8333, 36.4000, 38.5833, 40.6167,
        60.2500, 31.1167, 39.5500, 17.5833, 41.4833, 35.8333, 38.4500, 35.5500,
        24.0000, 29.8400, 54.1167, 64.6167, 44.4167, 19.0000, 63.8333, 18.2833],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 073 / 120   Loss: 1.559e+04   Precision: 45.514%   Recall: 51.788%
Valid                   Loss: 2.092e+04   Precision: 21.794%   Recall: 34.049%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([25.5000, 53.9333, 51.5000, 28.2500, 44.3167, 47.3167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 074 / 120   Loss: 1.494e+04   Precision: 45.330%   Recall: 54.742%
Valid                   Loss: 2.09e+04   Precision: 22.065%   Recall: 31.604%
Train   Epoch: 075 / 120   Loss: 1.448e+04   Precision: 45.444%   Recall: 53.669%
Valid                   Loss: 2.098e+04   Precision: 17.923%   Recall: 58.974%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[67.75675201 45.46666667]
	 [75.34648895 36.        ]
	 [63.69741821 34.45      ]
	 [74.68335724 26.        ]
	 [69.42605591 49.33333333]]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([52.0833, 75.9167, 79.0000, 35.5000, 32.3333,  8.0833],
       dtype=torch.float64) events: tensor([[1],
        [2]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 076 / 120   Loss: 1.61e+04   Precision: 46.694%   Recall: 51.969%
Valid                   Loss: 2.121e+04   Precision: 14.578%   Recall: 98.927%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([55.4500, 31.9333, 31.9500, 19.5833, 30.4167, 45.4167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 077 / 120   Loss: 1.511e+04   Precision: 47.944%   Recall: 51.938%
Valid                   Loss: 2.078e+04   Precision: 24.065%   Recall: 18.426%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([29.4333, 21.7167, 48.3333, 65.5000, 72.6667, 35.9667, 62.7333, 18.4667,
        17.2833, 11.7333,  9.8333, 14.8333, 32.3500, 20.6000, 54.3833, 22.0333,
        34.8500, 59.9167, 28.0500, 72.4667, 22.4500, 18.0000, 61.9500, 12.0000,
        23.7333, 38.7667, 39.9667, 47.4500, 24.2000,  8.2500, 25.9833, 46.4333,
        55.9333, 17.7833, 39.1167, 48.2500, 62.6167, 12.9500, 44.0500, 68.7333,
        18.4167, 25.6667, 41.0500, 38.0000, 25.2167, 40.5000, 23.1333, 39.8000,
        19.8333, 35.9833, 50.4667, 66.6167, 46.4500, 68.0667, 37.7400, 14.7500,
        42.0500, 44.6833, 21.0000, 29.0667, 57.7833, 38.9167, 12.0000, 58.8833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 078 / 120   Loss: 1.527e+04   Precision: 47.956%   Recall: 51.390%
Valid                   Loss: 2.084e+04   Precision: 23.651%   Recall: 25.880%
Train   Epoch: 079 / 120   Loss: 1.533e+04   Precision: 45.876%   Recall: 58.936%
Valid                   Loss: 2.079e+04   Precision: 24.258%   Recall: 22.421%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([59.1167, 25.9333, 18.3000, 41.8833, 73.0000, 18.4833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 080 / 120   Loss: 1.55e+04   Precision: 41.404%   Recall: 75.134%
Valid                   Loss: 2.141e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[80.58088684 51.46666667]
	 [81.64473724  4.91666667]
	 [82.56673431 52.        ]
	 [79.28855896 55.45      ]
	 [80.86122131 38.93333333]]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 41.2000, 370.5000,  36.9667,  79.9500,  36.6333,  27.2500],
       dtype=torch.float64) events: tensor([[1],
        [3]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 081 / 120   Loss: 1.527e+04   Precision: 42.872%   Recall: 65.925%
Valid                   Loss: 2.133e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 082 / 120   Loss: 1.584e+04   Precision: 46.270%   Recall: 54.855%
Valid                   Loss: 2.092e+04   Precision: 17.108%   Recall: 75.552%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([35.0000, 28.5500, 19.5000, 61.1167, 66.7833, 24.9667, 19.7500, 63.5000,
        16.8000, 41.2833, 37.6333, 18.1667, 63.6500, 17.4667, 58.3167, 47.0500,
        31.2333, 42.1667, 66.3000, 68.3667, 60.6667, 52.9500, 52.1167, 16.9500,
        66.0333, 25.1667, 32.7833, 31.7167, 41.1167, 39.5833, 31.3500, 26.5833,
        35.6500, 32.0167, 26.5500, 16.9667, 30.9333, 39.5333, 34.5000, 12.5167,
        49.8333, 24.5500, 35.7500, 48.0000, 27.6667, 30.7833, 26.1500, 43.5833,
        18.0667, 29.9000, 35.6333, 37.3333, 26.2833, 32.2500, 30.7833, 58.4500,
        41.1167, 26.3333, 46.9500, 49.5167, 41.4667, 16.3833, 36.6667, 53.1833],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([11.0000, 17.0000, 17.0000, 60.5500, 26.6667, 61.0000, 51.3333, 48.3167,
        44.6167, 48.8667, 72.5000, 29.0000, 40.4500, 16.4000, 37.0500, 30.0167,
        65.9667, 51.3333, 51.9000, 21.1000, 57.4667,  3.0000, 30.5000, 26.1667,
        20.4500, 65.6000, 36.7500, 56.5333, 17.0000, 19.7833, 50.8333, 25.6833,
        28.6000, 35.5833, 60.9250, 22.2333, 50.1000, 35.3333, 24.9500, 31.2833,
        27.3167, 31.0500, 43.2500, 42.2833, 22.2500, 15.3333, 26.4000, 60.3333,
        24.9833, 17.8333, 59.6667, 15.1333, 26.8667, 24.1167, 25.5000, 30.4167,
        35.5833, 70.5500, 16.5167, 24.1667, 53.7500, 70.5500, 66.4000, 63.0667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 083 / 120   Loss: 1.517e+04   Precision: 44.400%   Recall: 60.578%
Valid                   Loss: 2.091e+04   Precision: 25.413%   Recall: 16.518%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([15.4167, 49.0000, 23.8333, 13.4167, 10.0000, 45.6000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 084 / 120   Loss: 1.681e+04   Precision: 45.665%   Recall: 61.680%
Valid                   Loss: 2.098e+04   Precision: 16.182%   Recall: 82.588%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([40.3333, 56.4500, 20.4667, 28.2167, 70.6333, 32.9000, 27.0000, 49.8167,
        33.4167, 52.4500, 21.3667, 26.2167, 45.1167, 11.2667, 29.1167, 16.4667,
        57.9833, 15.6667, 46.7167, 24.4667,  8.0000, 41.1167, 24.3333, 56.2833,
        47.9000, 26.7833, 16.5333, 25.6333, 20.7167, 10.8667, 32.8333, 47.5000,
        56.2167, 14.3333, 54.5000, 16.8333, 14.0833, 21.1333, 23.5333, 44.7833,
        44.4500, 53.6000,  8.7833, 39.6667, 42.3833, 61.8833, 40.0000,  6.1667,
        41.0000, 27.0000, 38.3000, 41.2500, 22.4500, 48.4500, 23.0333, 46.3833,
        27.7000, 64.8333, 36.5167, 22.9833, 43.9167, 31.7833,  6.5833, 39.0000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([12.7833, 24.1333, 60.6667, 42.2667, 46.3333, 18.6167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 085 / 120   Loss: 1.53e+04   Precision: 46.283%   Recall: 57.654%
Valid                   Loss: 2.086e+04   Precision: 22.963%   Recall: 32.439%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[59.45455933 16.45      ]
	 [67.93612671 34.33333333]
	 [72.01046753 71.45      ]
	 [69.83230591 13.        ]
	 [73.05039215 27.2       ]]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([23.1667, 94.6333, 89.8167, 54.8833, 55.9333, 31.0000],
       dtype=torch.float64) events: tensor([[1],
        [2]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 086 / 120   Loss: 1.472e+04   Precision: 47.635%   Recall: 51.897%
Valid                   Loss: 2.113e+04   Precision: 16.992%   Recall: 78.891%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([44.0500, 39.3000, 67.6333, 36.0000, 20.9667, 48.4500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 087 / 120   Loss: 1.499e+04   Precision: 46.046%   Recall: 60.691%
Valid                   Loss: 2.138e+04   Precision: 14.839%   Recall: 93.262%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([27.0500, 27.3167, 19.3333, 22.5000, 26.0000, 23.3667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 088 / 120   Loss: 1.622e+04   Precision: 44.549%   Recall: 64.155%
Valid                   Loss: 2.099e+04   Precision: 35.455%   Recall: 9.302%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([24.1333, 34.2000, 48.3333, 48.2833, 39.5000, 31.6667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 089 / 120   Loss: 1.609e+04   Precision: 45.270%   Recall: 63.103%
Valid                   Loss: 2.095e+04   Precision: 19.876%   Recall: 66.846%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([45.8000, 20.2500, 69.3333, 28.0167, 23.7167, 48.1333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 090 / 120   Loss: 1.613e+04   Precision: 44.545%   Recall: 62.175%
Valid                   Loss: 2.093e+04   Precision: 23.607%   Recall: 33.095%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[97.22655487 54.33333333]
	 [63.51537323 27.25      ]
	 [55.55612183 49.26666667]
	 [69.28772736 28.13333333]
	 [72.99209595 31.75      ]]
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 33.2167,  24.4667, 127.5500,  90.6667,  41.0000,  36.9167],
       dtype=torch.float64) events: tensor([[2],
        [3]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 091 / 120   Loss: 1.536e+04   Precision: 47.319%   Recall: 53.494%
Valid                   Loss: 2.092e+04   Precision: 25.270%   Recall: 33.512%
Train   Epoch: 092 / 120   Loss: 1.632e+04   Precision: 45.738%   Recall: 63.709%
Valid                   Loss: 2.124e+04   Precision: 14.096%   Recall: 99.404%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([28.0000, 58.2667, 61.8833, 38.1833, 30.4500, 44.1167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 093 / 120   Loss: 1.446e+04   Precision: 46.700%   Recall: 56.897%
Valid                   Loss: 2.102e+04   Precision: 26.575%   Recall: 19.618%
Train   Epoch: 094 / 120   Loss: 1.615e+04   Precision: 44.512%   Recall: 56.926%
Valid                   Loss: 2.09e+04   Precision: 18.782%   Recall: 43.411%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([25.8333, 41.2667, 42.6333, 40.5500, 35.0000, 27.4500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 095 / 120   Loss: 1.493e+04   Precision: 44.649%   Recall: 56.515%
Valid                   Loss: 2.106e+04   Precision: 14.129%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 84.30552673  33.23333333]
	 [ 88.60678864  80.15      ]
	 [ 80.34325409  32.        ]
	 [ 81.61126709  24.26666667]
	 [129.96191406  22.41666667]]
Train   Epoch: 096 / 120   Loss: 1.53e+04   Precision: 44.689%   Recall: 55.679%
Valid                   Loss: 2.079e+04   Precision: 25.104%   Recall: 17.949%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([  15.5500,   38.4500,   41.3000, 1052.6000,  203.0500,   30.5000],
       dtype=torch.float64) events: tensor([[3],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 097 / 120   Loss: 1.63e+04   Precision: 44.188%   Recall: 60.730%
Valid                   Loss: 2.11e+04   Precision: 17.931%   Recall: 60.048%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([35.1333, 46.3833, 25.2833, 52.3667, 21.5000, 55.5000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 098 / 120   Loss: 1.461e+04   Precision: 44.824%   Recall: 56.649%
Valid                   Loss: 2.093e+04   Precision: 22.164%   Recall: 30.054%
Train   Epoch: 099 / 120   Loss: 1.566e+04   Precision: 46.097%   Recall: 54.051%
Valid                   Loss: 2.118e+04   Precision: 15.270%   Recall: 92.367%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([127.9167,  76.6333,  15.4167,  42.1167,  61.4833,  37.5000],
       dtype=torch.float64) events: tensor([[0],
        [1]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 100 / 120   Loss: 1.514e+04   Precision: 44.778%   Recall: 53.958%
Valid                   Loss: 2.136e+04   Precision: 14.337%   Recall: 99.225%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[87.94568634 17.8       ]
	 [88.00468445 51.        ]
	 [78.30937195 58.        ]
	 [80.54267883 14.9       ]
	 [80.89324188 42.2       ]]
Train   Epoch: 101 / 120   Loss: 1.624e+04   Precision: 42.866%   Recall: 67.409%
Valid                   Loss: 2.1e+04   Precision: 23.426%   Recall: 29.517%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 37.8667,  23.1667,  90.8833, 226.0000,  25.0833,  68.1000],
       dtype=torch.float64) events: tensor([[2],
        [3]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 102 / 120   Loss: 1.482e+04   Precision: 47.212%   Recall: 55.329%
Valid                   Loss: 2.096e+04   Precision: 21.138%   Recall: 35.003%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([52.0000, 48.3333, 57.9500, 46.5000, 19.2167, 30.8333],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 103 / 120   Loss: 1.504e+04   Precision: 44.696%   Recall: 59.330%
Valid                   Loss: 2.079e+04   Precision: 24.529%   Recall: 24.866%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([35.7333, 37.0000, 56.6667, 25.8333, 57.6667, 59.8167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 104 / 120   Loss: 1.607e+04   Precision: 43.511%   Recall: 57.134%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([55.0000, 53.2500, 52.7833, 39.5000, 54.7500, 61.6000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 105 / 120   Loss: 1.651e+04   Precision: 39.405%   Recall: 97.062%
Valid                   Loss: 2.144e+04   Precision: 14.009%   Recall: 98.748%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[82.51683807 30.38333333]
	 [82.56374359 80.33333333]
	 [74.72662354  9.33333333]
	 [84.62509155 60.16666667]
	 [82.89337158 23.        ]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([16.3833,  9.1500, 42.9333, 42.6333, 22.8000, 29.5000],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 106 / 120   Loss: 1.679e+04   Precision: 39.456%   Recall: 96.371%
Valid                   Loss: 2.093e+04   Precision: 26.965%   Recall: 14.729%
Train   Epoch: 107 / 120   Loss: 1.621e+04   Precision: 44.094%   Recall: 64.677%
Valid                   Loss: 2.089e+04   Precision: 24.596%   Recall: 15.444%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([ 32.3000,  12.4000,  30.0500, 504.8000, 264.9667,  17.6667],
       dtype=torch.float64) events: tensor([[3],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 108 / 120   Loss: 1.542e+04   Precision: 47.783%   Recall: 54.649%
Valid                   Loss: 2.108e+04   Precision: 16.988%   Recall: 68.277%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([34.9333, 23.0500, 16.7167, 30.2167, 34.8000, 51.4167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 109 / 120   Loss: 1.553e+04   Precision: 44.763%   Recall: 65.608%
Valid                   Loss: 2.15e+04   Precision: 14.059%   Recall: 98.688%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([57.5000, 27.4500, 65.1167, 18.0000, 27.9000, 52.5667, 29.9333, 23.1167,
        37.9333, 34.6167, 25.3833, 48.6667, 54.0000, 72.7833, 41.2500, 32.3833,
        71.1667, 22.2167, 39.3000, 61.3833, 47.3167, 33.6667,  9.6667, 17.0833,
        63.0000, 36.7500, 31.4333, 20.6667, 21.8667, 23.6333, 19.2000, 41.8833,
        35.7000, 31.0000, 48.4167, 16.3000, 40.0833, 32.5500, 29.6167, 30.8667,
        33.2167, 24.0000, 16.4500, 26.6167, 28.0000, 24.0000, 33.0833, 31.7833,
        32.5000, 42.8833, 27.0000, 56.4500,  9.0000, 48.3333, 16.3333, 45.5500,
        26.9000, 28.2667, 33.7500, 16.9167, 47.7833, 24.5833, 46.2833, 30.2500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 25 num_events_to_populate = final_num_events-events_idxs.shape[0]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([64.0000, 39.5500, 48.0000, 60.5500, 47.0000, 48.6167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 110 / 120   Loss: 1.642e+04   Precision: 43.942%   Recall: 67.059%
Valid                   Loss: 2.102e+04   Precision: 21.318%   Recall: 37.806%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[43.48136139 40.5       ]
	 [55.33772659 47.08333333]
	 [39.41418076 66.5       ]
	 [67.18364716 25.41666667]
	 [79.97497559 18.43333333]]
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([35.5833, 23.0333, 16.2167, 25.4833, 63.9833, 47.2500],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 111 / 120   Loss: 1.6e+04   Precision: 46.939%   Recall: 63.876%
Valid                   Loss: 2.114e+04   Precision: 18.406%   Recall: 53.429%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([85.8833, 33.7500, 31.5833, 20.4000, 39.0500, 29.7667],
       dtype=torch.float64) events: tensor([[0]]) torch.Size([1, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 112 / 120   Loss: 1.63e+04   Precision: 48.647%   Recall: 59.654%
Valid                   Loss: 2.108e+04   Precision: 16.991%   Recall: 66.667%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([155.7167,  35.9167,  45.3333,  27.0000, 312.0000,  36.3333],
       dtype=torch.float64) events: tensor([[0],
        [4]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 113 / 120   Loss: 1.554e+04   Precision: 47.996%   Recall: 55.288%
Valid                   Loss: 2.09e+04   Precision: 19.375%   Recall: 43.590%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([51.2167, 71.8333, 21.9500, 47.1000, 28.8833, 36.8667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 114 / 120   Loss: 1.602e+04   Precision: 47.416%   Recall: 60.052%
Valid                   Loss: 2.164e+04   Precision: 14.748%   Recall: 88.312%
Train   Epoch: 115 / 120   Loss: 1.524e+04   Precision: 49.067%   Recall: 54.216%
Valid                   Loss: 2.101e+04   Precision: 25.102%   Recall: 14.609%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 52.92291641  33.98333333]
	 [ 37.78826141  23.33333333]
	 [ 42.07906342  38.11666667]
	 [ 43.74726868  17.7       ]
	 [ 45.64295959 114.55      ]]
Train   Epoch: 116 / 120   Loss: 1.53e+04   Precision: 46.524%   Recall: 58.009%
Valid                   Loss: 2.078e+04   Precision: 20.000%   Recall: 38.521%
Train   Epoch: 117 / 120   Loss: 1.603e+04   Precision: 44.226%   Recall: 69.470%
Valid                   Loss: 2.109e+04   Precision: 21.543%   Recall: 36.971%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([56.4667, 59.1167, 23.4500, 13.1333, 34.8167, 56.5667],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 118 / 120   Loss: 1.502e+04   Precision: 48.880%   Recall: 52.876%
Valid                   Loss: 2.084e+04   Precision: 19.477%   Recall: 50.209%
Could not perform importance sampling for this batch:
cannot sample n_sample &lt;= 0 samples
dust: tensor([306.1167,  30.5000,  87.0833,  23.1167,  36.6833,  41.0333],
       dtype=torch.float64) events: tensor([[0],
        [2]]) torch.Size([2, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 119 / 120   Loss: 1.517e+04   Precision: 45.410%   Recall: 57.308%
Valid                   Loss: 2.121e+04   Precision: 14.314%   Recall: 95.528%
Could not perform importance sampling for this batch:
invalid multinomial distribution (sum of probabilities &lt;= 0)
dust: tensor([31.2000, 33.4667, 32.6333, 10.8833, 47.3500, 55.6167],
       dtype=torch.float64) events: tensor([], size=(0, 1), dtype=torch.int64) torch.Size([0, 1])
final_num_events: 2 num_events_to_populate = final_num_events-events_idxs.shape[0]
Train   Epoch: 120 / 120   Loss: 1.606e+04   Precision: 46.534%   Recall: 57.928%
Valid                   Loss: 2.198e+04   Precision: 15.088%   Recall: 90.340%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 72.10689545  39.        ]
	 [ 77.81459808  11.        ]
	 [ 82.48201752 129.65      ]
	 [ 84.26207733  60.95      ]
	 [ 84.40684509  24.41666667]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split5 - different lr, more epchs</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split5</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">model_split5</span> <span class="o">=</span> <span class="n">model_split5</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split5</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>

<span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([64, 17, 81, 81]) torch.Size([64, 10]) 64
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 600   Loss: 1.993e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 2.235e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 600   Loss: 1.726e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 2.15e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 600   Loss: 1.73e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 2.132e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 600   Loss: 1.699e+04   Precision: 39.057%   Recall: 26.046%
Valid                   Loss: 2.139e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 005 / 600   Loss: 1.737e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.146e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 80.81092834 105.6       ]
	 [ 80.81079865  50.95      ]
	 [ 80.81100464  30.        ]
	 [ 80.81093597  53.        ]
	 [ 80.81075287  23.33333333]]
Train   Epoch: 006 / 600   Loss: 1.573e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.15e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 007 / 600   Loss: 1.664e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 008 / 600   Loss: 1.688e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 009 / 600   Loss: 1.556e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 010 / 600   Loss: 1.611e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[86.57823944 58.68333333]
	 [86.57827759 41.        ]
	 [86.57835388 33.43333333]
	 [86.5783844  32.91666667]
	 [86.57836914 13.16666667]]
Train   Epoch: 011 / 600   Loss: 1.571e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 012 / 600   Loss: 1.645e+04   Precision: 39.061%   Recall: 99.742%
Valid                   Loss: 2.151e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 013 / 600   Loss: 1.72e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.157e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 014 / 600   Loss: 1.684e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 015 / 600   Loss: 1.645e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[87.25939941 35.66666667]
	 [87.25968933 40.4       ]
	 [87.25957489 13.        ]
	 [87.25935364 25.03333333]
	 [87.25943756 16.68333333]]
Train   Epoch: 016 / 600   Loss: 1.668e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 017 / 600   Loss: 1.682e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 018 / 600   Loss: 1.612e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.152e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 019 / 600   Loss: 1.643e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 020 / 600   Loss: 1.63e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[87.49775696 22.03333333]
	 [87.49749756 22.78333333]
	 [87.49765015 27.88333333]
	 [87.49772644 34.66666667]
	 [87.49871826 76.88333333]]
Train   Epoch: 021 / 600   Loss: 1.757e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.158e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 022 / 600   Loss: 1.584e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.155e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 023 / 600   Loss: 1.64e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.152e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 024 / 600   Loss: 1.648e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.156e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 025 / 600   Loss: 1.659e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.154e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[86.58384705 45.66666667]
	 [86.58067322 29.58333333]
	 [86.58193207 27.01666667]
	 [86.59384918 39.66666667]
	 [86.59850311 69.        ]]
Train   Epoch: 026 / 600   Loss: 1.618e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 027 / 600   Loss: 1.626e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.15e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 028 / 600   Loss: 1.587e+04   Precision: 39.079%   Recall: 99.784%
Valid                   Loss: 2.153e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 029 / 600   Loss: 1.653e+04   Precision: 39.045%   Recall: 99.918%
Valid                   Loss: 2.151e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 030 / 600   Loss: 1.619e+04   Precision: 39.061%   Recall: 100.000%
Valid                   Loss: 2.147e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 80.70335388  32.88333333]
	 [ 83.90019989  26.45      ]
	 [ 83.24265289  40.11666667]
	 [ 82.55373383  51.35      ]
	 [ 81.28759003 226.13333333]]
Train   Epoch: 031 / 600   Loss: 1.653e+04   Precision: 39.487%   Recall: 89.289%
Valid                   Loss: 2.138e+04   Precision: 13.858%   Recall: 85.808%
Train   Epoch: 032 / 600   Loss: 1.593e+04   Precision: 40.785%   Recall: 65.381%
Valid                   Loss: 2.132e+04   Precision: 16.499%   Recall: 56.768%
Train   Epoch: 033 / 600   Loss: 1.603e+04   Precision: 39.420%   Recall: 92.381%
Valid                   Loss: 2.161e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 034 / 600   Loss: 1.73e+04   Precision: 39.053%   Recall: 100.000%
Valid                   Loss: 2.143e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 035 / 600   Loss: 1.641e+04   Precision: 39.448%   Recall: 97.124%
Valid                   Loss: 2.144e+04   Precision: 14.085%   Recall: 100.000%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[100.37918854  28.61666667]
	 [ 86.90566254  18.05      ]
	 [ 83.98579407  28.41666667]
	 [ 89.91711426  24.83333333]
	 [ 84.41886139  30.95      ]]
Train   Epoch: 036 / 600   Loss: 1.666e+04   Precision: 40.617%   Recall: 87.268%
Valid                   Loss: 2.135e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 037 / 600   Loss: 1.636e+04   Precision: 45.084%   Recall: 72.645%
Valid                   Loss: 2.126e+04   Precision: 20.431%   Recall: 79.129%
Train   Epoch: 038 / 600   Loss: 1.566e+04   Precision: 47.372%   Recall: 65.121%
Valid                   Loss: 2.119e+04   Precision: 24.464%   Recall: 41.503%
Train   Epoch: 039 / 600   Loss: 1.55e+04   Precision: 48.733%   Recall: 63.626%
Valid                   Loss: 2.114e+04   Precision: 23.135%   Recall: 67.680%
Train   Epoch: 040 / 600   Loss: 1.576e+04   Precision: 49.229%   Recall: 64.172%
Valid                   Loss: 2.106e+04   Precision: 27.144%   Recall: 23.971%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 54.38286209  36.66666667]
	 [ 55.70534134  48.        ]
	 [ 65.83890533   9.33333333]
	 [134.5136261  140.        ]
	 [ 57.29024506  47.95      ]]
Train   Epoch: 041 / 600   Loss: 1.494e+04   Precision: 49.483%   Recall: 64.090%
Valid                   Loss: 2.125e+04   Precision: 22.050%   Recall: 75.313%
Train   Epoch: 042 / 600   Loss: 1.539e+04   Precision: 51.509%   Recall: 64.206%
Valid                   Loss: 2.107e+04   Precision: 23.375%   Recall: 71.855%
Train   Epoch: 043 / 600   Loss: 1.628e+04   Precision: 52.914%   Recall: 66.825%
Valid                   Loss: 2.11e+04   Precision: 26.421%   Recall: 47.406%
Train   Epoch: 044 / 600   Loss: 1.588e+04   Precision: 53.210%   Recall: 65.007%
Valid                   Loss: 2.14e+04   Precision: 14.191%   Recall: 99.940%
Train   Epoch: 045 / 600   Loss: 1.535e+04   Precision: 55.344%   Recall: 65.021%
Valid                   Loss: 2.11e+04   Precision: 23.284%   Recall: 59.690%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 83.08234406  31.46666667]
	 [ 83.42417908  58.95      ]
	 [158.62767029  24.75      ]
	 [ 77.37425232  49.21666667]
	 [166.01187134  51.06666667]]
Train   Epoch: 046 / 600   Loss: 1.629e+04   Precision: 54.573%   Recall: 68.526%
Valid                   Loss: 2.119e+04   Precision: 23.451%   Recall: 65.891%
Train   Epoch: 047 / 600   Loss: 1.59e+04   Precision: 55.349%   Recall: 65.660%
Valid                   Loss: 2.094e+04   Precision: 27.189%   Recall: 56.470%
Train   Epoch: 048 / 600   Loss: 1.53e+04   Precision: 56.069%   Recall: 68.182%
Valid                   Loss: 2.101e+04   Precision: 26.453%   Recall: 54.562%
Train   Epoch: 049 / 600   Loss: 1.518e+04   Precision: 57.041%   Recall: 67.056%
Valid                   Loss: 2.119e+04   Precision: 24.223%   Recall: 61.360%
Train   Epoch: 050 / 600   Loss: 1.672e+04   Precision: 57.027%   Recall: 67.897%
Valid                   Loss: 2.103e+04   Precision: 24.254%   Recall: 62.075%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 69.36582184  36.16666667]
	 [ 81.58859253  53.        ]
	 [ 93.10714722  26.11666667]
	 [103.1984024   19.53333333]
	 [ 46.02783203  55.78333333]]
Train   Epoch: 051 / 600   Loss: 1.504e+04   Precision: 56.461%   Recall: 64.546%
Valid                   Loss: 2.096e+04   Precision: 26.820%   Recall: 52.713%
Train   Epoch: 052 / 600   Loss: 1.657e+04   Precision: 58.044%   Recall: 67.491%
Valid                   Loss: 2.137e+04   Precision: 23.099%   Recall: 57.245%
Train   Epoch: 053 / 600   Loss: 1.46e+04   Precision: 57.156%   Recall: 66.481%
Valid                   Loss: 2.122e+04   Precision: 25.132%   Recall: 56.589%
Train   Epoch: 054 / 600   Loss: 1.678e+04   Precision: 57.954%   Recall: 66.216%
Valid                   Loss: 2.113e+04   Precision: 24.524%   Recall: 62.254%
Train   Epoch: 055 / 600   Loss: 1.493e+04   Precision: 58.479%   Recall: 66.014%
Valid                   Loss: 2.113e+04   Precision: 25.362%   Recall: 53.250%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[101.90380096  91.83333333]
	 [ 57.47732162  37.6       ]
	 [ 37.0462532   15.95      ]
	 [ 32.07268143  26.66666667]
	 [ 39.97986984  33.16666667]]
Train   Epoch: 056 / 600   Loss: 1.581e+04   Precision: 58.512%   Recall: 66.794%
Valid                   Loss: 2.182e+04   Precision: 17.903%   Recall: 91.652%
Train   Epoch: 057 / 600   Loss: 1.637e+04   Precision: 58.201%   Recall: 64.152%
Valid                   Loss: 2.099e+04   Precision: 30.063%   Recall: 34.168%
Train   Epoch: 058 / 600   Loss: 1.535e+04   Precision: 56.217%   Recall: 65.437%
Valid                   Loss: 2.088e+04   Precision: 26.667%   Recall: 52.952%
Train   Epoch: 059 / 600   Loss: 1.478e+04   Precision: 58.281%   Recall: 67.100%
Valid                   Loss: 2.088e+04   Precision: 25.341%   Recall: 57.662%
Train   Epoch: 060 / 600   Loss: 1.536e+04   Precision: 58.656%   Recall: 67.031%
Valid                   Loss: 2.083e+04   Precision: 25.635%   Recall: 60.763%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[44.61827087 33.21666667]
	 [38.13964081 44.2       ]
	 [52.71095276 42.18333333]
	 [33.57797623 12.15      ]
	 [81.61952972 25.36666667]]
Train   Epoch: 061 / 600   Loss: 1.548e+04   Precision: 58.481%   Recall: 66.986%
Valid                   Loss: 2.105e+04   Precision: 22.109%   Recall: 77.758%
Train   Epoch: 062 / 600   Loss: 1.571e+04   Precision: 57.651%   Recall: 66.718%
Valid                   Loss: 2.103e+04   Precision: 24.491%   Recall: 63.089%
Train   Epoch: 063 / 600   Loss: 1.442e+04   Precision: 59.338%   Recall: 65.412%
Valid                   Loss: 2.078e+04   Precision: 27.806%   Recall: 49.493%
Train   Epoch: 064 / 600   Loss: 1.458e+04   Precision: 59.114%   Recall: 67.656%
Valid                   Loss: 2.094e+04   Precision: 26.338%   Recall: 56.351%
Train   Epoch: 065 / 600   Loss: 1.54e+04   Precision: 57.013%   Recall: 67.381%
Valid                   Loss: 2.109e+04   Precision: 24.926%   Recall: 60.286%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 47.26558685  35.        ]
	 [ 82.51626587  36.11666667]
	 [109.34415436  69.16666667]
	 [101.59850311  63.98333333]
	 [ 52.99262619  25.25      ]]
Train   Epoch: 066 / 600   Loss: 1.548e+04   Precision: 57.987%   Recall: 67.048%
Valid                   Loss: 2.105e+04   Precision: 25.011%   Recall: 65.951%
Train   Epoch: 067 / 600   Loss: 1.553e+04   Precision: 58.836%   Recall: 68.769%
Valid                   Loss: 2.096e+04   Precision: 26.105%   Recall: 62.671%
Train   Epoch: 068 / 600   Loss: 1.607e+04   Precision: 57.783%   Recall: 64.711%
Valid                   Loss: 2.105e+04   Precision: 27.191%   Recall: 56.052%
Train   Epoch: 069 / 600   Loss: 1.556e+04   Precision: 58.778%   Recall: 66.540%
Valid                   Loss: 2.082e+04   Precision: 23.472%   Recall: 73.524%
Train   Epoch: 070 / 600   Loss: 1.525e+04   Precision: 57.732%   Recall: 64.069%
Valid                   Loss: 2.084e+04   Precision: 26.867%   Recall: 62.850%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 49.0008316   33.        ]
	 [ 51.48273468  41.75      ]
	 [109.73316193  31.88333333]
	 [ 48.07984161  40.55      ]
	 [ 73.96026611  40.03333333]]
Train   Epoch: 071 / 600   Loss: 1.507e+04   Precision: 59.236%   Recall: 60.914%
Valid                   Loss: 2.086e+04   Precision: 32.216%   Recall: 40.131%
Train   Epoch: 072 / 600   Loss: 1.536e+04   Precision: 59.365%   Recall: 64.928%
Valid                   Loss: 2.084e+04   Precision: 26.605%   Recall: 61.300%
Train   Epoch: 073 / 600   Loss: 1.461e+04   Precision: 59.350%   Recall: 64.979%
Valid                   Loss: 2.073e+04   Precision: 28.622%   Recall: 48.062%
Train   Epoch: 074 / 600   Loss: 1.497e+04   Precision: 60.039%   Recall: 66.433%
Valid                   Loss: 2.117e+04   Precision: 23.091%   Recall: 75.730%
Train   Epoch: 075 / 600   Loss: 1.507e+04   Precision: 58.927%   Recall: 67.831%
Valid                   Loss: 2.096e+04   Precision: 28.928%   Recall: 57.424%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 39.75461197  70.55      ]
	 [ 41.74422836  37.63333333]
	 [351.88845825  38.25      ]
	 [ 54.20064545  40.05      ]
	 [ 85.91134644  31.95      ]]
Train   Epoch: 076 / 600   Loss: 1.555e+04   Precision: 57.936%   Recall: 65.275%
Valid                   Loss: 2.077e+04   Precision: 24.339%   Recall: 69.708%
Train   Epoch: 077 / 600   Loss: 1.483e+04   Precision: 59.272%   Recall: 64.801%
Valid                   Loss: 2.074e+04   Precision: 30.066%   Recall: 46.273%
Train   Epoch: 078 / 600   Loss: 1.501e+04   Precision: 59.280%   Recall: 64.657%
Valid                   Loss: 2.077e+04   Precision: 27.495%   Recall: 60.286%
Train   Epoch: 079 / 600   Loss: 1.45e+04   Precision: 59.303%   Recall: 62.103%
Valid                   Loss: 2.07e+04   Precision: 28.008%   Recall: 58.438%
Train   Epoch: 080 / 600   Loss: 1.449e+04   Precision: 59.798%   Recall: 64.007%
Valid                   Loss: 2.076e+04   Precision: 23.360%   Recall: 73.882%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[57.07575226 38.08333333]
	 [48.70682526 23.28333333]
	 [54.95851898 39.2       ]
	 [51.18176651 30.08333333]
	 [59.29348755 23.78333333]]
Train   Epoch: 081 / 600   Loss: 1.445e+04   Precision: 59.349%   Recall: 63.142%
Valid                   Loss: 2.092e+04   Precision: 25.410%   Recall: 64.639%
Train   Epoch: 082 / 600   Loss: 1.432e+04   Precision: 58.795%   Recall: 63.468%
Valid                   Loss: 2.082e+04   Precision: 26.541%   Recall: 66.249%
Train   Epoch: 083 / 600   Loss: 1.457e+04   Precision: 58.438%   Recall: 63.028%
Valid                   Loss: 2.069e+04   Precision: 31.299%   Recall: 42.516%
Train   Epoch: 084 / 600   Loss: 1.487e+04   Precision: 58.953%   Recall: 63.055%
Valid                   Loss: 2.07e+04   Precision: 30.361%   Recall: 47.108%
Train   Epoch: 085 / 600   Loss: 1.456e+04   Precision: 61.417%   Recall: 62.113%
Valid                   Loss: 2.093e+04   Precision: 26.635%   Recall: 56.112%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[120.23839569  64.88333333]
	 [187.6499939   23.        ]
	 [ 62.61938477  14.36666667]
	 [ 75.27328491  63.56666667]
	 [112.70713043 143.9       ]]
Train   Epoch: 086 / 600   Loss: 1.591e+04   Precision: 60.319%   Recall: 62.781%
Valid                   Loss: 2.08e+04   Precision: 26.240%   Recall: 64.043%
Train   Epoch: 087 / 600   Loss: 1.494e+04   Precision: 60.092%   Recall: 60.711%
Valid                   Loss: 2.09e+04   Precision: 24.089%   Recall: 69.767%
Train   Epoch: 088 / 600   Loss: 1.418e+04   Precision: 60.210%   Recall: 63.835%
Valid                   Loss: 2.079e+04   Precision: 27.782%   Recall: 59.690%
Train   Epoch: 089 / 600   Loss: 1.453e+04   Precision: 60.536%   Recall: 63.588%
Valid                   Loss: 2.072e+04   Precision: 32.494%   Recall: 43.113%
Train   Epoch: 090 / 600   Loss: 1.444e+04   Precision: 60.212%   Recall: 60.330%
Valid                   Loss: 2.098e+04   Precision: 20.835%   Recall: 86.583%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 92.76591492  23.        ]
	 [ 85.41395569  72.81666667]
	 [ 79.27946472  61.76666667]
	 [ 94.72392273  36.43333333]
	 [108.49382019 105.16666667]]
Train   Epoch: 091 / 600   Loss: 1.416e+04   Precision: 60.634%   Recall: 63.340%
Valid                   Loss: 2.076e+04   Precision: 26.122%   Recall: 64.222%
Train   Epoch: 092 / 600   Loss: 1.547e+04   Precision: 58.842%   Recall: 61.711%
Valid                   Loss: 2.092e+04   Precision: 27.609%   Recall: 58.378%
Train   Epoch: 093 / 600   Loss: 1.528e+04   Precision: 60.646%   Recall: 64.028%
Valid                   Loss: 2.078e+04   Precision: 28.091%   Recall: 60.823%
Train   Epoch: 094 / 600   Loss: 1.484e+04   Precision: 61.121%   Recall: 62.059%
Valid                   Loss: 2.128e+04   Precision: 28.314%   Recall: 50.566%
Train   Epoch: 095 / 600   Loss: 1.488e+04   Precision: 61.079%   Recall: 62.760%
Valid                   Loss: 2.112e+04   Precision: 28.408%   Recall: 53.309%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 48.6476326   36.33333333]
	 [ 53.47434998  35.11666667]
	 [472.36956787 850.33333333]
	 [ 69.65213776  45.65      ]
	 [ 57.79406738  62.08333333]]
Train   Epoch: 096 / 600   Loss: 1.433e+04   Precision: 60.328%   Recall: 63.791%
Valid                   Loss: 2.101e+04   Precision: 27.563%   Recall: 57.066%
Train   Epoch: 097 / 600   Loss: 1.438e+04   Precision: 61.256%   Recall: 64.337%
Valid                   Loss: 2.093e+04   Precision: 31.231%   Recall: 48.122%
Train   Epoch: 098 / 600   Loss: 1.502e+04   Precision: 61.880%   Recall: 60.144%
Valid                   Loss: 2.057e+04   Precision: 31.398%   Recall: 44.186%
Train   Epoch: 099 / 600   Loss: 1.498e+04   Precision: 61.158%   Recall: 61.606%
Valid                   Loss: 2.068e+04   Precision: 26.860%   Recall: 64.162%
Train   Epoch: 100 / 600   Loss: 1.408e+04   Precision: 59.729%   Recall: 61.822%
Valid                   Loss: 2.065e+04   Precision: 31.449%   Recall: 50.089%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[140.31117249  73.31666667]
	 [ 32.62327576  16.75      ]
	 [174.42419434 166.3       ]
	 [ 61.63499832  33.55      ]
	 [ 46.43214798  16.21666667]]
Train   Epoch: 101 / 600   Loss: 1.432e+04   Precision: 61.291%   Recall: 60.216%
Valid                   Loss: 2.074e+04   Precision: 29.208%   Recall: 56.291%
Train   Epoch: 102 / 600   Loss: 1.465e+04   Precision: 60.370%   Recall: 59.804%
Valid                   Loss: 2.078e+04   Precision: 27.913%   Recall: 54.860%
Train   Epoch: 103 / 600   Loss: 1.459e+04   Precision: 61.806%   Recall: 62.577%
Valid                   Loss: 2.059e+04   Precision: 32.699%   Recall: 45.081%
Train   Epoch: 104 / 600   Loss: 1.443e+04   Precision: 61.240%   Recall: 62.938%
Valid                   Loss: 2.067e+04   Precision: 30.620%   Recall: 54.502%
Train   Epoch: 105 / 600   Loss: 1.498e+04   Precision: 60.690%   Recall: 62.041%
Valid                   Loss: 2.068e+04   Precision: 26.920%   Recall: 62.910%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 67.99846649  43.16666667]
	 [ 35.68613434  36.9       ]
	 [ 84.08831787 101.05      ]
	 [ 32.89966965  22.91666667]
	 [ 66.93543243  27.83333333]]
Train   Epoch: 106 / 600   Loss: 1.46e+04   Precision: 61.463%   Recall: 62.173%
Valid                   Loss: 2.071e+04   Precision: 32.212%   Recall: 45.677%
Train   Epoch: 107 / 600   Loss: 1.399e+04   Precision: 61.552%   Recall: 61.323%
Valid                   Loss: 2.074e+04   Precision: 26.956%   Recall: 68.217%
Train   Epoch: 108 / 600   Loss: 1.461e+04   Precision: 62.469%   Recall: 60.299%
Valid                   Loss: 2.085e+04   Precision: 24.624%   Recall: 70.364%
Train   Epoch: 109 / 600   Loss: 1.597e+04   Precision: 60.266%   Recall: 61.732%
Valid                   Loss: 2.089e+04   Precision: 24.869%   Recall: 64.997%
Train   Epoch: 110 / 600   Loss: 1.471e+04   Precision: 60.856%   Recall: 59.784%
Valid                   Loss: 2.06e+04   Precision: 29.326%   Recall: 57.603%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[183.75291443  15.56666667]
	 [ 65.45903778  26.28333333]
	 [179.373703    46.98333333]
	 [ 65.39209747  24.53333333]
	 [ 46.56261444  53.45      ]]
Train   Epoch: 111 / 600   Loss: 1.466e+04   Precision: 61.816%   Recall: 61.753%
Valid                   Loss: 2.07e+04   Precision: 24.696%   Recall: 69.052%
Train   Epoch: 112 / 600   Loss: 1.418e+04   Precision: 61.088%   Recall: 62.410%
Valid                   Loss: 2.067e+04   Precision: 34.498%   Recall: 41.205%
Train   Epoch: 113 / 600   Loss: 1.407e+04   Precision: 62.014%   Recall: 59.854%
Valid                   Loss: 2.064e+04   Precision: 30.313%   Recall: 57.245%
Train   Epoch: 114 / 600   Loss: 1.48e+04   Precision: 59.595%   Recall: 64.306%
Valid                   Loss: 2.059e+04   Precision: 31.461%   Recall: 55.098%
Train   Epoch: 115 / 600   Loss: 1.368e+04   Precision: 62.191%   Recall: 62.691%
Valid                   Loss: 2.092e+04   Precision: 25.061%   Recall: 67.084%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[124.76815796  90.06666667]
	 [ 65.30325317  28.38333333]
	 [ 57.24956512  38.45      ]
	 [ 70.1180191   70.61666667]
	 [ 58.60272598  23.85      ]]
Train   Epoch: 116 / 600   Loss: 1.416e+04   Precision: 62.334%   Recall: 57.258%
Valid                   Loss: 2.064e+04   Precision: 30.063%   Recall: 54.442%
Train   Epoch: 117 / 600   Loss: 1.462e+04   Precision: 62.090%   Recall: 58.134%
Valid                   Loss: 2.06e+04   Precision: 26.681%   Recall: 60.823%
Train   Epoch: 118 / 600   Loss: 1.514e+04   Precision: 61.737%   Recall: 61.278%
Valid                   Loss: 2.081e+04   Precision: 28.138%   Recall: 50.924%
Train   Epoch: 119 / 600   Loss: 1.394e+04   Precision: 61.496%   Recall: 57.452%
Valid                   Loss: 2.111e+04   Precision: 21.641%   Recall: 72.510%
Train   Epoch: 120 / 600   Loss: 1.535e+04   Precision: 60.520%   Recall: 56.196%
Valid                   Loss: 2.085e+04   Precision: 25.297%   Recall: 68.456%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 47.00872421  84.8       ]
	 [104.06666565  56.13333333]
	 [120.57056427   4.18333333]
	 [ 56.60752106  19.        ]
	 [ 87.79972839  44.5       ]]
Train   Epoch: 121 / 600   Loss: 1.414e+04   Precision: 61.775%   Recall: 60.361%
Valid                   Loss: 2.067e+04   Precision: 30.545%   Recall: 52.475%
Train   Epoch: 122 / 600   Loss: 1.353e+04   Precision: 60.905%   Recall: 57.133%
Valid                   Loss: 2.101e+04   Precision: 23.110%   Recall: 67.263%
Train   Epoch: 123 / 600   Loss: 1.443e+04   Precision: 59.697%   Recall: 60.503%
Valid                   Loss: 2.099e+04   Precision: 19.305%   Recall: 84.794%
Train   Epoch: 124 / 600   Loss: 1.464e+04   Precision: 61.222%   Recall: 57.869%
Valid                   Loss: 2.062e+04   Precision: 30.671%   Recall: 45.796%
Train   Epoch: 125 / 600   Loss: 1.394e+04   Precision: 59.446%   Recall: 59.029%
Valid                   Loss: 2.13e+04   Precision: 20.501%   Recall: 78.593%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[51.21236038 21.13333333]
	 [62.20923996  9.55      ]
	 [55.41076279 19.08333333]
	 [70.03057861 54.61666667]
	 [81.39678955 25.8       ]]
Train   Epoch: 126 / 600   Loss: 1.4e+04   Precision: 60.309%   Recall: 59.072%
Valid                   Loss: 2.064e+04   Precision: 32.161%   Recall: 44.186%
Train   Epoch: 127 / 600   Loss: 1.379e+04   Precision: 62.389%   Recall: 58.472%
Valid                   Loss: 2.064e+04   Precision: 33.534%   Recall: 33.154%
Train   Epoch: 128 / 600   Loss: 1.388e+04   Precision: 60.492%   Recall: 57.435%
Valid                   Loss: 2.065e+04   Precision: 27.886%   Recall: 56.172%
Train   Epoch: 129 / 600   Loss: 1.385e+04   Precision: 60.069%   Recall: 61.338%
Valid                   Loss: 2.073e+04   Precision: 28.005%   Recall: 62.791%
Train   Epoch: 130 / 600   Loss: 1.444e+04   Precision: 59.168%   Recall: 62.162%
Valid                   Loss: 2.085e+04   Precision: 22.180%   Recall: 76.565%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 72.81395721  36.66666667]
	 [ 37.36187363   7.33333333]
	 [ 49.8861084   52.96666667]
	 [107.07694244 104.36666667]
	 [ 74.43352509  86.56666667]]
Train   Epoch: 131 / 600   Loss: 1.406e+04   Precision: 60.287%   Recall: 61.633%
Valid                   Loss: 2.096e+04   Precision: 27.229%   Recall: 51.163%
Train   Epoch: 132 / 600   Loss: 1.458e+04   Precision: 60.428%   Recall: 60.309%
Valid                   Loss: 2.09e+04   Precision: 25.892%   Recall: 54.502%
Train   Epoch: 133 / 600   Loss: 1.395e+04   Precision: 61.410%   Recall: 59.530%
Valid                   Loss: 2.082e+04   Precision: 30.980%   Recall: 41.085%
Train   Epoch: 134 / 600   Loss: 1.447e+04   Precision: 59.376%   Recall: 60.049%
Valid                   Loss: 2.088e+04   Precision: 24.726%   Recall: 68.575%
Train   Epoch: 135 / 600   Loss: 1.312e+04   Precision: 61.087%   Recall: 61.194%
Valid                   Loss: 2.087e+04   Precision: 24.661%   Recall: 66.070%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[74.68241882 17.06666667]
	 [38.88245773 12.58333333]
	 [68.39422607 44.5       ]
	 [41.9462204  46.51666667]
	 [66.21282959 67.56666667]]
Train   Epoch: 136 / 600   Loss: 1.44e+04   Precision: 61.608%   Recall: 59.786%
Valid                   Loss: 2.069e+04   Precision: 26.600%   Recall: 55.277%
Train   Epoch: 137 / 600   Loss: 1.425e+04   Precision: 60.409%   Recall: 59.504%
Valid                   Loss: 2.137e+04   Precision: 21.029%   Recall: 81.157%
Train   Epoch: 138 / 600   Loss: 1.449e+04   Precision: 61.366%   Recall: 56.773%
Valid                   Loss: 2.056e+04   Precision: 32.838%   Recall: 32.916%
Train   Epoch: 139 / 600   Loss: 1.416e+04   Precision: 57.303%   Recall: 58.320%
Valid                   Loss: 2.117e+04   Precision: 22.437%   Recall: 74.120%
Train   Epoch: 140 / 600   Loss: 1.346e+04   Precision: 58.770%   Recall: 60.575%
Valid                   Loss: 2.073e+04   Precision: 31.062%   Recall: 49.195%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 41.68992233  43.33333333]
	 [ 80.61778259  35.2       ]
	 [140.21562195  38.8       ]
	 [ 52.38541031  41.58333333]
	 [ 29.67610741  86.        ]]
Train   Epoch: 141 / 600   Loss: 1.464e+04   Precision: 61.722%   Recall: 56.619%
Valid                   Loss: 2.059e+04   Precision: 28.218%   Recall: 48.241%
Train   Epoch: 142 / 600   Loss: 1.423e+04   Precision: 59.854%   Recall: 57.658%
Valid                   Loss: 2.067e+04   Precision: 31.085%   Recall: 41.503%
Train   Epoch: 143 / 600   Loss: 1.35e+04   Precision: 61.581%   Recall: 58.782%
Valid                   Loss: 2.088e+04   Precision: 20.680%   Recall: 64.580%
Train   Epoch: 144 / 600   Loss: 1.379e+04   Precision: 59.086%   Recall: 60.256%
Valid                   Loss: 2.089e+04   Precision: 27.037%   Recall: 54.621%
Train   Epoch: 145 / 600   Loss: 1.32e+04   Precision: 60.759%   Recall: 59.895%
Valid                   Loss: 2.081e+04   Precision: 27.693%   Recall: 55.039%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[85.1385498  19.11666667]
	 [45.65154266 29.18333333]
	 [41.28024292 19.03333333]
	 [78.6975708  12.38333333]
	 [57.03368378 24.78333333]]
Train   Epoch: 146 / 600   Loss: 1.352e+04   Precision: 61.342%   Recall: 59.250%
Valid                   Loss: 2.081e+04   Precision: 29.080%   Recall: 49.195%
Train   Epoch: 147 / 600   Loss: 1.43e+04   Precision: 60.182%   Recall: 61.299%
Valid                   Loss: 2.068e+04   Precision: 23.513%   Recall: 66.726%
Train   Epoch: 148 / 600   Loss: 1.443e+04   Precision: 58.902%   Recall: 59.856%
Valid                   Loss: 2.138e+04   Precision: 25.301%   Recall: 68.813%
Train   Epoch: 149 / 600   Loss: 1.422e+04   Precision: 59.219%   Recall: 60.946%
Valid                   Loss: 2.082e+04   Precision: 29.754%   Recall: 48.241%
Train   Epoch: 150 / 600   Loss: 1.409e+04   Precision: 58.228%   Recall: 60.227%
Valid                   Loss: 2.095e+04   Precision: 23.932%   Recall: 66.786%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 63.16016388  31.63333333]
	 [ 59.62062073  25.88333333]
	 [107.19863892  73.48333333]
	 [ 41.39767075  41.45      ]
	 [ 73.45334625  28.33333333]]
Train   Epoch: 151 / 600   Loss: 1.372e+04   Precision: 60.481%   Recall: 61.691%
Valid                   Loss: 2.087e+04   Precision: 29.278%   Recall: 48.360%
Train   Epoch: 152 / 600   Loss: 1.344e+04   Precision: 58.899%   Recall: 61.090%
Valid                   Loss: 2.065e+04   Precision: 27.646%   Recall: 57.484%
Train   Epoch: 153 / 600   Loss: 1.376e+04   Precision: 59.710%   Recall: 60.732%
Valid                   Loss: 2.085e+04   Precision: 25.692%   Recall: 65.891%
Train   Epoch: 154 / 600   Loss: 1.327e+04   Precision: 60.727%   Recall: 58.887%
Valid                   Loss: 2.138e+04   Precision: 25.760%   Recall: 62.671%
Train   Epoch: 155 / 600   Loss: 1.445e+04   Precision: 60.935%   Recall: 57.938%
Valid                   Loss: 2.07e+04   Precision: 26.053%   Recall: 61.956%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 72.09066772  14.41666667]
	 [ 67.82015228  20.7       ]
	 [ 60.23091888  33.6       ]
	 [ 69.55783844  19.41666667]
	 [253.37242126 746.7       ]]
Train   Epoch: 156 / 600   Loss: 1.274e+04   Precision: 61.299%   Recall: 59.981%
Valid                   Loss: 2.057e+04   Precision: 30.603%   Recall: 51.461%
Train   Epoch: 157 / 600   Loss: 1.394e+04   Precision: 60.974%   Recall: 59.365%
Valid                   Loss: 2.121e+04   Precision: 21.637%   Recall: 76.148%
Train   Epoch: 158 / 600   Loss: 1.301e+04   Precision: 61.259%   Recall: 59.525%
Valid                   Loss: 2.069e+04   Precision: 30.441%   Recall: 49.374%
Train   Epoch: 159 / 600   Loss: 1.413e+04   Precision: 58.879%   Recall: 59.134%
Valid                   Loss: 2.09e+04   Precision: 28.368%   Recall: 39.177%
Train   Epoch: 160 / 600   Loss: 1.388e+04   Precision: 59.284%   Recall: 60.990%
Valid                   Loss: 2.115e+04   Precision: 21.794%   Recall: 69.827%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 49.43042755  48.33333333]
	 [ 86.1939621   30.66666667]
	 [ 61.5292778   52.9       ]
	 [146.22000122   9.2       ]
	 [ 54.64989853  87.58333333]]
Train   Epoch: 161 / 600   Loss: 1.279e+04   Precision: 60.959%   Recall: 57.906%
Valid                   Loss: 2.082e+04   Precision: 30.151%   Recall: 34.466%
Train   Epoch: 162 / 600   Loss: 1.365e+04   Precision: 59.273%   Recall: 61.052%
Valid                   Loss: 2.084e+04   Precision: 28.057%   Recall: 54.860%
Train   Epoch: 163 / 600   Loss: 1.421e+04   Precision: 57.626%   Recall: 58.814%
Valid                   Loss: 2.108e+04   Precision: 22.167%   Recall: 83.065%
Train   Epoch: 164 / 600   Loss: 1.425e+04   Precision: 58.029%   Recall: 57.951%
Valid                   Loss: 2.069e+04   Precision: 26.277%   Recall: 63.208%
Train   Epoch: 165 / 600   Loss: 1.293e+04   Precision: 62.176%   Recall: 58.184%
Valid                   Loss: 2.08e+04   Precision: 30.780%   Recall: 41.205%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[108.08190918  40.38333333]
	 [ 57.02378082  15.33333333]
	 [170.47167969  59.        ]
	 [ 53.22928619  33.4       ]
	 [ 46.90036392  35.38333333]]
Train   Epoch: 166 / 600   Loss: 1.239e+04   Precision: 62.530%   Recall: 59.771%
Valid                   Loss: 2.076e+04   Precision: 27.359%   Recall: 60.346%
Train   Epoch: 167 / 600   Loss: 1.385e+04   Precision: 60.783%   Recall: 58.897%
Valid                   Loss: 2.068e+04   Precision: 28.242%   Recall: 53.250%
Train   Epoch: 168 / 600   Loss: 1.326e+04   Precision: 58.359%   Recall: 61.060%
Valid                   Loss: 2.091e+04   Precision: 30.190%   Recall: 49.255%
Train   Epoch: 169 / 600   Loss: 1.358e+04   Precision: 59.518%   Recall: 57.984%
Valid                   Loss: 2.125e+04   Precision: 26.320%   Recall: 56.470%
Train   Epoch: 170 / 600   Loss: 1.375e+04   Precision: 59.027%   Recall: 55.906%
Valid                   Loss: 2.098e+04   Precision: 23.488%   Recall: 64.401%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[68.5291214  41.33333333]
	 [56.10271835 60.41666667]
	 [78.71117401 70.55      ]
	 [63.43598938 33.15      ]
	 [67.06419373  8.        ]]
Train   Epoch: 171 / 600   Loss: 1.331e+04   Precision: 62.325%   Recall: 57.711%
Valid                   Loss: 2.059e+04   Precision: 28.367%   Recall: 54.383%
Train   Epoch: 172 / 600   Loss: 1.329e+04   Precision: 61.212%   Recall: 57.804%
Valid                   Loss: 2.133e+04   Precision: 23.522%   Recall: 67.859%
Train   Epoch: 173 / 600   Loss: 1.378e+04   Precision: 61.224%   Recall: 57.664%
Valid                   Loss: 2.114e+04   Precision: 26.676%   Recall: 57.662%
Train   Epoch: 174 / 600   Loss: 1.292e+04   Precision: 61.369%   Recall: 57.485%
Valid                   Loss: 2.067e+04   Precision: 31.436%   Recall: 44.126%
Train   Epoch: 175 / 600   Loss: 1.321e+04   Precision: 61.010%   Recall: 57.788%
Valid                   Loss: 2.078e+04   Precision: 28.159%   Recall: 51.163%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 50.59720993  47.38333333]
	 [ 57.65158844  15.48333333]
	 [ 50.58338547  45.03333333]
	 [ 42.88346863  26.58333333]
	 [148.11627197  15.58333333]]
Train   Epoch: 176 / 600   Loss: 1.286e+04   Precision: 63.753%   Recall: 57.227%
Valid                   Loss: 2.079e+04   Precision: 25.695%   Recall: 60.644%
Train   Epoch: 177 / 600   Loss: 1.294e+04   Precision: 62.382%   Recall: 58.369%
Valid                   Loss: 2.093e+04   Precision: 27.037%   Recall: 58.378%
Train   Epoch: 178 / 600   Loss: 1.266e+04   Precision: 62.381%   Recall: 59.365%
Valid                   Loss: 2.087e+04   Precision: 26.451%   Recall: 65.236%
Train   Epoch: 179 / 600   Loss: 1.308e+04   Precision: 60.736%   Recall: 56.854%
Valid                   Loss: 2.086e+04   Precision: 27.446%   Recall: 60.048%
Train   Epoch: 180 / 600   Loss: 1.301e+04   Precision: 60.957%   Recall: 60.668%
Valid                   Loss: 2.1e+04   Precision: 27.226%   Recall: 47.764%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 44.94843292  26.53333333]
	 [ 86.07554626  44.        ]
	 [ 26.45926666  16.8       ]
	 [ 63.10338593  96.35      ]
	 [102.94576263  17.66666667]]
Train   Epoch: 181 / 600   Loss: 1.29e+04   Precision: 62.576%   Recall: 57.041%
Valid                   Loss: 2.076e+04   Precision: 31.715%   Recall: 43.232%
Train   Epoch: 182 / 600   Loss: 1.266e+04   Precision: 62.056%   Recall: 57.814%
Valid                   Loss: 2.087e+04   Precision: 23.072%   Recall: 66.011%
Train   Epoch: 183 / 600   Loss: 1.242e+04   Precision: 58.681%   Recall: 59.709%
Valid                   Loss: 2.076e+04   Precision: 24.197%   Recall: 66.011%
Train   Epoch: 184 / 600   Loss: 1.292e+04   Precision: 61.764%   Recall: 57.536%
Valid                   Loss: 2.065e+04   Precision: 30.330%   Recall: 50.984%
Train   Epoch: 185 / 600   Loss: 1.284e+04   Precision: 62.094%   Recall: 59.163%
Valid                   Loss: 2.086e+04   Precision: 26.302%   Recall: 59.034%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 69.27562714  32.03333333]
	 [150.46582031  83.        ]
	 [ 60.82818222  21.61666667]
	 [ 90.93844604  24.86666667]
	 [ 68.87355804  89.7       ]]
Train   Epoch: 186 / 600   Loss: 1.168e+04   Precision: 64.260%   Recall: 58.216%
Valid                   Loss: 2.119e+04   Precision: 23.693%   Recall: 70.781%
Train   Epoch: 187 / 600   Loss: 1.144e+04   Precision: 61.964%   Recall: 58.668%
Valid                   Loss: 2.077e+04   Precision: 30.539%   Recall: 29.756%
Train   Epoch: 188 / 600   Loss: 1.291e+04   Precision: 58.250%   Recall: 57.554%
Valid                   Loss: 2.104e+04   Precision: 23.716%   Recall: 69.112%
Train   Epoch: 189 / 600   Loss: 1.223e+04   Precision: 62.697%   Recall: 57.474%
Valid                   Loss: 2.095e+04   Precision: 25.964%   Recall: 61.002%
Train   Epoch: 190 / 600   Loss: 1.248e+04   Precision: 62.226%   Recall: 58.078%
Valid                   Loss: 2.127e+04   Precision: 25.271%   Recall: 63.983%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 67.9006424   27.11666667]
	 [ 72.65171814  78.58333333]
	 [162.72325134  37.11666667]
	 [ 40.89677811  89.65      ]
	 [ 52.08486176  43.63333333]]
Train   Epoch: 191 / 600   Loss: 1.225e+04   Precision: 62.634%   Recall: 61.033%
Valid                   Loss: 2.083e+04   Precision: 29.165%   Recall: 53.965%
Train   Epoch: 192 / 600   Loss: 1.292e+04   Precision: 60.879%   Recall: 58.701%
Valid                   Loss: 2.1e+04   Precision: 26.931%   Recall: 54.681%
Train   Epoch: 193 / 600   Loss: 1.342e+04   Precision: 61.519%   Recall: 59.293%
Valid                   Loss: 2.071e+04   Precision: 31.608%   Recall: 37.150%
Train   Epoch: 194 / 600   Loss: 1.228e+04   Precision: 62.938%   Recall: 58.701%
Valid                   Loss: 2.089e+04   Precision: 25.045%   Recall: 66.667%
Train   Epoch: 195 / 600   Loss: 1.226e+04   Precision: 62.547%   Recall: 60.348%
Valid                   Loss: 2.088e+04   Precision: 32.802%   Recall: 47.883%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[76.07873535 30.53333333]
	 [48.66240692 28.21666667]
	 [29.48631668 15.08333333]
	 [57.30942917 57.66666667]
	 [46.53139877 26.03333333]]
Train   Epoch: 196 / 600   Loss: 1.212e+04   Precision: 64.093%   Recall: 61.431%
Valid                   Loss: 2.094e+04   Precision: 32.734%   Recall: 44.484%
Train   Epoch: 197 / 600   Loss: 1.203e+04   Precision: 62.689%   Recall: 60.596%
Valid                   Loss: 2.093e+04   Precision: 27.145%   Recall: 52.832%
Train   Epoch: 198 / 600   Loss: 1.223e+04   Precision: 63.045%   Recall: 60.400%
Valid                   Loss: 2.084e+04   Precision: 26.511%   Recall: 59.630%
Train   Epoch: 199 / 600   Loss: 1.251e+04   Precision: 62.684%   Recall: 59.596%
Valid                   Loss: 2.136e+04   Precision: 22.348%   Recall: 76.267%
Train   Epoch: 200 / 600   Loss: 1.241e+04   Precision: 62.711%   Recall: 62.330%
Valid                   Loss: 2.108e+04   Precision: 26.026%   Recall: 65.772%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 98.9381485   47.81666667]
	 [407.17318726 209.45      ]
	 [ 63.54060364  31.58333333]
	 [114.19248962  31.78333333]
	 [ 44.88588715  18.65      ]]
Train   Epoch: 201 / 600   Loss: 1.194e+04   Precision: 62.072%   Recall: 59.730%
Valid                   Loss: 2.115e+04   Precision: 25.352%   Recall: 66.607%
Train   Epoch: 202 / 600   Loss: 1.281e+04   Precision: 60.403%   Recall: 59.668%
Valid                   Loss: 2.074e+04   Precision: 31.082%   Recall: 50.209%
Train   Epoch: 203 / 600   Loss: 1.233e+04   Precision: 60.395%   Recall: 60.588%
Valid                   Loss: 2.108e+04   Precision: 28.775%   Recall: 49.434%
Train   Epoch: 204 / 600   Loss: 1.142e+04   Precision: 64.513%   Recall: 59.598%
Valid                   Loss: 2.1e+04   Precision: 25.882%   Recall: 62.552%
Train   Epoch: 205 / 600   Loss: 1.168e+04   Precision: 62.937%   Recall: 59.330%
Valid                   Loss: 2.094e+04   Precision: 33.164%   Recall: 38.998%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[55.73574829 47.71666667]
	 [29.97441864 28.21666667]
	 [42.27357864 36.91666667]
	 [40.56290054 36.73333333]
	 [31.55422592 19.7       ]]
Train   Epoch: 206 / 600   Loss: 1.208e+04   Precision: 61.238%   Recall: 61.402%
Valid                   Loss: 2.13e+04   Precision: 29.224%   Recall: 35.480%
Train   Epoch: 207 / 600   Loss: 1.169e+04   Precision: 63.851%   Recall: 56.505%
Valid                   Loss: 2.1e+04   Precision: 27.257%   Recall: 57.424%
Train   Epoch: 208 / 600   Loss: 1.182e+04   Precision: 63.133%   Recall: 58.740%
Valid                   Loss: 2.085e+04   Precision: 26.431%   Recall: 63.626%
Train   Epoch: 209 / 600   Loss: 1.195e+04   Precision: 63.797%   Recall: 56.912%
Valid                   Loss: 2.095e+04   Precision: 27.088%   Recall: 63.447%
Train   Epoch: 210 / 600   Loss: 1.157e+04   Precision: 63.146%   Recall: 60.998%
Valid                   Loss: 2.105e+04   Precision: 26.150%   Recall: 67.442%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[70.77359009 41.33333333]
	 [40.22075653 15.95      ]
	 [43.39127731 29.91666667]
	 [40.30474472 28.66666667]
	 [87.39235687 27.96666667]]
Train   Epoch: 211 / 600   Loss: 1.456e+04   Precision: 57.458%   Recall: 61.990%
Valid                   Loss: 2.124e+04   Precision: 16.935%   Recall: 93.500%
Train   Epoch: 212 / 600   Loss: 1.343e+04   Precision: 54.588%   Recall: 60.585%
Valid                   Loss: 2.154e+04   Precision: 21.732%   Recall: 73.763%
Train   Epoch: 213 / 600   Loss: 1.295e+04   Precision: 60.813%   Recall: 60.938%
Valid                   Loss: 2.077e+04   Precision: 29.349%   Recall: 52.713%
Train   Epoch: 214 / 600   Loss: 1.27e+04   Precision: 60.237%   Recall: 62.317%
Valid                   Loss: 2.146e+04   Precision: 25.424%   Recall: 62.552%
Train   Epoch: 215 / 600   Loss: 1.228e+04   Precision: 62.637%   Recall: 60.029%
Valid                   Loss: 2.111e+04   Precision: 24.010%   Recall: 70.483%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 56.12393188  36.6       ]
	 [ 65.17992401  32.66666667]
	 [ 80.49238586  24.8       ]
	 [ 55.2061882   29.21666667]
	 [123.24476624  29.        ]]
Train   Epoch: 216 / 600   Loss: 1.233e+04   Precision: 62.761%   Recall: 61.454%
Valid                   Loss: 2.085e+04   Precision: 27.605%   Recall: 59.392%
Train   Epoch: 217 / 600   Loss: 1.262e+04   Precision: 62.169%   Recall: 61.227%
Valid                   Loss: 2.105e+04   Precision: 22.867%   Recall: 69.529%
Train   Epoch: 218 / 600   Loss: 1.249e+04   Precision: 63.068%   Recall: 59.070%
Valid                   Loss: 2.1e+04   Precision: 30.146%   Recall: 52.832%
Train   Epoch: 219 / 600   Loss: 1.228e+04   Precision: 64.538%   Recall: 60.526%
Valid                   Loss: 2.117e+04   Precision: 29.589%   Recall: 33.453%
Train   Epoch: 220 / 600   Loss: 1.169e+04   Precision: 63.432%   Recall: 60.915%
Valid                   Loss: 2.1e+04   Precision: 26.030%   Recall: 67.800%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[32.32117081  9.2       ]
	 [63.35385132 12.63333333]
	 [59.07197571 58.33333333]
	 [59.0163002  51.        ]
	 [59.05165482 38.8       ]]
Train   Epoch: 221 / 600   Loss: 1.158e+04   Precision: 63.191%   Recall: 57.536%
Valid                   Loss: 2.117e+04   Precision: 24.649%   Recall: 69.052%
Train   Epoch: 222 / 600   Loss: 1.202e+04   Precision: 62.627%   Recall: 59.430%
Valid                   Loss: 2.097e+04   Precision: 25.702%   Recall: 68.754%
Train   Epoch: 223 / 600   Loss: 1.282e+04   Precision: 61.115%   Recall: 61.701%
Valid                   Loss: 2.187e+04   Precision: 17.580%   Recall: 89.922%
Train   Epoch: 224 / 600   Loss: 1.242e+04   Precision: 62.852%   Recall: 61.433%
Valid                   Loss: 2.067e+04   Precision: 31.877%   Recall: 40.906%
Train   Epoch: 225 / 600   Loss: 1.255e+04   Precision: 61.291%   Recall: 58.928%
Valid                   Loss: 2.075e+04   Precision: 29.096%   Recall: 54.323%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 36.64990234  29.33333333]
	 [ 75.68547058  43.16666667]
	 [ 97.0636673  103.63333333]
	 [ 80.10590363  15.41666667]
	 [ 40.43154144  58.11666667]]
Train   Epoch: 226 / 600   Loss: 1.199e+04   Precision: 62.378%   Recall: 60.680%
Valid                   Loss: 2.102e+04   Precision: 27.621%   Recall: 61.121%
Train   Epoch: 227 / 600   Loss: 1.14e+04   Precision: 63.799%   Recall: 60.926%
Valid                   Loss: 2.114e+04   Precision: 27.448%   Recall: 57.007%
Train   Epoch: 228 / 600   Loss: 1.188e+04   Precision: 64.719%   Recall: 60.629%
Valid                   Loss: 2.092e+04   Precision: 26.829%   Recall: 59.928%
Train   Epoch: 229 / 600   Loss: 1.19e+04   Precision: 63.770%   Recall: 59.761%
Valid                   Loss: 2.121e+04   Precision: 31.429%   Recall: 46.571%
Train   Epoch: 230 / 600   Loss: 1.192e+04   Precision: 63.399%   Recall: 58.948%
Valid                   Loss: 2.124e+04   Precision: 26.792%   Recall: 67.084%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 54.09255219  28.98333333]
	 [ 40.40999222  58.8       ]
	 [ 55.11309433  30.18333333]
	 [ 45.00897598  16.        ]
	 [109.98578644  33.55      ]]
Train   Epoch: 231 / 600   Loss: 1.09e+04   Precision: 63.963%   Recall: 61.127%
Valid                   Loss: 2.107e+04   Precision: 29.608%   Recall: 56.231%
Train   Epoch: 232 / 600   Loss: 1.149e+04   Precision: 64.389%   Recall: 61.773%
Valid                   Loss: 2.154e+04   Precision: 26.843%   Recall: 58.617%
Train   Epoch: 233 / 600   Loss: 1.126e+04   Precision: 62.720%   Recall: 60.948%
Valid                   Loss: 2.136e+04   Precision: 23.521%   Recall: 70.900%
Train   Epoch: 234 / 600   Loss: 1.225e+04   Precision: 60.833%   Recall: 59.008%
Valid                   Loss: 2.089e+04   Precision: 30.751%   Recall: 46.631%
Train   Epoch: 235 / 600   Loss: 1.28e+04   Precision: 63.935%   Recall: 60.627%
Valid                   Loss: 2.094e+04   Precision: 27.850%   Recall: 57.841%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[54.81358719 15.46666667]
	 [78.36212921 12.3       ]
	 [34.38417053 25.86666667]
	 [45.61481857 15.        ]
	 [88.86694336 22.45      ]]
Train   Epoch: 236 / 600   Loss: 1.186e+04   Precision: 65.343%   Recall: 59.866%
Valid                   Loss: 2.112e+04   Precision: 26.394%   Recall: 66.309%
Train   Epoch: 237 / 600   Loss: 1.065e+04   Precision: 66.055%   Recall: 61.235%
Valid                   Loss: 2.111e+04   Precision: 29.271%   Recall: 57.424%
Train   Epoch: 238 / 600   Loss: 1.105e+04   Precision: 65.409%   Recall: 59.340%
Valid                   Loss: 2.197e+04   Precision: 29.968%   Recall: 44.961%
Train   Epoch: 239 / 600   Loss: 1.113e+04   Precision: 65.076%   Recall: 60.843%
Valid                   Loss: 2.114e+04   Precision: 29.347%   Recall: 52.534%
Train   Epoch: 240 / 600   Loss: 1.092e+04   Precision: 64.291%   Recall: 60.905%
Valid                   Loss: 2.131e+04   Precision: 27.883%   Recall: 60.704%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[  80.16197205   19.08333333]
	 [1094.88464355 1421.86666667]
	 [  36.00128555   30.61666667]
	 [  71.59645844   45.38333333]
	 [  83.57006836   45.83333333]]
Train   Epoch: 241 / 600   Loss: 1.074e+04   Precision: 64.821%   Recall: 61.194%
Valid                   Loss: 2.1e+04   Precision: 31.459%   Recall: 43.709%
Train   Epoch: 242 / 600   Loss: 1.065e+04   Precision: 64.527%   Recall: 60.423%
Valid                   Loss: 2.112e+04   Precision: 29.670%   Recall: 43.948%
Train   Epoch: 243 / 600   Loss: 1.311e+04   Precision: 60.886%   Recall: 60.052%
Valid                   Loss: 2.095e+04   Precision: 30.224%   Recall: 50.626%
Train   Epoch: 244 / 600   Loss: 1.237e+04   Precision: 61.387%   Recall: 58.565%
Valid                   Loss: 2.161e+04   Precision: 28.694%   Recall: 51.878%
Train   Epoch: 245 / 600   Loss: 1.097e+04   Precision: 65.023%   Recall: 59.802%
Valid                   Loss: 2.093e+04   Precision: 31.129%   Recall: 40.429%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[46.08694077 35.25      ]
	 [44.78743362 63.25      ]
	 [67.70677948 16.38333333]
	 [62.3338356  41.98333333]
	 [46.5143013  44.78333333]]
Train   Epoch: 246 / 600   Loss: 1.138e+04   Precision: 64.383%   Recall: 61.031%
Valid                   Loss: 2.126e+04   Precision: 26.802%   Recall: 63.208%
Train   Epoch: 247 / 600   Loss: 1.062e+04   Precision: 65.126%   Recall: 60.864%
Valid                   Loss: 2.113e+04   Precision: 26.996%   Recall: 67.740%
Train   Epoch: 248 / 600   Loss: 1.099e+04   Precision: 64.962%   Recall: 63.763%
Valid                   Loss: 2.131e+04   Precision: 23.100%   Recall: 72.868%
Train   Epoch: 249 / 600   Loss: 1.109e+04   Precision: 64.667%   Recall: 58.988%
Valid                   Loss: 2.125e+04   Precision: 26.561%   Recall: 64.699%
Train   Epoch: 250 / 600   Loss: 1.037e+04   Precision: 67.076%   Recall: 60.278%
Valid                   Loss: 2.109e+04   Precision: 27.254%   Recall: 62.194%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 72.39029694  54.        ]
	 [ 44.3094368   29.16666667]
	 [ 39.50997543  20.        ]
	 [141.39434814  15.1       ]
	 [167.35713196  47.15      ]]
Train   Epoch: 251 / 600   Loss: 1.141e+04   Precision: 65.063%   Recall: 59.812%
Valid                   Loss: 2.106e+04   Precision: 24.302%   Recall: 64.878%
Train   Epoch: 252 / 600   Loss: 1.165e+04   Precision: 64.609%   Recall: 56.644%
Valid                   Loss: 2.095e+04   Precision: 34.655%   Recall: 33.870%
Train   Epoch: 253 / 600   Loss: 1.045e+04   Precision: 65.565%   Recall: 59.307%
Valid                   Loss: 2.131e+04   Precision: 25.240%   Recall: 70.602%
Train   Epoch: 254 / 600   Loss: 1.203e+04   Precision: 60.578%   Recall: 60.922%
Valid                   Loss: 2.101e+04   Precision: 27.368%   Recall: 60.823%
Train   Epoch: 255 / 600   Loss: 1.081e+04   Precision: 65.936%   Recall: 60.329%
Valid                   Loss: 2.116e+04   Precision: 27.646%   Recall: 57.782%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[55.96331406 38.35      ]
	 [90.70053864 45.96666667]
	 [39.59101486 43.75      ]
	 [54.83352661 12.        ]
	 [81.09597778 35.9       ]]
Train   Epoch: 256 / 600   Loss: 1.09e+04   Precision: 66.102%   Recall: 58.784%
Valid                   Loss: 2.093e+04   Precision: 30.256%   Recall: 48.658%
Train   Epoch: 257 / 600   Loss: 1.005e+04   Precision: 64.933%   Recall: 59.196%
Valid                   Loss: 2.108e+04   Precision: 28.776%   Recall: 51.580%
Train   Epoch: 258 / 600   Loss: 1.072e+04   Precision: 66.199%   Recall: 58.856%
Valid                   Loss: 2.144e+04   Precision: 29.599%   Recall: 54.204%
Train   Epoch: 259 / 600   Loss: 1.082e+04   Precision: 66.756%   Recall: 59.103%
Valid                   Loss: 2.117e+04   Precision: 30.187%   Recall: 47.108%
Train   Epoch: 260 / 600   Loss:    9671   Precision: 67.450%   Recall: 59.204%
Valid                   Loss: 2.129e+04   Precision: 26.587%   Recall: 58.438%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[46.38845062 40.33333333]
	 [35.46515274 14.51666667]
	 [66.33404541 24.66666667]
	 [84.55163574 51.16666667]
	 [83.18520355 50.88333333]]
Train   Epoch: 261 / 600   Loss: 1.073e+04   Precision: 63.319%   Recall: 61.680%
Valid                   Loss: 2.159e+04   Precision: 16.323%   Recall: 92.129%
Train   Epoch: 262 / 600   Loss: 1.282e+04   Precision: 57.415%   Recall: 66.299%
Valid                   Loss: 2.122e+04   Precision: 25.286%   Recall: 63.327%
Train   Epoch: 263 / 600   Loss: 1.084e+04   Precision: 62.257%   Recall: 62.379%
Valid                   Loss: 2.107e+04   Precision: 35.033%   Recall: 19.261%
Train   Epoch: 264 / 600   Loss: 1.2e+04   Precision: 61.604%   Recall: 58.277%
Valid                   Loss: 2.11e+04   Precision: 27.499%   Recall: 64.460%
Train   Epoch: 265 / 600   Loss: 1.053e+04   Precision: 63.361%   Recall: 60.658%
Valid                   Loss: 2.096e+04   Precision: 31.687%   Recall: 43.232%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[41.15421295 53.06666667]
	 [50.62290192 12.45      ]
	 [59.83837891 62.11666667]
	 [53.06258392 24.        ]
	 [30.09833336 27.11666667]]
Train   Epoch: 266 / 600   Loss: 1.085e+04   Precision: 65.279%   Recall: 61.093%
Valid                   Loss: 2.128e+04   Precision: 30.491%   Recall: 56.291%
Train   Epoch: 267 / 600   Loss: 1.04e+04   Precision: 65.276%   Recall: 61.750%
Valid                   Loss: 2.136e+04   Precision: 26.255%   Recall: 70.185%
Train   Epoch: 268 / 600   Loss: 1.017e+04   Precision: 65.274%   Recall: 61.351%
Valid                   Loss: 2.127e+04   Precision: 27.086%   Recall: 63.685%
Train   Epoch: 269 / 600   Loss:    9522   Precision: 66.415%   Recall: 59.938%
Valid                   Loss: 2.118e+04   Precision: 29.612%   Recall: 56.947%
Train   Epoch: 270 / 600   Loss: 1.053e+04   Precision: 66.408%   Recall: 61.794%
Valid                   Loss: 2.154e+04   Precision: 29.684%   Recall: 57.066%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 31.79463959  20.38333333]
	 [ 42.57579422  14.28333333]
	 [103.55696869  38.33333333]
	 [ 34.15007019  25.61666667]
	 [ 50.83773804  35.88333333]]
Train   Epoch: 271 / 600   Loss: 1.054e+04   Precision: 65.765%   Recall: 60.884%
Valid                   Loss: 2.097e+04   Precision: 27.645%   Recall: 64.818%
Train   Epoch: 272 / 600   Loss: 1.014e+04   Precision: 67.452%   Recall: 60.513%
Valid                   Loss: 2.175e+04   Precision: 30.262%   Recall: 59.332%
Train   Epoch: 273 / 600   Loss:    9986   Precision: 67.254%   Recall: 61.763%
Valid                   Loss: 2.147e+04   Precision: 28.934%   Recall: 57.126%
Train   Epoch: 274 / 600   Loss: 1.008e+04   Precision: 66.269%   Recall: 58.905%
Valid                   Loss: 2.126e+04   Precision: 30.117%   Recall: 52.117%
Train   Epoch: 275 / 600   Loss:    9602   Precision: 66.253%   Recall: 61.070%
Valid                   Loss: 2.117e+04   Precision: 30.696%   Recall: 49.970%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 49.59712982  37.8       ]
	 [ 72.82447815  24.25      ]
	 [ 66.30348206  67.55      ]
	 [ 57.90420532  24.83333333]
	 [105.98345184   8.83333333]]
Train   Epoch: 276 / 600   Loss:    9707   Precision: 66.268%   Recall: 59.936%
Valid                   Loss: 2.145e+04   Precision: 29.426%   Recall: 51.938%
Train   Epoch: 277 / 600   Loss:    9849   Precision: 65.597%   Recall: 62.392%
Valid                   Loss: 2.146e+04   Precision: 26.055%   Recall: 66.249%
Train   Epoch: 278 / 600   Loss: 1.031e+04   Precision: 66.236%   Recall: 59.142%
Valid                   Loss: 2.248e+04   Precision: 21.899%   Recall: 81.574%
Train   Epoch: 279 / 600   Loss: 1.161e+04   Precision: 60.582%   Recall: 61.588%
Valid                   Loss: 2.115e+04   Precision: 25.143%   Recall: 68.277%
Train   Epoch: 280 / 600   Loss: 1.038e+04   Precision: 66.543%   Recall: 60.979%
Valid                   Loss: 2.107e+04   Precision: 29.273%   Recall: 53.309%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[63.74635315 48.25      ]
	 [86.23475647 51.11666667]
	 [72.04818726 40.48333333]
	 [68.0929718  25.13333333]
	 [61.6626358  42.6       ]]
Train   Epoch: 281 / 600   Loss: 1.009e+04   Precision: 66.728%   Recall: 59.781%
Valid                   Loss: 2.125e+04   Precision: 29.997%   Recall: 58.259%
Train   Epoch: 282 / 600   Loss: 1.176e+04   Precision: 64.272%   Recall: 59.946%
Valid                   Loss: 2.109e+04   Precision: 31.486%   Recall: 51.163%
Train   Epoch: 283 / 600   Loss: 1.289e+04   Precision: 61.477%   Recall: 60.761%
Valid                   Loss: 2.151e+04   Precision: 28.586%   Recall: 66.070%
Train   Epoch: 284 / 600   Loss: 1.064e+04   Precision: 65.032%   Recall: 59.326%
Valid                   Loss: 2.181e+04   Precision: 29.097%   Recall: 57.066%
Train   Epoch: 285 / 600   Loss: 1.134e+04   Precision: 63.829%   Recall: 57.964%
Valid                   Loss: 2.24e+04   Precision: 18.909%   Recall: 82.230%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 72.49356079  56.28333333]
	 [247.52966309 397.38333333]
	 [179.95953369  31.71666667]
	 [104.65154266  41.28333333]
	 [ 69.32852936  73.38333333]]
Train   Epoch: 286 / 600   Loss: 1.023e+04   Precision: 64.123%   Recall: 60.216%
Valid                   Loss: 2.105e+04   Precision: 23.631%   Recall: 61.240%
Train   Epoch: 287 / 600   Loss: 1.022e+04   Precision: 66.753%   Recall: 61.083%
Valid                   Loss: 2.163e+04   Precision: 27.575%   Recall: 53.488%
Train   Epoch: 288 / 600   Loss:    9819   Precision: 67.711%   Recall: 60.381%
Valid                   Loss: 2.134e+04   Precision: 27.163%   Recall: 60.465%
Train   Epoch: 289 / 600   Loss:    9659   Precision: 66.886%   Recall: 61.065%
Valid                   Loss: 2.121e+04   Precision: 30.021%   Recall: 52.236%
Train   Epoch: 290 / 600   Loss:    9710   Precision: 67.724%   Recall: 60.730%
Valid                   Loss: 2.104e+04   Precision: 33.319%   Recall: 47.704%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[41.7820816  40.        ]
	 [91.81813049 29.33333333]
	 [30.59067535 42.66666667]
	 [ 7.10171509 14.55      ]
	 [71.38905334 35.78333333]]
Train   Epoch: 291 / 600   Loss: 1.061e+04   Precision: 64.150%   Recall: 61.227%
Valid                   Loss: 2.132e+04   Precision: 27.135%   Recall: 39.595%
Train   Epoch: 292 / 600   Loss:    9995   Precision: 65.830%   Recall: 57.281%
Valid                   Loss: 2.17e+04   Precision: 28.610%   Recall: 50.805%
Train   Epoch: 293 / 600   Loss:    9558   Precision: 67.435%   Recall: 59.454%
Valid                   Loss: 2.122e+04   Precision: 27.613%   Recall: 55.933%
Train   Epoch: 294 / 600   Loss:    9380   Precision: 67.325%   Recall: 60.427%
Valid                   Loss: 2.154e+04   Precision: 27.803%   Recall: 54.264%
Train   Epoch: 295 / 600   Loss: 1.011e+04   Precision: 66.682%   Recall: 59.266%
Valid                   Loss: 2.129e+04   Precision: 30.447%   Recall: 52.415%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[59.35404968 49.55      ]
	 [32.09508896 21.35      ]
	 [44.43769455 53.08333333]
	 [69.86066437 33.41666667]
	 [56.80069733 26.6       ]]
Train   Epoch: 296 / 600   Loss: 1.006e+04   Precision: 64.919%   Recall: 60.637%
Valid                   Loss: 2.149e+04   Precision: 29.970%   Recall: 52.952%
Train   Epoch: 297 / 600   Loss:    9269   Precision: 66.352%   Recall: 60.792%
Valid                   Loss: 2.164e+04   Precision: 29.446%   Recall: 57.066%
Train   Epoch: 298 / 600   Loss:    9695   Precision: 67.155%   Recall: 60.441%
Valid                   Loss: 2.138e+04   Precision: 27.234%   Recall: 56.708%
Train   Epoch: 299 / 600   Loss:    9884   Precision: 66.014%   Recall: 62.577%
Valid                   Loss: 2.179e+04   Precision: 27.751%   Recall: 54.144%
Train   Epoch: 300 / 600   Loss: 1.015e+04   Precision: 63.835%   Recall: 60.959%
Valid                   Loss: 2.216e+04   Precision: 28.159%   Recall: 64.043%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 43.13951111  39.05      ]
	 [ 51.98142242  25.58333333]
	 [131.30603027  47.38333333]
	 [152.730896    97.3       ]
	 [ 37.14520645  41.61666667]]
Train   Epoch: 301 / 600   Loss: 1.363e+04   Precision: 52.346%   Recall: 61.173%
Valid                   Loss: 2.106e+04   Precision: 22.538%   Recall: 67.680%
Train   Epoch: 302 / 600   Loss: 1.382e+04   Precision: 56.351%   Recall: 65.481%
Valid                   Loss: 2.075e+04   Precision: 26.169%   Recall: 62.075%
Train   Epoch: 303 / 600   Loss: 1.315e+04   Precision: 58.565%   Recall: 64.275%
Valid                   Loss: 2.1e+04   Precision: 25.705%   Recall: 63.626%
Train   Epoch: 304 / 600   Loss: 1.229e+04   Precision: 61.171%   Recall: 63.454%
Valid                   Loss: 2.112e+04   Precision: 30.706%   Recall: 46.691%
Train   Epoch: 305 / 600   Loss: 1.172e+04   Precision: 62.966%   Recall: 59.571%
Valid                   Loss: 2.158e+04   Precision: 28.764%   Recall: 44.425%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[67.72793579 19.33333333]
	 [46.40282059 44.26666667]
	 [21.56415176 47.96666667]
	 [88.13576508 36.08333333]
	 [53.16550446 46.78333333]]
Train   Epoch: 306 / 600   Loss: 1.12e+04   Precision: 64.766%   Recall: 55.854%
Valid                   Loss: 2.095e+04   Precision: 29.599%   Recall: 49.314%
Train   Epoch: 307 / 600   Loss: 1.114e+04   Precision: 64.944%   Recall: 57.144%
Valid                   Loss: 2.156e+04   Precision: 28.263%   Recall: 57.841%
Train   Epoch: 308 / 600   Loss: 1.07e+04   Precision: 65.558%   Recall: 60.536%
Valid                   Loss: 2.068e+04   Precision: 26.891%   Recall: 50.030%
Train   Epoch: 309 / 600   Loss: 1.07e+04   Precision: 63.964%   Recall: 62.309%
Valid                   Loss: 2.13e+04   Precision: 27.615%   Recall: 53.846%
Train   Epoch: 310 / 600   Loss: 1.127e+04   Precision: 62.555%   Recall: 61.381%
Valid                   Loss: 2.061e+04   Precision: 26.258%   Recall: 55.993%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[51.93332672 20.7       ]
	 [43.90264893 41.13333333]
	 [58.9006958  23.43333333]
	 [78.08531952 72.78333333]
	 [47.38647079 78.4       ]]
Train   Epoch: 311 / 600   Loss: 1.14e+04   Precision: 62.710%   Recall: 59.399%
Valid                   Loss: 2.154e+04   Precision: 30.197%   Recall: 52.057%
Train   Epoch: 312 / 600   Loss: 1.082e+04   Precision: 65.856%   Recall: 59.773%
Valid                   Loss: 2.095e+04   Precision: 29.612%   Recall: 59.153%
Train   Epoch: 313 / 600   Loss: 1.013e+04   Precision: 64.930%   Recall: 59.093%
Valid                   Loss: 2.112e+04   Precision: 30.366%   Recall: 50.447%
Train   Epoch: 314 / 600   Loss:    9505   Precision: 67.305%   Recall: 58.340%
Valid                   Loss: 2.115e+04   Precision: 29.780%   Recall: 50.805%
Train   Epoch: 315 / 600   Loss:    9742   Precision: 66.098%   Recall: 59.155%
Valid                   Loss: 2.124e+04   Precision: 30.080%   Recall: 49.255%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[68.76530457 18.66666667]
	 [44.83386612 16.        ]
	 [65.71056366 20.5       ]
	 [52.85842133 78.66666667]
	 [33.1578064  23.08333333]]
Train   Epoch: 316 / 600   Loss:    9354   Precision: 67.407%   Recall: 60.979%
Valid                   Loss: 2.145e+04   Precision: 26.965%   Recall: 67.084%
Train   Epoch: 317 / 600   Loss:    9022   Precision: 66.574%   Recall: 58.936%
Valid                   Loss: 2.118e+04   Precision: 29.414%   Recall: 57.722%
Train   Epoch: 318 / 600   Loss: 1.014e+04   Precision: 63.448%   Recall: 62.258%
Valid                   Loss: 2.118e+04   Precision: 31.412%   Recall: 52.654%
Train   Epoch: 319 / 600   Loss:    9543   Precision: 64.714%   Recall: 60.485%
Valid                   Loss: 2.197e+04   Precision: 29.973%   Recall: 52.654%
Train   Epoch: 320 / 600   Loss:    9777   Precision: 66.621%   Recall: 59.670%
Valid                   Loss: 2.112e+04   Precision: 28.247%   Recall: 57.185%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[100.93869019 112.78333333]
	 [ 62.29581451   9.45      ]
	 [ 43.76905441  60.78333333]
	 [ 47.98621368  33.1       ]
	 [112.97866821  33.55      ]]
Train   Epoch: 321 / 600   Loss:    8733   Precision: 68.337%   Recall: 60.864%
Valid                   Loss: 2.117e+04   Precision: 27.206%   Recall: 55.337%
Train   Epoch: 322 / 600   Loss:    8525   Precision: 67.337%   Recall: 58.340%
Valid                   Loss: 2.114e+04   Precision: 28.734%   Recall: 58.617%
Train   Epoch: 323 / 600   Loss:    8674   Precision: 67.396%   Recall: 57.504%
Valid                   Loss: 2.129e+04   Precision: 29.954%   Recall: 54.800%
Train   Epoch: 324 / 600   Loss:    8527   Precision: 68.133%   Recall: 60.372%
Valid                   Loss: 2.12e+04   Precision: 31.552%   Recall: 47.883%
Train   Epoch: 325 / 600   Loss:    9683   Precision: 65.038%   Recall: 61.183%
Valid                   Loss: 2.158e+04   Precision: 29.845%   Recall: 51.699%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[122.74203491  78.66666667]
	 [ 58.09109116  16.8       ]
	 [ 56.18214035  15.66666667]
	 [ 47.04910278  26.43333333]
	 [ 61.47277451  55.48333333]]
Train   Epoch: 326 / 600   Loss:    8934   Precision: 67.799%   Recall: 60.678%
Valid                   Loss: 2.203e+04   Precision: 26.754%   Recall: 62.314%
Train   Epoch: 327 / 600   Loss:    9600   Precision: 65.967%   Recall: 60.062%
Valid                   Loss: 2.147e+04   Precision: 28.856%   Recall: 38.044%
Train   Epoch: 328 / 600   Loss:    9648   Precision: 67.703%   Recall: 61.255%
Valid                   Loss: 2.166e+04   Precision: 26.930%   Recall: 64.699%
Train   Epoch: 329 / 600   Loss:    9020   Precision: 67.988%   Recall: 61.868%
Valid                   Loss: 2.131e+04   Precision: 33.063%   Recall: 46.273%
Train   Epoch: 330 / 600   Loss:    9122   Precision: 68.464%   Recall: 60.709%
Valid                   Loss: 2.149e+04   Precision: 30.844%   Recall: 57.126%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[63.1541214  43.2       ]
	 [74.58315277 31.25      ]
	 [56.64138031 40.5       ]
	 [53.49605942 24.81666667]
	 [89.81612396 28.58333333]]
Train   Epoch: 331 / 600   Loss:    8512   Precision: 69.190%   Recall: 58.505%
Valid                   Loss: 2.125e+04   Precision: 30.127%   Recall: 48.002%
Train   Epoch: 332 / 600   Loss:    8552   Precision: 67.470%   Recall: 58.019%
Valid                   Loss: 2.139e+04   Precision: 26.182%   Recall: 64.699%
Train   Epoch: 333 / 600   Loss:    8433   Precision: 68.734%   Recall: 60.670%
Valid                   Loss: 2.134e+04   Precision: 30.201%   Recall: 43.113%
Train   Epoch: 334 / 600   Loss:    9687   Precision: 67.268%   Recall: 61.124%
Valid                   Loss: 2.119e+04   Precision: 26.660%   Recall: 46.452%
Train   Epoch: 335 / 600   Loss:    8460   Precision: 68.658%   Recall: 62.070%
Valid                   Loss: 2.119e+04   Precision: 28.434%   Recall: 49.493%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[92.51578522 33.95      ]
	 [53.05604553 44.78333333]
	 [65.38402557 45.2       ]
	 [35.96680832 39.4       ]
	 [41.10348892 35.95      ]]
Train   Epoch: 336 / 600   Loss:    9297   Precision: 63.401%   Recall: 58.505%
Valid                   Loss: 2.112e+04   Precision: 27.796%   Recall: 51.878%
Train   Epoch: 337 / 600   Loss:    8900   Precision: 67.553%   Recall: 58.520%
Valid                   Loss: 2.123e+04   Precision: 27.803%   Recall: 53.965%
Train   Epoch: 338 / 600   Loss:    8485   Precision: 68.189%   Recall: 59.268%
Valid                   Loss: 2.152e+04   Precision: 27.972%   Recall: 52.475%
Train   Epoch: 339 / 600   Loss:    8644   Precision: 68.185%   Recall: 59.091%
Valid                   Loss: 2.114e+04   Precision: 30.480%   Recall: 52.654%
Train   Epoch: 340 / 600   Loss:    8841   Precision: 67.402%   Recall: 58.897%
Valid                   Loss: 2.147e+04   Precision: 28.464%   Recall: 54.144%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 96.27887726  17.55      ]
	 [ 64.09082031  38.31666667]
	 [ 46.67983246  61.9       ]
	 [112.33992767  16.75      ]
	 [ 48.46548462  32.45      ]]
Train   Epoch: 341 / 600   Loss:    8959   Precision: 68.417%   Recall: 59.918%
Valid                   Loss: 2.188e+04   Precision: 24.941%   Recall: 56.708%
Train   Epoch: 342 / 600   Loss:    8831   Precision: 68.418%   Recall: 58.792%
Valid                   Loss: 2.13e+04   Precision: 29.684%   Recall: 50.924%
Train   Epoch: 343 / 600   Loss:    8214   Precision: 69.192%   Recall: 59.592%
Valid                   Loss: 2.148e+04   Precision: 28.151%   Recall: 53.011%
Train   Epoch: 344 / 600   Loss:    8322   Precision: 66.985%   Recall: 57.876%
Valid                   Loss: 2.135e+04   Precision: 26.512%   Recall: 60.644%
Train   Epoch: 345 / 600   Loss:    8898   Precision: 66.440%   Recall: 58.454%
Valid                   Loss: 2.194e+04   Precision: 26.876%   Recall: 64.281%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 76.05350494  48.51666667]
	 [100.8141861   17.75      ]
	 [ 46.57814026  34.41666667]
	 [ 55.01652908  17.51666667]
	 [107.69512177  58.5       ]]
Train   Epoch: 346 / 600   Loss:    8924   Precision: 67.240%   Recall: 59.678%
Valid                   Loss: 2.157e+04   Precision: 28.299%   Recall: 56.530%
Train   Epoch: 347 / 600   Loss:    8290   Precision: 69.048%   Recall: 61.175%
Valid                   Loss: 2.164e+04   Precision: 32.258%   Recall: 46.512%
Train   Epoch: 348 / 600   Loss:    8540   Precision: 69.291%   Recall: 61.049%
Valid                   Loss: 2.146e+04   Precision: 30.931%   Recall: 53.488%
Train   Epoch: 349 / 600   Loss:    8700   Precision: 68.288%   Recall: 59.617%
Valid                   Loss: 2.124e+04   Precision: 26.365%   Recall: 47.227%
Train   Epoch: 350 / 600   Loss:    8889   Precision: 68.478%   Recall: 59.091%
Valid                   Loss: 2.138e+04   Precision: 29.096%   Recall: 61.002%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[176.1194458   60.4       ]
	 [ 73.78840637  16.71666667]
	 [ 62.35268021  22.6       ]
	 [ 50.45038223  29.3       ]
	 [ 47.60611725  25.66666667]]
Train   Epoch: 351 / 600   Loss:    9052   Precision: 68.020%   Recall: 59.936%
Valid                   Loss: 2.133e+04   Precision: 28.592%   Recall: 59.094%
Train   Epoch: 352 / 600   Loss:    8124   Precision: 68.370%   Recall: 58.885%
Valid                   Loss: 2.11e+04   Precision: 30.727%   Recall: 53.190%
Train   Epoch: 353 / 600   Loss:    8322   Precision: 69.227%   Recall: 59.419%
Valid                   Loss: 2.12e+04   Precision: 29.132%   Recall: 53.608%
Train   Epoch: 354 / 600   Loss:    8380   Precision: 69.801%   Recall: 59.928%
Valid                   Loss: 2.103e+04   Precision: 32.968%   Recall: 35.897%
Train   Epoch: 355 / 600   Loss:    7551   Precision: 69.477%   Recall: 60.555%
Valid                   Loss: 2.141e+04   Precision: 29.351%   Recall: 52.892%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[77.76689148 30.91666667]
	 [42.44747925 52.83333333]
	 [44.93851089 34.65      ]
	 [36.63607407 25.66666667]
	 [35.80945969 44.05      ]]
Train   Epoch: 356 / 600   Loss:    8541   Precision: 68.278%   Recall: 60.410%
Valid                   Loss: 2.127e+04   Precision: 29.018%   Recall: 45.438%
Train   Epoch: 357 / 600   Loss:    9331   Precision: 68.188%   Recall: 59.598%
Valid                   Loss: 2.135e+04   Precision: 29.212%   Recall: 47.764%
Train   Epoch: 358 / 600   Loss:    8492   Precision: 68.202%   Recall: 59.520%
Valid                   Loss: 2.11e+04   Precision: 30.999%   Recall: 52.534%
Train   Epoch: 359 / 600   Loss:    8370   Precision: 68.994%   Recall: 60.402%
Valid                   Loss: 2.131e+04   Precision: 29.715%   Recall: 49.135%
Train   Epoch: 360 / 600   Loss:    7666   Precision: 69.283%   Recall: 60.247%
Valid                   Loss: 2.133e+04   Precision: 30.134%   Recall: 45.677%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[92.10413361 46.88333333]
	 [39.6198349  57.        ]
	 [61.0302124  25.7       ]
	 [77.62585449 84.28333333]
	 [31.66911697 19.38333333]]
Train   Epoch: 361 / 600   Loss:    8171   Precision: 70.000%   Recall: 60.462%
Valid                   Loss: 2.132e+04   Precision: 27.273%   Recall: 54.741%
Train   Epoch: 362 / 600   Loss:    7976   Precision: 69.925%   Recall: 60.835%
Valid                   Loss: 2.156e+04   Precision: 29.004%   Recall: 45.140%
Train   Epoch: 363 / 600   Loss:    7872   Precision: 71.450%   Recall: 59.998%
Valid                   Loss: 2.124e+04   Precision: 28.898%   Recall: 57.364%
Train   Epoch: 364 / 600   Loss:    7903   Precision: 68.886%   Recall: 60.678%
Valid                   Loss: 2.123e+04   Precision: 27.054%   Recall: 60.465%
Train   Epoch: 365 / 600   Loss:    7949   Precision: 68.681%   Recall: 61.629%
Valid                   Loss: 2.132e+04   Precision: 29.625%   Recall: 55.575%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[118.59397888  28.        ]
	 [ 43.50387573  39.08333333]
	 [ 52.97013474  25.26666667]
	 [ 81.90316772  62.78333333]
	 [ 50.81402588  70.51666667]]
Train   Epoch: 366 / 600   Loss:    8106   Precision: 69.852%   Recall: 59.441%
Valid                   Loss: 2.162e+04   Precision: 27.688%   Recall: 54.204%
Train   Epoch: 367 / 600   Loss:    8370   Precision: 68.207%   Recall: 60.866%
Valid                   Loss: 2.093e+04   Precision: 29.903%   Recall: 58.915%
Train   Epoch: 368 / 600   Loss:    8348   Precision: 68.945%   Recall: 60.297%
Valid                   Loss: 2.183e+04   Precision: 31.354%   Recall: 45.975%
Train   Epoch: 369 / 600   Loss:    8062   Precision: 69.812%   Recall: 60.604%
Valid                   Loss: 2.134e+04   Precision: 30.723%   Recall: 48.897%
Train   Epoch: 370 / 600   Loss:    9757   Precision: 64.637%   Recall: 61.379%
Valid                   Loss: 2.11e+04   Precision: 21.651%   Recall: 75.373%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 65.29911804  43.28333333]
	 [ 46.66789627  45.        ]
	 [ 47.67671585  23.5       ]
	 [ 41.25035095  47.71666667]
	 [115.02767944  28.25      ]]
Train   Epoch: 371 / 600   Loss: 1.449e+04   Precision: 55.984%   Recall: 64.577%
Valid                   Loss: 2.24e+04   Precision: 20.070%   Recall: 81.574%
Train   Epoch: 372 / 600   Loss: 1.381e+04   Precision: 57.923%   Recall: 59.918%
Valid                   Loss: 2.268e+04   Precision: 22.207%   Recall: 49.791%
Train   Epoch: 373 / 600   Loss:    8558   Precision: 68.270%   Recall: 58.959%
Valid                   Loss: 2.131e+04   Precision: 30.315%   Recall: 47.108%
Train   Epoch: 374 / 600   Loss:    8384   Precision: 68.267%   Recall: 58.466%
Valid                   Loss: 2.175e+04   Precision: 31.990%   Recall: 38.342%
Train   Epoch: 375 / 600   Loss: 1.198e+04   Precision: 58.214%   Recall: 62.606%
Valid                   Loss: 2.084e+04   Precision: 28.917%   Recall: 52.713%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 27.36320877  42.96666667]
	 [ 52.72161484  25.88333333]
	 [116.90625     35.83333333]
	 [ 37.35139084  27.11666667]
	 [ 49.33592987  28.5       ]]
Train   Epoch: 376 / 600   Loss:    9481   Precision: 66.747%   Recall: 60.142%
Valid                   Loss: 2.138e+04   Precision: 25.452%   Recall: 61.300%
Train   Epoch: 377 / 600   Loss:    8416   Precision: 69.003%   Recall: 57.524%
Valid                   Loss: 2.129e+04   Precision: 30.181%   Recall: 55.575%
Train   Epoch: 378 / 600   Loss:    7663   Precision: 70.049%   Recall: 58.988%
Valid                   Loss: 2.143e+04   Precision: 28.062%   Recall: 59.153%
Train   Epoch: 379 / 600   Loss:    8306   Precision: 71.327%   Recall: 61.049%
Valid                   Loss: 2.126e+04   Precision: 32.174%   Recall: 41.383%
Train   Epoch: 380 / 600   Loss:    8078   Precision: 70.408%   Recall: 63.113%
Valid                   Loss: 2.155e+04   Precision: 28.966%   Recall: 61.837%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[42.39654541 43.01666667]
	 [86.46586609 17.83333333]
	 [95.83596039 73.9       ]
	 [62.26205063 17.55      ]
	 [49.39706039 37.93333333]]
Train   Epoch: 381 / 600   Loss:    8086   Precision: 71.197%   Recall: 62.052%
Valid                   Loss: 2.134e+04   Precision: 30.477%   Recall: 51.103%
Train   Epoch: 382 / 600   Loss:    7991   Precision: 70.109%   Recall: 62.066%
Valid                   Loss: 2.153e+04   Precision: 30.811%   Recall: 44.425%
Train   Epoch: 383 / 600   Loss:    7578   Precision: 71.394%   Recall: 60.812%
Valid                   Loss: 2.145e+04   Precision: 30.968%   Recall: 49.434%
Train   Epoch: 384 / 600   Loss:    7616   Precision: 70.749%   Recall: 61.289%
Valid                   Loss: 2.152e+04   Precision: 30.561%   Recall: 53.250%
Train   Epoch: 385 / 600   Loss:    7897   Precision: 69.719%   Recall: 62.000%
Valid                   Loss: 2.123e+04   Precision: 29.639%   Recall: 49.911%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 61.42625046  98.83333333]
	 [ 39.49231339  56.61666667]
	 [116.87234497  61.76666667]
	 [ 44.38313293  16.83333333]
	 [100.53358459  22.5       ]]
Train   Epoch: 386 / 600   Loss:    7249   Precision: 70.850%   Recall: 61.289%
Valid                   Loss: 2.183e+04   Precision: 24.026%   Recall: 68.396%
Train   Epoch: 387 / 600   Loss:    7430   Precision: 71.804%   Recall: 60.619%
Valid                   Loss: 2.157e+04   Precision: 28.357%   Recall: 53.011%
Train   Epoch: 388 / 600   Loss:    7005   Precision: 72.037%   Recall: 60.462%
Valid                   Loss: 2.11e+04   Precision: 32.589%   Recall: 46.094%
Train   Epoch: 389 / 600   Loss:    7598   Precision: 70.999%   Recall: 61.103%
Valid                   Loss: 2.141e+04   Precision: 28.282%   Recall: 56.649%
Train   Epoch: 390 / 600   Loss:    7554   Precision: 69.979%   Recall: 62.781%
Valid                   Loss: 2.157e+04   Precision: 30.247%   Recall: 34.287%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 44.18008041  21.21666667]
	 [289.01651001 386.25      ]
	 [ 46.69960403  43.        ]
	 [149.81095886 184.86666667]
	 [ 48.97548294  43.08333333]]
Train   Epoch: 391 / 600   Loss:    7345   Precision: 71.640%   Recall: 61.317%
Valid                   Loss: 2.132e+04   Precision: 30.436%   Recall: 47.406%
Train   Epoch: 392 / 600   Loss:    7155   Precision: 71.295%   Recall: 61.990%
Valid                   Loss: 2.175e+04   Precision: 34.249%   Recall: 41.622%
Train   Epoch: 393 / 600   Loss:    7733   Precision: 70.997%   Recall: 60.289%
Valid                   Loss: 2.132e+04   Precision: 32.075%   Recall: 41.026%
Train   Epoch: 394 / 600   Loss:    7565   Precision: 70.419%   Recall: 60.874%
Valid                   Loss: 2.197e+04   Precision: 28.801%   Recall: 55.456%
Train   Epoch: 395 / 600   Loss:    7840   Precision: 71.072%   Recall: 60.245%
Valid                   Loss: 2.15e+04   Precision: 29.042%   Recall: 53.131%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[50.27630997 25.        ]
	 [13.16645718  2.15      ]
	 [47.78994751 70.46666667]
	 [46.81736374 40.36666667]
	 [56.39774704 28.7       ]]
Train   Epoch: 396 / 600   Loss:    7383   Precision: 71.229%   Recall: 61.639%
Valid                   Loss: 2.116e+04   Precision: 30.230%   Recall: 52.475%
Train   Epoch: 397 / 600   Loss:    7595   Precision: 70.161%   Recall: 61.546%
Valid                   Loss: 2.129e+04   Precision: 30.609%   Recall: 43.769%
Train   Epoch: 398 / 600   Loss:    6887   Precision: 71.688%   Recall: 61.128%
Valid                   Loss: 2.16e+04   Precision: 30.358%   Recall: 53.131%
Train   Epoch: 399 / 600   Loss:    6722   Precision: 72.988%   Recall: 63.111%
Valid                   Loss: 2.16e+04   Precision: 29.900%   Recall: 39.296%
Train   Epoch: 400 / 600   Loss:    7096   Precision: 72.822%   Recall: 61.268%
Valid                   Loss: 2.162e+04   Precision: 25.980%   Recall: 54.562%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 45.10221863  22.66666667]
	 [ 38.734272    61.        ]
	 [ 95.69615173 161.58333333]
	 [359.70349121  83.53333333]
	 [100.88019562  87.        ]]
Train   Epoch: 401 / 600   Loss:    7716   Precision: 69.852%   Recall: 60.730%
Valid                   Loss: 2.15e+04   Precision: 30.826%   Recall: 44.723%
Train   Epoch: 402 / 600   Loss:    8122   Precision: 72.482%   Recall: 62.320%
Valid                   Loss: 2.167e+04   Precision: 26.520%   Recall: 56.947%
Train   Epoch: 403 / 600   Loss:    6685   Precision: 72.631%   Recall: 62.649%
Valid                   Loss: 2.148e+04   Precision: 29.747%   Recall: 51.938%
Train   Epoch: 404 / 600   Loss:    7047   Precision: 72.882%   Recall: 63.220%
Valid                   Loss: 2.187e+04   Precision: 25.238%   Recall: 63.268%
Train   Epoch: 405 / 600   Loss:    6990   Precision: 72.667%   Recall: 63.505%
Valid                   Loss: 2.146e+04   Precision: 29.313%   Recall: 48.122%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 58.5630455   18.5       ]
	 [ 50.60087967  56.28333333]
	 [ 84.24983215 175.03333333]
	 [ 46.94483185  19.        ]
	 [ 45.93361282  49.        ]]
Train   Epoch: 406 / 600   Loss:    7144   Precision: 71.601%   Recall: 61.276%
Valid                   Loss: 2.176e+04   Precision: 31.063%   Recall: 45.140%
Train   Epoch: 407 / 600   Loss:    7206   Precision: 70.639%   Recall: 63.234%
Valid                   Loss: 2.224e+04   Precision: 24.844%   Recall: 68.634%
Train   Epoch: 408 / 600   Loss:    7150   Precision: 72.080%   Recall: 63.928%
Valid                   Loss: 2.136e+04   Precision: 28.934%   Recall: 50.984%
Train   Epoch: 409 / 600   Loss:    7302   Precision: 71.851%   Recall: 62.667%
Valid                   Loss: 2.175e+04   Precision: 30.620%   Recall: 34.764%
Train   Epoch: 410 / 600   Loss:    7285   Precision: 72.640%   Recall: 64.090%
Valid                   Loss: 2.161e+04   Precision: 28.982%   Recall: 49.255%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 58.18740082  34.83333333]
	 [ 55.72742081  35.45      ]
	 [282.1305542  322.        ]
	 [ 67.73628235  74.88333333]
	 [ 45.99969864  40.65      ]]
Train   Epoch: 411 / 600   Loss:    6944   Precision: 72.994%   Recall: 61.715%
Valid                   Loss: 2.179e+04   Precision: 30.270%   Recall: 46.750%
Train   Epoch: 412 / 600   Loss:    6458   Precision: 74.679%   Recall: 61.333%
Valid                   Loss: 2.267e+04   Precision: 29.724%   Recall: 39.773%
Train   Epoch: 413 / 600   Loss:    7236   Precision: 72.253%   Recall: 62.443%
Valid                   Loss: 2.15e+04   Precision: 24.285%   Recall: 57.245%
Train   Epoch: 414 / 600   Loss:    7111   Precision: 73.300%   Recall: 61.348%
Valid                   Loss: 2.141e+04   Precision: 28.936%   Recall: 54.800%
Train   Epoch: 415 / 600   Loss:    6944   Precision: 73.301%   Recall: 61.392%
Valid                   Loss: 2.162e+04   Precision: 28.952%   Recall: 50.566%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 27.99224472  47.96666667]
	 [138.56285095 164.83333333]
	 [220.99368286 246.33333333]
	 [ 52.19828033  56.61666667]
	 [ 59.62316513  39.        ]]
Train   Epoch: 416 / 600   Loss:    6087   Precision: 73.471%   Recall: 62.513%
Valid                   Loss: 2.147e+04   Precision: 30.676%   Recall: 33.274%
Train   Epoch: 417 / 600   Loss:    6430   Precision: 73.766%   Recall: 62.990%
Valid                   Loss: 2.142e+04   Precision: 30.280%   Recall: 41.205%
Train   Epoch: 418 / 600   Loss:    6984   Precision: 71.749%   Recall: 62.224%
Valid                   Loss: 2.159e+04   Precision: 30.927%   Recall: 44.961%
Train   Epoch: 419 / 600   Loss:    6703   Precision: 74.994%   Recall: 63.072%
Valid                   Loss: 2.169e+04   Precision: 28.092%   Recall: 39.416%
Train   Epoch: 420 / 600   Loss:    6327   Precision: 74.586%   Recall: 63.206%
Valid                   Loss: 2.18e+04   Precision: 31.277%   Recall: 42.636%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[48.59835434 27.33333333]
	 [56.84965897 32.71666667]
	 [68.77970123 26.03333333]
	 [58.15711975 26.73333333]
	 [44.7169838  42.01666667]]
Train   Epoch: 421 / 600   Loss:    7948   Precision: 69.516%   Recall: 64.134%
Valid                   Loss: 2.141e+04   Precision: 32.083%   Recall: 36.732%
Train   Epoch: 422 / 600   Loss:    6954   Precision: 72.378%   Recall: 61.740%
Valid                   Loss: 2.157e+04   Precision: 27.220%   Recall: 44.425%
Train   Epoch: 423 / 600   Loss:    7058   Precision: 72.717%   Recall: 61.476%
Valid                   Loss: 2.233e+04   Precision: 23.578%   Recall: 64.758%
Train   Epoch: 424 / 600   Loss:    7330   Precision: 70.314%   Recall: 60.740%
Valid                   Loss: 2.127e+04   Precision: 25.273%   Recall: 51.044%
Train   Epoch: 425 / 600   Loss:    7273   Precision: 72.000%   Recall: 61.979%
Valid                   Loss: 2.158e+04   Precision: 28.359%   Recall: 46.571%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 39.84243011  53.11666667]
	 [ 46.38912582  37.05      ]
	 [445.10391235 306.11666667]
	 [100.04705811  22.5       ]
	 [ 57.61604691  21.16666667]]
Train   Epoch: 426 / 600   Loss:    6669   Precision: 74.317%   Recall: 62.482%
Valid                   Loss: 2.18e+04   Precision: 27.049%   Recall: 60.823%
Train   Epoch: 427 / 600   Loss:    6238   Precision: 74.236%   Recall: 63.616%
Valid                   Loss: 2.135e+04   Precision: 28.984%   Recall: 44.902%
Train   Epoch: 428 / 600   Loss:    6153   Precision: 75.528%   Recall: 63.763%
Valid                   Loss: 2.143e+04   Precision: 28.338%   Recall: 47.585%
Train   Epoch: 429 / 600   Loss:    6025   Precision: 75.385%   Recall: 62.202%
Valid                   Loss: 2.159e+04   Precision: 30.066%   Recall: 40.787%
Train   Epoch: 430 / 600   Loss:    5992   Precision: 76.025%   Recall: 64.433%
Valid                   Loss: 2.181e+04   Precision: 28.081%   Recall: 52.713%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 44.43173218  16.11666667]
	 [ 54.98156357  70.46666667]
	 [141.82345581 215.45      ]
	 [ 30.15184975  41.11666667]
	 [ 65.47974396  61.11666667]]
Train   Epoch: 431 / 600   Loss:    6222   Precision: 74.146%   Recall: 61.987%
Valid                   Loss: 2.177e+04   Precision: 30.154%   Recall: 44.305%
Train   Epoch: 432 / 600   Loss:    6364   Precision: 75.692%   Recall: 65.121%
Valid                   Loss: 2.183e+04   Precision: 26.506%   Recall: 50.626%
Train   Epoch: 433 / 600   Loss:    5931   Precision: 73.827%   Recall: 63.573%
Valid                   Loss: 2.166e+04   Precision: 31.449%   Recall: 36.494%
Train   Epoch: 434 / 600   Loss:    6419   Precision: 75.332%   Recall: 63.753%
Valid                   Loss: 2.153e+04   Precision: 27.332%   Recall: 55.039%
Train   Epoch: 435 / 600   Loss:    6706   Precision: 74.842%   Recall: 63.379%
Valid                   Loss: 2.146e+04   Precision: 28.460%   Recall: 50.268%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[29.94825554 38.55      ]
	 [40.97861862 26.96666667]
	 [50.09140015 27.83333333]
	 [52.11893082 48.66666667]
	 [57.25300217 45.31666667]]
Train   Epoch: 436 / 600   Loss:    6637   Precision: 73.122%   Recall: 62.907%
Valid                   Loss: 2.243e+04   Precision: 30.401%   Recall: 43.888%
Train   Epoch: 437 / 600   Loss:    6655   Precision: 74.409%   Recall: 60.658%
Valid                   Loss: 2.175e+04   Precision: 29.552%   Recall: 42.099%
Train   Epoch: 438 / 600   Loss:    5699   Precision: 76.702%   Recall: 65.011%
Valid                   Loss: 2.179e+04   Precision: 30.182%   Recall: 48.539%
Train   Epoch: 439 / 600   Loss:    5992   Precision: 75.575%   Recall: 63.688%
Valid                   Loss: 2.17e+04   Precision: 29.712%   Recall: 46.810%
Train   Epoch: 440 / 600   Loss:    6187   Precision: 75.781%   Recall: 62.472%
Valid                   Loss: 2.178e+04   Precision: 28.239%   Recall: 41.205%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[70.90219879 22.        ]
	 [56.73516083 49.78333333]
	 [99.04877472 82.06666667]
	 [61.04397964 78.25      ]
	 [47.59224319 30.38333333]]
Train   Epoch: 441 / 600   Loss:    6020   Precision: 75.263%   Recall: 64.835%
Valid                   Loss: 2.146e+04   Precision: 31.379%   Recall: 41.801%
Train   Epoch: 442 / 600   Loss:    5940   Precision: 75.224%   Recall: 64.103%
Valid                   Loss: 2.155e+04   Precision: 27.522%   Recall: 51.401%
Train   Epoch: 443 / 600   Loss:    5763   Precision: 77.284%   Recall: 63.750%
Valid                   Loss: 2.168e+04   Precision: 33.445%   Recall: 35.778%
Train   Epoch: 444 / 600   Loss:    5628   Precision: 76.757%   Recall: 64.175%
Valid                   Loss: 2.149e+04   Precision: 29.745%   Recall: 48.599%
Train   Epoch: 445 / 600   Loss:    5754   Precision: 77.085%   Recall: 62.876%
Valid                   Loss: 2.159e+04   Precision: 31.726%   Recall: 40.012%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[44.30554199 28.28333333]
	 [40.29515457 38.45      ]
	 [45.90738678 62.33333333]
	 [59.08564758 31.66666667]
	 [90.97925568 47.6       ]]
Train   Epoch: 446 / 600   Loss:    6116   Precision: 75.359%   Recall: 64.306%
Valid                   Loss: 2.145e+04   Precision: 30.564%   Recall: 42.993%
Train   Epoch: 447 / 600   Loss:    7933   Precision: 72.233%   Recall: 62.688%
Valid                   Loss: 2.244e+04   Precision: 27.380%   Recall: 51.282%
Train   Epoch: 448 / 600   Loss:    6089   Precision: 75.725%   Recall: 62.454%
Valid                   Loss: 2.151e+04   Precision: 28.395%   Recall: 46.631%
Train   Epoch: 449 / 600   Loss:    5488   Precision: 77.233%   Recall: 63.825%
Valid                   Loss: 2.161e+04   Precision: 29.308%   Recall: 35.599%
Train   Epoch: 450 / 600   Loss:    5325   Precision: 78.255%   Recall: 64.543%
Valid                   Loss: 2.155e+04   Precision: 31.683%   Recall: 41.980%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[47.71308899 21.63333333]
	 [71.70348358 24.23333333]
	 [33.25528717 25.46666667]
	 [61.08345032 20.46666667]
	 [77.35211945 99.78333333]]
Train   Epoch: 451 / 600   Loss:    5424   Precision: 78.123%   Recall: 64.451%
Valid                   Loss: 2.16e+04   Precision: 26.156%   Recall: 55.337%
Train   Epoch: 452 / 600   Loss:    5399   Precision: 77.096%   Recall: 62.832%
Valid                   Loss: 2.195e+04   Precision: 28.784%   Recall: 46.035%
Train   Epoch: 453 / 600   Loss:    5608   Precision: 77.130%   Recall: 62.877%
Valid                   Loss: 2.158e+04   Precision: 31.838%   Recall: 39.773%
Train   Epoch: 454 / 600   Loss:    5468   Precision: 78.349%   Recall: 65.162%
Valid                   Loss: 2.183e+04   Precision: 29.675%   Recall: 45.796%
Train   Epoch: 455 / 600   Loss:    5575   Precision: 77.593%   Recall: 64.461%
Valid                   Loss: 2.186e+04   Precision: 33.023%   Recall: 35.957%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[45.75082779 21.16666667]
	 [82.64890289 45.33333333]
	 [44.12747574 51.8       ]
	 [51.50099564 56.7       ]
	 [30.99812317 56.66666667]]
Train   Epoch: 456 / 600   Loss:    5708   Precision: 75.866%   Recall: 62.540%
Valid                   Loss: 2.164e+04   Precision: 26.070%   Recall: 54.860%
Train   Epoch: 457 / 600   Loss:    5861   Precision: 76.733%   Recall: 64.007%
Valid                   Loss: 2.227e+04   Precision: 28.626%   Recall: 49.553%
Train   Epoch: 458 / 600   Loss:    6483   Precision: 75.892%   Recall: 63.791%
Valid                   Loss: 2.183e+04   Precision: 32.946%   Recall: 35.480%
Train   Epoch: 459 / 600   Loss: 1.09e+04   Precision: 59.256%   Recall: 67.194%
Valid                   Loss: 2.136e+04   Precision: 29.303%   Recall: 30.352%
Train   Epoch: 460 / 600   Loss:    8528   Precision: 66.852%   Recall: 60.602%
Valid                   Loss: 2.162e+04   Precision: 28.327%   Recall: 43.411%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 32.96905518  48.68333333]
	 [ 62.2541275   38.58333333]
	 [ 39.86673737  42.58333333]
	 [215.69161987 223.3       ]
	 [ 37.93270874  63.98333333]]
Train   Epoch: 461 / 600   Loss:    5690   Precision: 76.397%   Recall: 64.701%
Valid                   Loss: 2.141e+04   Precision: 30.725%   Recall: 41.443%
Train   Epoch: 462 / 600   Loss:    5250   Precision: 78.225%   Recall: 64.681%
Valid                   Loss: 2.162e+04   Precision: 29.375%   Recall: 44.246%
Train   Epoch: 463 / 600   Loss:    4937   Precision: 78.338%   Recall: 65.021%
Valid                   Loss: 2.161e+04   Precision: 28.172%   Recall: 50.447%
Train   Epoch: 464 / 600   Loss:    5282   Precision: 77.540%   Recall: 63.946%
Valid                   Loss: 2.166e+04   Precision: 30.820%   Recall: 36.076%
Train   Epoch: 465 / 600   Loss:    5430   Precision: 77.630%   Recall: 65.492%
Valid                   Loss: 2.165e+04   Precision: 32.806%   Recall: 39.535%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 42.79692459  54.        ]
	 [ 59.34637451  42.66666667]
	 [ 35.27312469  42.66666667]
	 [ 54.75275803  45.06666667]
	 [230.52990723 281.8       ]]
Train   Epoch: 466 / 600   Loss:    6463   Precision: 75.813%   Recall: 64.646%
Valid                   Loss: 2.14e+04   Precision: 33.093%   Recall: 32.856%
Train   Epoch: 467 / 600   Loss:    5108   Precision: 78.808%   Recall: 64.323%
Valid                   Loss: 2.137e+04   Precision: 31.230%   Recall: 34.228%
Train   Epoch: 468 / 600   Loss:    4968   Precision: 79.063%   Recall: 65.247%
Valid                   Loss: 2.204e+04   Precision: 28.659%   Recall: 44.723%
Train   Epoch: 469 / 600   Loss:    4793   Precision: 79.248%   Recall: 65.420%
Valid                   Loss: 2.2e+04   Precision: 31.803%   Recall: 40.072%
Train   Epoch: 470 / 600   Loss:    5269   Precision: 78.196%   Recall: 63.793%
Valid                   Loss: 2.242e+04   Precision: 30.686%   Recall: 42.397%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[175.28733826 175.05      ]
	 [ 42.2508049   24.28333333]
	 [ 15.49129295  28.26666667]
	 [ 18.76232338  10.83333333]
	 [ 34.63311768  62.55      ]]
Train   Epoch: 471 / 600   Loss:    8580   Precision: 66.375%   Recall: 64.966%
Valid                   Loss: 2.122e+04   Precision: 32.916%   Recall: 36.076%
Train   Epoch: 472 / 600   Loss:    6790   Precision: 74.892%   Recall: 64.713%
Valid                   Loss: 2.358e+04   Precision: 23.704%   Recall: 64.878%
Train   Epoch: 473 / 600   Loss:    5822   Precision: 76.729%   Recall: 65.523%
Valid                   Loss: 2.163e+04   Precision: 31.432%   Recall: 41.741%
Train   Epoch: 474 / 600   Loss:    5002   Precision: 79.488%   Recall: 63.709%
Valid                   Loss: 2.218e+04   Precision: 31.236%   Recall: 42.636%
Train   Epoch: 475 / 600   Loss:    4689   Precision: 79.838%   Recall: 66.134%
Valid                   Loss: 2.171e+04   Precision: 29.141%   Recall: 45.319%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[34.86746979 11.33333333]
	 [63.03218079 55.9       ]
	 [40.26434708 33.66666667]
	 [50.37422943 18.6       ]
	 [40.2984848  21.56666667]]
Train   Epoch: 476 / 600   Loss:    4703   Precision: 80.641%   Recall: 64.791%
Valid                   Loss: 2.208e+04   Precision: 27.063%   Recall: 49.076%
Train   Epoch: 477 / 600   Loss:    5203   Precision: 77.952%   Recall: 64.035%
Valid                   Loss: 2.178e+04   Precision: 30.597%   Recall: 35.122%
Train   Epoch: 478 / 600   Loss:    5375   Precision: 76.041%   Recall: 62.856%
Valid                   Loss: 2.24e+04   Precision: 25.069%   Recall: 65.236%
Train   Epoch: 479 / 600   Loss:    6081   Precision: 76.594%   Recall: 64.907%
Valid                   Loss: 2.186e+04   Precision: 29.808%   Recall: 37.984%
Train   Epoch: 480 / 600   Loss:    5312   Precision: 77.367%   Recall: 65.605%
Valid                   Loss: 2.194e+04   Precision: 30.292%   Recall: 39.595%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[24.29653549 28.96666667]
	 [47.80207825 35.55      ]
	 [31.42280197 24.96666667]
	 [39.33837509 35.46666667]
	 [36.26868439 49.85      ]]
Train   Epoch: 481 / 600   Loss:    5204   Precision: 79.099%   Recall: 65.546%
Valid                   Loss: 2.146e+04   Precision: 28.833%   Recall: 45.081%
Train   Epoch: 482 / 600   Loss:    5103   Precision: 79.223%   Recall: 65.797%
Valid                   Loss: 2.14e+04   Precision: 29.902%   Recall: 36.553%
Train   Epoch: 483 / 600   Loss:    4848   Precision: 78.501%   Recall: 65.756%
Valid                   Loss: 2.18e+04   Precision: 27.772%   Recall: 45.259%
Train   Epoch: 484 / 600   Loss:    5838   Precision: 77.475%   Recall: 64.784%
Valid                   Loss: 2.22e+04   Precision: 29.381%   Recall: 46.691%
Train   Epoch: 485 / 600   Loss:    4599   Precision: 80.099%   Recall: 66.543%
Valid                   Loss: 2.251e+04   Precision: 26.573%   Recall: 52.892%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 51.41302872  22.38333333]
	 [ 68.2253418   19.38333333]
	 [-18.60308838  42.31666667]
	 [ 66.82546234   7.        ]
	 [ 47.39304733  54.91666667]]
Train   Epoch: 486 / 600   Loss:    4915   Precision: 79.544%   Recall: 65.866%
Valid                   Loss: 2.257e+04   Precision: 30.330%   Recall: 32.320%
Train   Epoch: 487 / 600   Loss:    4853   Precision: 80.354%   Recall: 65.990%
Valid                   Loss: 2.165e+04   Precision: 32.281%   Recall: 32.320%
Train   Epoch: 488 / 600   Loss:    4480   Precision: 80.484%   Recall: 66.121%
Valid                   Loss: 2.166e+04   Precision: 27.729%   Recall: 45.438%
Train   Epoch: 489 / 600   Loss:    4260   Precision: 80.719%   Recall: 67.330%
Valid                   Loss: 2.191e+04   Precision: 31.693%   Recall: 36.494%
Train   Epoch: 490 / 600   Loss:    4905   Precision: 79.655%   Recall: 66.639%
Valid                   Loss: 2.144e+04   Precision: 28.180%   Recall: 48.479%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[61.57636642 32.18333333]
	 [61.68458176  9.11666667]
	 [57.81367493 39.63333333]
	 [67.85513306 25.58333333]
	 [59.76369095 43.83333333]]
Train   Epoch: 491 / 600   Loss:    4490   Precision: 78.702%   Recall: 66.120%
Valid                   Loss: 2.17e+04   Precision: 29.318%   Recall: 46.154%
Train   Epoch: 492 / 600   Loss:    4228   Precision: 81.407%   Recall: 66.309%
Valid                   Loss: 2.178e+04   Precision: 28.315%   Recall: 49.791%
Train   Epoch: 493 / 600   Loss:    4266   Precision: 82.362%   Recall: 67.876%
Valid                   Loss: 2.178e+04   Precision: 32.157%   Recall: 29.338%
Train   Epoch: 494 / 600   Loss:    6080   Precision: 76.782%   Recall: 64.935%
Valid                   Loss: 2.138e+04   Precision: 30.254%   Recall: 39.833%
Train   Epoch: 495 / 600   Loss:    5075   Precision: 79.271%   Recall: 65.430%
Valid                   Loss: 2.187e+04   Precision: 29.944%   Recall: 31.604%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[53.97203445 91.21666667]
	 [62.90333939 24.98333333]
	 [42.16583252 27.71666667]
	 [21.61740494 13.51666667]
	 [41.84127426 30.61666667]]
Train   Epoch: 496 / 600   Loss:    5033   Precision: 78.565%   Recall: 65.244%
Valid                   Loss: 2.157e+04   Precision: 31.358%   Recall: 34.705%
Train   Epoch: 497 / 600   Loss:    4479   Precision: 80.928%   Recall: 67.790%
Valid                   Loss: 2.168e+04   Precision: 30.164%   Recall: 36.136%
Train   Epoch: 498 / 600   Loss:    4234   Precision: 81.466%   Recall: 67.155%
Valid                   Loss: 2.187e+04   Precision: 29.854%   Recall: 37.686%
Train   Epoch: 499 / 600   Loss:    4368   Precision: 81.549%   Recall: 68.285%
Valid                   Loss: 2.17e+04   Precision: 27.667%   Recall: 45.617%
Train   Epoch: 500 / 600   Loss:    4156   Precision: 81.928%   Recall: 67.206%
Valid                   Loss: 2.198e+04   Precision: 32.434%   Recall: 32.976%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[34.70501328 55.58333333]
	 [45.99491501 21.88333333]
	 [59.84671783 55.        ]
	 [62.12887573 64.25      ]
	 [27.04361534 31.25      ]]
Train   Epoch: 501 / 600   Loss:    4220   Precision: 82.018%   Recall: 66.866%
Valid                   Loss: 2.16e+04   Precision: 30.712%   Recall: 34.705%
Train   Epoch: 502 / 600   Loss:    4740   Precision: 78.738%   Recall: 66.385%
Valid                   Loss: 2.243e+04   Precision: 29.193%   Recall: 38.402%
Train   Epoch: 503 / 600   Loss:    4784   Precision: 79.897%   Recall: 67.031%
Valid                   Loss: 2.192e+04   Precision: 28.007%   Recall: 49.434%
Train   Epoch: 504 / 600   Loss:    4421   Precision: 80.322%   Recall: 66.770%
Valid                   Loss: 2.17e+04   Precision: 32.828%   Recall: 36.136%
Train   Epoch: 505 / 600   Loss:    4323   Precision: 81.500%   Recall: 65.990%
Valid                   Loss: 2.163e+04   Precision: 27.155%   Recall: 48.837%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[59.79050064 40.5       ]
	 [49.27854919 36.68333333]
	 [68.25245667 46.        ]
	 [36.56828308  9.45      ]
	 [75.59884644 26.86666667]]
Train   Epoch: 506 / 600   Loss:    4251   Precision: 80.972%   Recall: 68.216%
Valid                   Loss: 2.176e+04   Precision: 28.556%   Recall: 48.002%
Train   Epoch: 507 / 600   Loss:    4307   Precision: 80.816%   Recall: 68.577%
Valid                   Loss: 2.182e+04   Precision: 26.390%   Recall: 44.425%
Train   Epoch: 508 / 600   Loss:    4981   Precision: 79.176%   Recall: 65.564%
Valid                   Loss: 2.204e+04   Precision: 25.442%   Recall: 60.048%
Train   Epoch: 509 / 600   Loss:    4551   Precision: 80.915%   Recall: 67.485%
Valid                   Loss: 2.208e+04   Precision: 29.639%   Recall: 42.099%
Train   Epoch: 510 / 600   Loss:    4118   Precision: 82.370%   Recall: 67.289%
Valid                   Loss: 2.181e+04   Precision: 27.404%   Recall: 41.801%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 58.33041     35.55      ]
	 [ 37.29737854  20.01666667]
	 [ 55.44330978 125.4       ]
	 [ 57.45967102  92.6       ]
	 [ 31.80072975  15.53333333]]
Train   Epoch: 511 / 600   Loss:    4319   Precision: 82.326%   Recall: 67.849%
Valid                   Loss: 2.183e+04   Precision: 28.974%   Recall: 41.741%
Train   Epoch: 512 / 600   Loss:    4104   Precision: 82.174%   Recall: 67.577%
Valid                   Loss: 2.218e+04   Precision: 29.685%   Recall: 41.562%
Train   Epoch: 513 / 600   Loss:    3782   Precision: 82.866%   Recall: 67.996%
Valid                   Loss: 2.196e+04   Precision: 27.950%   Recall: 39.833%
Train   Epoch: 514 / 600   Loss:    4005   Precision: 82.340%   Recall: 67.038%
Valid                   Loss: 2.203e+04   Precision: 28.724%   Recall: 38.402%
Train   Epoch: 515 / 600   Loss:    5040   Precision: 77.637%   Recall: 66.247%
Valid                   Loss: 2.189e+04   Precision: 29.802%   Recall: 38.581%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[39.40895844 25.33333333]
	 [45.02015686 28.45      ]
	 [34.39641571 46.75      ]
	 [49.42037201 22.        ]
	 [54.11390686 36.25      ]]
Train   Epoch: 516 / 600   Loss:    4450   Precision: 81.957%   Recall: 67.948%
Valid                   Loss: 2.181e+04   Precision: 32.032%   Recall: 40.608%
Train   Epoch: 517 / 600   Loss:    4446   Precision: 81.298%   Recall: 67.670%
Valid                   Loss: 2.184e+04   Precision: 32.784%   Recall: 34.407%
Train   Epoch: 518 / 600   Loss:    4124   Precision: 82.577%   Recall: 68.439%
Valid                   Loss: 2.163e+04   Precision: 30.641%   Recall: 38.223%
Train   Epoch: 519 / 600   Loss:    4252   Precision: 82.089%   Recall: 67.990%
Valid                   Loss: 2.191e+04   Precision: 31.699%   Recall: 33.930%
Train   Epoch: 520 / 600   Loss:    3915   Precision: 82.760%   Recall: 68.938%
Valid                   Loss: 2.176e+04   Precision: 29.838%   Recall: 37.329%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[47.43229294 14.18333333]
	 [30.03369141 24.75      ]
	 [60.32411194 30.83333333]
	 [64.72953033 42.        ]
	 [10.68801212 10.83333333]]
Train   Epoch: 521 / 600   Loss:    3821   Precision: 82.634%   Recall: 68.419%
Valid                   Loss: 2.193e+04   Precision: 29.133%   Recall: 37.090%
Train   Epoch: 522 / 600   Loss:    3901   Precision: 82.745%   Recall: 68.110%
Valid                   Loss: 2.156e+04   Precision: 28.152%   Recall: 42.338%
Train   Epoch: 523 / 600   Loss:    4029   Precision: 82.401%   Recall: 67.852%
Valid                   Loss: 2.178e+04   Precision: 28.833%   Recall: 45.081%
Train   Epoch: 524 / 600   Loss:    4564   Precision: 79.528%   Recall: 67.166%
Valid                   Loss: 2.225e+04   Precision: 31.020%   Recall: 36.255%
Train   Epoch: 525 / 600   Loss:    4763   Precision: 80.194%   Recall: 66.557%
Valid                   Loss: 2.207e+04   Precision: 30.052%   Recall: 34.764%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 19.76150513  16.46666667]
	 [ 41.87163162  56.5       ]
	 [ 88.76850891  32.6       ]
	 [660.22369385 575.6       ]
	 [ 81.93344879  60.78333333]]
Train   Epoch: 526 / 600   Loss:    3857   Precision: 82.876%   Recall: 68.317%
Valid                   Loss: 2.179e+04   Precision: 31.518%   Recall: 35.897%
Train   Epoch: 527 / 600   Loss:    3379   Precision: 84.752%   Recall: 69.680%
Valid                   Loss: 2.205e+04   Precision: 29.795%   Recall: 37.329%
Train   Epoch: 528 / 600   Loss:    3463   Precision: 84.593%   Recall: 70.361%
Valid                   Loss: 2.178e+04   Precision: 33.162%   Recall: 26.953%
Train   Epoch: 529 / 600   Loss:    3580   Precision: 84.248%   Recall: 70.247%
Valid                   Loss: 2.188e+04   Precision: 29.896%   Recall: 30.769%
Train   Epoch: 530 / 600   Loss:    3430   Precision: 83.908%   Recall: 68.955%
Valid                   Loss: 2.226e+04   Precision: 27.094%   Recall: 46.094%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 94.86717224 124.25      ]
	 [ 49.23930359  40.66666667]
	 [ 47.89849854  16.41666667]
	 [ 59.45743942  51.58333333]
	 [ 59.71330643  56.5       ]]
Train   Epoch: 531 / 600   Loss:    4022   Precision: 81.524%   Recall: 68.918%
Valid                   Loss: 2.207e+04   Precision: 30.867%   Recall: 34.586%
Train   Epoch: 532 / 600   Loss:    4304   Precision: 81.616%   Recall: 68.728%
Valid                   Loss: 2.201e+04   Precision: 27.968%   Recall: 42.278%
Train   Epoch: 533 / 600   Loss:    4278   Precision: 82.097%   Recall: 69.767%
Valid                   Loss: 2.197e+04   Precision: 28.203%   Recall: 39.117%
Train   Epoch: 534 / 600   Loss:    3893   Precision: 83.292%   Recall: 68.505%
Valid                   Loss: 2.162e+04   Precision: 31.808%   Recall: 33.154%
Train   Epoch: 535 / 600   Loss:    4516   Precision: 78.630%   Recall: 67.216%
Valid                   Loss: 2.222e+04   Precision: 29.278%   Recall: 35.301%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[32.16859436 45.6       ]
	 [50.85908127 52.36666667]
	 [31.73122787 37.58333333]
	 [47.55331421 13.38333333]
	 [47.35057068 35.41666667]]
Train   Epoch: 536 / 600   Loss:    3591   Precision: 84.367%   Recall: 68.419%
Valid                   Loss: 2.17e+04   Precision: 30.878%   Recall: 35.242%
Train   Epoch: 537 / 600   Loss:    3617   Precision: 84.081%   Recall: 67.790%
Valid                   Loss: 2.206e+04   Precision: 27.103%   Recall: 43.232%
Train   Epoch: 538 / 600   Loss:    3546   Precision: 84.795%   Recall: 69.794%
Valid                   Loss: 2.19e+04   Precision: 29.171%   Recall: 33.989%
Train   Epoch: 539 / 600   Loss:    3445   Precision: 84.182%   Recall: 70.062%
Valid                   Loss: 2.189e+04   Precision: 29.618%   Recall: 33.274%
Train   Epoch: 540 / 600   Loss:    3346   Precision: 84.911%   Recall: 70.546%
Valid                   Loss: 2.189e+04   Precision: 28.203%   Recall: 37.150%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 36.04154968  29.33333333]
	 [ 53.63388824  34.36666667]
	 [111.65616608 111.4       ]
	 [ 50.35791016  38.78333333]
	 [ 39.47300339  23.83333333]]
Train   Epoch: 541 / 600   Loss:    3372   Precision: 85.711%   Recall: 70.283%
Valid                   Loss: 2.188e+04   Precision: 28.645%   Recall: 39.952%
Train   Epoch: 542 / 600   Loss:    4354   Precision: 81.404%   Recall: 70.670%
Valid                   Loss: 2.178e+04   Precision: 28.028%   Recall: 25.939%
Train   Epoch: 543 / 600   Loss:    5957   Precision: 74.660%   Recall: 67.825%
Valid                   Loss: 2.098e+04   Precision: 29.852%   Recall: 39.714%
Train   Epoch: 544 / 600   Loss:    7510   Precision: 69.564%   Recall: 62.299%
Valid                   Loss: 2.234e+04   Precision: 28.873%   Recall: 35.122%
Train   Epoch: 545 / 600   Loss:    4125   Precision: 81.100%   Recall: 67.055%
Valid                   Loss: 2.261e+04   Precision: 30.224%   Recall: 33.810%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[41.1507988  22.25      ]
	 [47.1914711  25.56666667]
	 [47.79153442 31.33333333]
	 [30.25636292 56.83333333]
	 [48.64326859 67.        ]]
Train   Epoch: 546 / 600   Loss:    3491   Precision: 84.585%   Recall: 70.243%
Valid                   Loss: 2.227e+04   Precision: 29.351%   Recall: 38.819%
Train   Epoch: 547 / 600   Loss:    3358   Precision: 86.205%   Recall: 68.852%
Valid                   Loss: 2.189e+04   Precision: 30.684%   Recall: 29.696%
Train   Epoch: 548 / 600   Loss:    3368   Precision: 84.232%   Recall: 69.707%
Valid                   Loss: 2.188e+04   Precision: 29.921%   Recall: 33.989%
Train   Epoch: 549 / 600   Loss:    3103   Precision: 85.980%   Recall: 70.113%
Valid                   Loss: 2.202e+04   Precision: 29.256%   Recall: 34.943%
Train   Epoch: 550 / 600   Loss:    3350   Precision: 85.374%   Recall: 70.274%
Valid                   Loss: 2.197e+04   Precision: 26.736%   Recall: 45.677%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 37.56396866  20.5       ]
	 [127.18254852 103.66666667]
	 [-11.54761696  17.86666667]
	 [ 99.9520874  105.53333333]
	 [112.59604645 156.98333333]]
Train   Epoch: 551 / 600   Loss:    3451   Precision: 83.861%   Recall: 70.495%
Valid                   Loss: 2.18e+04   Precision: 27.623%   Recall: 31.246%
Train   Epoch: 552 / 600   Loss:    3787   Precision: 84.284%   Recall: 69.759%
Valid                   Loss: 2.187e+04   Precision: 31.554%   Recall: 27.847%
Train   Epoch: 553 / 600   Loss:    3096   Precision: 85.194%   Recall: 70.295%
Valid                   Loss: 2.201e+04   Precision: 28.461%   Recall: 37.388%
Train   Epoch: 554 / 600   Loss:    3112   Precision: 86.356%   Recall: 71.892%
Valid                   Loss: 2.222e+04   Precision: 27.511%   Recall: 36.911%
Train   Epoch: 555 / 600   Loss:    3329   Precision: 85.077%   Recall: 71.161%
Valid                   Loss: 2.236e+04   Precision: 27.696%   Recall: 36.911%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 53.75511551  48.        ]
	 [166.95739746  53.66666667]
	 [ 57.28830719  26.85      ]
	 [ 29.62089157  36.05      ]
	 [ 28.6346817   38.55      ]]
Train   Epoch: 556 / 600   Loss:    4620   Precision: 80.078%   Recall: 67.804%
Valid                   Loss: 2.251e+04   Precision: 24.682%   Recall: 44.007%
Train   Epoch: 557 / 600   Loss:    3847   Precision: 82.609%   Recall: 68.948%
Valid                   Loss: 2.179e+04   Precision: 28.887%   Recall: 49.851%
Train   Epoch: 558 / 600   Loss:    3454   Precision: 84.187%   Recall: 71.831%
Valid                   Loss: 2.197e+04   Precision: 29.668%   Recall: 34.586%
Train   Epoch: 559 / 600   Loss:    2977   Precision: 86.192%   Recall: 72.315%
Valid                   Loss: 2.237e+04   Precision: 29.611%   Recall: 28.145%
Train   Epoch: 560 / 600   Loss:    2936   Precision: 86.404%   Recall: 72.134%
Valid                   Loss: 2.185e+04   Precision: 30.281%   Recall: 39.833%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[68.90864563 65.16666667]
	 [61.48007202 38.88333333]
	 [42.44178772 47.1       ]
	 [52.40615463 32.21666667]
	 [40.55335236 37.45      ]]
Train   Epoch: 561 / 600   Loss:    3312   Precision: 84.492%   Recall: 70.546%
Valid                   Loss: 2.189e+04   Precision: 30.127%   Recall: 33.989%
Train   Epoch: 562 / 600   Loss:    2898   Precision: 86.923%   Recall: 71.268%
Valid                   Loss: 2.176e+04   Precision: 28.035%   Recall: 44.067%
Train   Epoch: 563 / 600   Loss:    3125   Precision: 85.966%   Recall: 71.546%
Valid                   Loss: 2.226e+04   Precision: 28.284%   Recall: 38.521%
Train   Epoch: 564 / 600   Loss:    4227   Precision: 81.306%   Recall: 68.903%
Valid                   Loss: 2.176e+04   Precision: 25.324%   Recall: 43.113%
Train   Epoch: 565 / 600   Loss:    3216   Precision: 85.634%   Recall: 70.918%
Valid                   Loss: 2.189e+04   Precision: 29.555%   Recall: 28.145%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[48.04547119 61.11666667]
	 [87.54137421 17.4       ]
	 [55.96107101 17.28333333]
	 [52.98965073 79.        ]
	 [20.9539032  22.5       ]]
Train   Epoch: 566 / 600   Loss:    2914   Precision: 85.811%   Recall: 71.686%
Valid                   Loss: 2.185e+04   Precision: 29.497%   Recall: 36.374%
Train   Epoch: 567 / 600   Loss:    4252   Precision: 81.682%   Recall: 68.573%
Valid                   Loss: 2.176e+04   Precision: 28.847%   Recall: 35.659%
Train   Epoch: 568 / 600   Loss:    3312   Precision: 84.878%   Recall: 72.779%
Valid                   Loss: 2.179e+04   Precision: 31.528%   Recall: 30.888%
Train   Epoch: 569 / 600   Loss:    3523   Precision: 84.092%   Recall: 70.284%
Valid                   Loss: 2.183e+04   Precision: 28.339%   Recall: 39.475%
Train   Epoch: 570 / 600   Loss:    7214   Precision: 75.177%   Recall: 66.653%
Valid                   Loss: 2.203e+04   Precision: 30.358%   Recall: 36.911%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 53.29371643  41.78333333]
	 [ 53.70321655  29.7       ]
	 [112.20754242  66.38333333]
	 [ 43.11116409  31.        ]
	 [ 43.55237198  32.13333333]]
Train   Epoch: 571 / 600   Loss:    4105   Precision: 83.051%   Recall: 68.285%
Valid                   Loss: 2.176e+04   Precision: 28.884%   Recall: 37.806%
Train   Epoch: 572 / 600   Loss:    3211   Precision: 85.850%   Recall: 69.666%
Valid                   Loss: 2.188e+04   Precision: 27.115%   Recall: 47.585%
Train   Epoch: 573 / 600   Loss:    3049   Precision: 86.489%   Recall: 70.082%
Valid                   Loss: 2.153e+04   Precision: 30.146%   Recall: 37.030%
Train   Epoch: 574 / 600   Loss:    2890   Precision: 86.202%   Recall: 70.268%
Valid                   Loss: 2.184e+04   Precision: 30.447%   Recall: 33.333%
Train   Epoch: 575 / 600   Loss:    2897   Precision: 86.391%   Recall: 71.794%
Valid                   Loss: 2.195e+04   Precision: 28.163%   Recall: 40.489%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[38.12430573 42.58333333]
	 [16.09932518 32.4       ]
	 [64.03116608 56.45      ]
	 [66.61441803 15.3       ]
	 [45.45719147 40.        ]]
Train   Epoch: 576 / 600   Loss:    2782   Precision: 87.282%   Recall: 72.732%
Valid                   Loss: 2.197e+04   Precision: 28.100%   Recall: 38.104%
Train   Epoch: 577 / 600   Loss:    2770   Precision: 87.045%   Recall: 71.691%
Valid                   Loss: 2.189e+04   Precision: 28.081%   Recall: 32.200%
Train   Epoch: 578 / 600   Loss:    2945   Precision: 85.190%   Recall: 71.753%
Valid                   Loss: 2.216e+04   Precision: 26.843%   Recall: 41.264%
Train   Epoch: 579 / 600   Loss:    3156   Precision: 85.297%   Recall: 71.704%
Valid                   Loss: 2.224e+04   Precision: 27.969%   Recall: 36.792%
Train   Epoch: 580 / 600   Loss:    3115   Precision: 86.298%   Recall: 71.016%
Valid                   Loss: 2.214e+04   Precision: 28.716%   Recall: 37.329%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[44.01784515 23.55      ]
	 [-1.74564123 20.41666667]
	 [55.43265152 61.21666667]
	 [60.73353195 64.88333333]
	 [57.58052063 92.        ]]
Train   Epoch: 581 / 600   Loss:    3195   Precision: 85.822%   Recall: 71.501%
Valid                   Loss: 2.203e+04   Precision: 30.702%   Recall: 31.544%
Train   Epoch: 582 / 600   Loss:    3049   Precision: 85.292%   Recall: 71.429%
Valid                   Loss: 2.194e+04   Precision: 28.913%   Recall: 37.448%
Train   Epoch: 583 / 600   Loss:    3245   Precision: 85.056%   Recall: 70.062%
Valid                   Loss: 2.209e+04   Precision: 25.994%   Recall: 35.480%
Train   Epoch: 584 / 600   Loss:    2927   Precision: 85.733%   Recall: 72.670%
Valid                   Loss: 2.194e+04   Precision: 28.677%   Recall: 34.764%
Train   Epoch: 585 / 600   Loss:    2716   Precision: 86.667%   Recall: 72.443%
Valid                   Loss: 2.191e+04   Precision: 27.264%   Recall: 35.361%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[ 42.62715149  38.05      ]
	 [ 33.21237946  36.36666667]
	 [ 69.25553894  90.38333333]
	 [ 75.39885712 155.71666667]
	 [ 50.59844971  32.5       ]]
Train   Epoch: 586 / 600   Loss:    2887   Precision: 86.402%   Recall: 73.096%
Valid                   Loss: 2.198e+04   Precision: 31.344%   Recall: 28.503%
Train   Epoch: 587 / 600   Loss:    3066   Precision: 85.191%   Recall: 70.691%
Valid                   Loss: 2.261e+04   Precision: 32.330%   Recall: 29.457%
Train   Epoch: 588 / 600   Loss:    2791   Precision: 85.884%   Recall: 72.381%
Valid                   Loss: 2.197e+04   Precision: 28.871%   Recall: 31.246%
Train   Epoch: 589 / 600   Loss:    2863   Precision: 87.065%   Recall: 73.057%
Valid                   Loss: 2.223e+04   Precision: 28.145%   Recall: 36.553%
Train   Epoch: 590 / 600   Loss:    2766   Precision: 86.844%   Recall: 71.446%
Valid                   Loss: 2.215e+04   Precision: 27.773%   Recall: 42.397%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[48.81770325 14.11666667]
	 [55.60434723 55.88333333]
	 [65.8097229  26.53333333]
	 [56.30841064 34.81666667]
	 [42.21567535 38.45      ]]
Train   Epoch: 591 / 600   Loss:    6524   Precision: 77.335%   Recall: 67.594%
Valid                   Loss: 2.231e+04   Precision: 26.324%   Recall: 50.984%
Train   Epoch: 592 / 600   Loss:    3823   Precision: 83.423%   Recall: 70.233%
Valid                   Loss: 2.229e+04   Precision: 28.837%   Recall: 34.168%
Train   Epoch: 593 / 600   Loss:    3001   Precision: 87.105%   Recall: 72.773%
Valid                   Loss: 2.191e+04   Precision: 29.429%   Recall: 30.411%
Train   Epoch: 594 / 600   Loss:    2532   Precision: 87.671%   Recall: 72.722%
Valid                   Loss: 2.196e+04   Precision: 28.897%   Recall: 32.499%
Train   Epoch: 595 / 600   Loss:    2643   Precision: 87.012%   Recall: 73.469%
Valid                   Loss: 2.212e+04   Precision: 30.187%   Recall: 33.751%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[45.55125046 41.55      ]
	 [10.62887859 16.75      ]
	 [68.5670929  47.51666667]
	 [30.37323761  7.16666667]
	 [40.88928986 29.5       ]]
Train   Epoch: 596 / 600   Loss:    2493   Precision: 87.580%   Recall: 72.902%
Valid                   Loss: 2.251e+04   Precision: 29.549%   Recall: 31.664%
Train   Epoch: 597 / 600   Loss:    2560   Precision: 88.221%   Recall: 72.026%
Valid                   Loss: 2.21e+04   Precision: 28.952%   Recall: 31.127%
Train   Epoch: 598 / 600   Loss:    2475   Precision: 87.898%   Recall: 74.335%
Valid                   Loss: 2.208e+04   Precision: 27.439%   Recall: 37.567%
Train   Epoch: 599 / 600   Loss:    2494   Precision: 88.090%   Recall: 72.670%
Valid                   Loss: 2.231e+04   Precision: 29.466%   Recall: 30.590%
Train   Epoch: 600 / 600   Loss:    2502   Precision: 85.595%   Recall: 73.676%
Valid                   Loss: 2.176e+04   Precision: 29.662%   Recall: 30.352%
        Sample predictions | targets (cols [:1] |  cols [1:])
	[[35.67411423 51.63333333]
	 [40.0861702  39.78333333]
	 [48.26780319 49.9       ]
	 [52.97663879 54.68333333]
	 [37.8310318  30.78333333]]
Wrong lengths - could not plot
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="ow">not</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_losses_split5</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_losses_split5</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[25]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>False</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">utils.training_loop_plotting</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plot_train_valid</span><span class="p">(</span><span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Wrong lengths - could not plot
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_losses_split5</span><span class="p">))]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_losses_split5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">valid_losses_split5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;validation loss&#39;</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split5 - different lr, more epchs, larger batch, dropout, loss with decaying_weights</span>

<span class="kn">from</span> <span class="nn">training.dust_loss</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split5</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model_split5</span> <span class="o">=</span> <span class="n">model_split5</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split5</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>

<span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 600   Loss: 1.487e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.536e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 600   Loss: 1.499e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.518e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 600   Loss: 1.43e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.499e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 600   Loss: 1.455e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.481e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 005 / 600   Loss: 1.39e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.464e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[19.16350365 24.7       ]
	 [19.16327286 29.78333333]
	 [19.16338539 54.03333333]
	 [19.16372108 89.63333333]
	 [19.16326523 32.68333333]]
Train   Epoch: 006 / 600   Loss: 1.429e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.448e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 007 / 600   Loss: 1.416e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.434e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 600   Loss: 1.443e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.42e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 009 / 600   Loss: 1.358e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.408e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 600   Loss: 1.381e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.398e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.05245972 29.08333333]
	 [34.05276489 28.21666667]
	 [34.05460358 16.76666667]
	 [34.05268478 34.8       ]
	 [34.05330276 37.75      ]]
Train   Epoch: 011 / 600   Loss: 1.339e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.388e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 600   Loss: 1.313e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.38e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 013 / 600   Loss: 1.273e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.373e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 600   Loss: 1.279e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.367e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 015 / 600   Loss: 1.217e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.363e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.83968353 40.5       ]
	 [47.83961487 31.95      ]
	 [47.83937073 10.75      ]
	 [47.8388176  23.5       ]
	 [47.83859634 35.43333333]]
Train   Epoch: 016 / 600   Loss: 1.281e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.359e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 017 / 600   Loss: 1.272e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.356e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 600   Loss: 1.175e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.354e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 019 / 600   Loss: 1.233e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 020 / 600   Loss: 1.229e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.15956116 37.43333333]
	 [60.159935   79.2       ]
	 [60.15939331 63.08333333]
	 [60.15880966 20.7       ]
	 [60.1601944  34.73333333]]
Train   Epoch: 021 / 600   Loss: 1.174e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 022 / 600   Loss: 1.329e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 023 / 600   Loss: 1.256e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 024 / 600   Loss: 1.267e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.354e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 025 / 600   Loss: 1.112e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.355e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 70.51241302  21.9       ]
	 [ 70.51306915  17.46666667]
	 [ 70.51333618  22.41666667]
	 [ 70.5144043   46.41666667]
	 [ 70.51422119 121.66666667]]
Train   Epoch: 026 / 600   Loss: 1.202e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.357e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 027 / 600   Loss: 1.183e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.358e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 028 / 600   Loss: 1.181e+04   Precision: 39.793%   Recall: 88.622%
Valid                   Loss: 1.359e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 029 / 600   Loss: 1.164e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.361e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 030 / 600   Loss: 1.226e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.362e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 77.56285095 114.38333333]
	 [ 77.56388855  40.06666667]
	 [ 77.56356049  13.2       ]
	 [ 77.56423187   6.86666667]
	 [ 77.56439972  28.78333333]]
Train   Epoch: 031 / 600   Loss: 1.27e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.364e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 032 / 600   Loss: 1.196e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.365e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 033 / 600   Loss: 1.247e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.367e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 034 / 600   Loss: 1.249e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.368e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 035 / 600   Loss: 1.227e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.369e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 83.25240326  23.8       ]
	 [ 83.25241852  39.68333333]
	 [ 83.25203705 156.75      ]
	 [ 83.25226593  50.66666667]
	 [ 83.25240326  27.3       ]]
Train   Epoch: 036 / 600   Loss: 1.167e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.37e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 037 / 600   Loss: 1.219e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.371e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 038 / 600   Loss: 1.185e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.372e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 039 / 600   Loss: 1.236e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.373e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 040 / 600   Loss: 1.155e+04   Precision: 39.953%   Recall: 99.151%
Valid                   Loss: 1.372e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.26994324 26.28333333]
	 [86.22133636 28.45      ]
	 [85.91506195 23.45      ]
	 [86.13866425  6.9       ]
	 [86.26947021 55.46666667]]
Train   Epoch: 041 / 600   Loss: 1.199e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 042 / 600   Loss: 1.179e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 043 / 600   Loss: 1.195e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 044 / 600   Loss: 1.177e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 045 / 600   Loss: 1.2e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 87.6231842   19.1       ]
	 [ 87.61081696  65.5       ]
	 [ 87.63365936 130.55      ]
	 [ 87.60635376  54.53333333]
	 [ 87.64764404  23.11666667]]
Train   Epoch: 046 / 600   Loss: 1.195e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 047 / 600   Loss: 1.225e+04   Precision: 39.837%   Recall: 99.970%
Valid                   Loss: 1.43e+04   Precision: 13.120%   Recall: 86.643%
Train   Epoch: 048 / 600   Loss: 1.214e+04   Precision: 39.789%   Recall: 99.616%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 049 / 600   Loss: 1.16e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 050 / 600   Loss: 1.073e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.39040375 17.81666667]
	 [87.44205475 21.9       ]
	 [87.44426727 69.13333333]
	 [87.43054199 24.25      ]
	 [87.57720184 75.16666667]]
Train   Epoch: 051 / 600   Loss: 1.21e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 052 / 600   Loss: 1.154e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 053 / 600   Loss: 1.236e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 054 / 600   Loss: 1.107e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 055 / 600   Loss: 1.228e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.21188354 48.78333333]
	 [89.15525818 26.86666667]
	 [89.1288147  17.        ]
	 [89.11595917 44.18333333]
	 [89.14286804 21.25      ]]
Train   Epoch: 056 / 600   Loss: 1.236e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 057 / 600   Loss: 1.192e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 058 / 600   Loss: 1.2e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 059 / 600   Loss: 1.224e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 060 / 600   Loss: 1.261e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.4438324  70.45      ]
	 [89.15007782 31.3       ]
	 [89.29544067  9.        ]
	 [89.39291382 66.96666667]
	 [89.19181061 51.16666667]]
Train   Epoch: 061 / 600   Loss: 1.219e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 062 / 600   Loss: 1.161e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 063 / 600   Loss: 1.22e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 064 / 600   Loss: 1.28e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 065 / 600   Loss: 1.246e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.87520599  40.05      ]
	 [ 89.15060425  64.5       ]
	 [ 88.47106171 103.58333333]
	 [ 89.46446228  13.        ]
	 [ 89.27042389  49.95      ]]
Train   Epoch: 066 / 600   Loss: 1.152e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.366e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 067 / 600   Loss: 1.189e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.384e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 068 / 600   Loss: 1.226e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.368e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 069 / 600   Loss: 1.126e+04   Precision: 39.847%   Recall: 99.980%
Valid                   Loss: 1.364e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 070 / 600   Loss: 1.295e+04   Precision: 39.992%   Recall: 99.798%
Valid                   Loss: 1.368e+04   Precision: 14.087%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[79.40671539 27.06666667]
	 [92.71000671 29.08333333]
	 [90.02231598 50.8       ]
	 [82.60205078 47.78333333]
	 [91.35298157 63.91666667]]
Train   Epoch: 071 / 600   Loss: 1.262e+04   Precision: 40.410%   Recall: 97.494%
Valid                   Loss: 1.356e+04   Precision: 16.643%   Recall: 91.234%
Train   Epoch: 072 / 600   Loss: 1.22e+04   Precision: 41.846%   Recall: 91.532%
Valid                   Loss: 1.364e+04   Precision: 14.185%   Recall: 100.000%
Train   Epoch: 073 / 600   Loss: 1.227e+04   Precision: 43.142%   Recall: 88.612%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 074 / 600   Loss: 1.215e+04   Precision: 39.939%   Recall: 99.960%
Valid                   Loss: 1.366e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 075 / 600   Loss: 1.155e+04   Precision: 41.786%   Recall: 94.068%
Valid                   Loss: 1.372e+04   Precision: 14.982%   Recall: 99.344%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.43948364 63.1       ]
	 [96.40574646 41.1       ]
	 [88.16701508 63.33333333]
	 [96.72052765 61.48333333]
	 [97.15067291 37.01666667]]
Train   Epoch: 076 / 600   Loss: 1.178e+04   Precision: 45.505%   Recall: 83.842%
Valid                   Loss: 1.386e+04   Precision: 14.091%   Recall: 100.000%
Train   Epoch: 077 / 600   Loss: 1.201e+04   Precision: 42.984%   Recall: 89.430%
Valid                   Loss: 1.367e+04   Precision: 16.252%   Recall: 96.303%
Train   Epoch: 078 / 600   Loss: 1.215e+04   Precision: 46.141%   Recall: 81.194%
Valid                   Loss: 1.375e+04   Precision: 15.582%   Recall: 98.211%
Train   Epoch: 079 / 600   Loss: 1.23e+04   Precision: 45.356%   Recall: 81.821%
Valid                   Loss: 1.359e+04   Precision: 16.818%   Recall: 95.886%
Train   Epoch: 080 / 600   Loss: 1.201e+04   Precision: 43.842%   Recall: 85.257%
Valid                   Loss: 1.361e+04   Precision: 16.832%   Recall: 95.945%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.31345367  25.38333333]
	 [ 87.93941498  19.46666667]
	 [ 73.57010651 186.33333333]
	 [104.55934906  44.55      ]
	 [ 99.27498627 145.45      ]]
Train   Epoch: 081 / 600   Loss: 1.204e+04   Precision: 46.398%   Recall: 81.225%
Valid                   Loss: 1.354e+04   Precision: 19.584%   Recall: 83.005%
Train   Epoch: 082 / 600   Loss: 1.206e+04   Precision: 46.730%   Recall: 78.628%
Valid                   Loss: 1.383e+04   Precision: 14.187%   Recall: 99.881%
Train   Epoch: 083 / 600   Loss: 1.097e+04   Precision: 47.190%   Recall: 81.114%
Valid                   Loss: 1.368e+04   Precision: 16.912%   Recall: 95.110%
Train   Epoch: 084 / 600   Loss: 1.214e+04   Precision: 48.740%   Recall: 76.021%
Valid                   Loss: 1.353e+04   Precision: 19.602%   Recall: 86.822%
Train   Epoch: 085 / 600   Loss: 1.151e+04   Precision: 50.179%   Recall: 75.091%
Valid                   Loss: 1.368e+04   Precision: 17.309%   Recall: 94.275%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 68.59745789  26.01666667]
	 [107.43405914  18.43333333]
	 [ 71.20518494  49.96666667]
	 [ 86.83722687  22.21666667]
	 [ 76.41842651  17.5       ]]
Train   Epoch: 086 / 600   Loss: 1.156e+04   Precision: 50.137%   Recall: 73.767%
Valid                   Loss: 1.372e+04   Precision: 17.282%   Recall: 93.858%
Train   Epoch: 087 / 600   Loss: 1.219e+04   Precision: 49.246%   Recall: 72.939%
Valid                   Loss: 1.349e+04   Precision: 21.448%   Recall: 77.042%
Train   Epoch: 088 / 600   Loss: 1.187e+04   Precision: 49.876%   Recall: 72.928%
Valid                   Loss: 1.348e+04   Precision: 21.193%   Recall: 78.593%
Train   Epoch: 089 / 600   Loss: 1.087e+04   Precision: 52.123%   Recall: 70.210%
Valid                   Loss: 1.355e+04   Precision: 19.277%   Recall: 87.179%
Train   Epoch: 090 / 600   Loss: 1.166e+04   Precision: 50.213%   Recall: 71.423%
Valid                   Loss: 1.358e+04   Precision: 18.504%   Recall: 89.267%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 70.65203857  36.45      ]
	 [108.93182373  64.25      ]
	 [ 89.90344238  11.58333333]
	 [ 74.92798615  35.33333333]
	 [112.93241119  21.21666667]]
Train   Epoch: 091 / 600   Loss: 1.15e+04   Precision: 51.406%   Recall: 70.584%
Valid                   Loss: 1.37e+04   Precision: 16.656%   Recall: 94.991%
Train   Epoch: 092 / 600   Loss: 1.129e+04   Precision: 53.540%   Recall: 67.553%
Valid                   Loss: 1.363e+04   Precision: 18.532%   Recall: 92.010%
Train   Epoch: 093 / 600   Loss: 1.092e+04   Precision: 51.230%   Recall: 69.452%
Valid                   Loss: 1.353e+04   Precision: 21.742%   Recall: 75.909%
Train   Epoch: 094 / 600   Loss: 1.223e+04   Precision: 52.741%   Recall: 68.139%
Valid                   Loss: 1.348e+04   Precision: 20.928%   Recall: 81.515%
Train   Epoch: 095 / 600   Loss: 1.217e+04   Precision: 51.721%   Recall: 65.734%
Valid                   Loss: 1.447e+04   Precision: 14.144%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[117.03555298  75.58333333]
	 [137.27059937  32.56666667]
	 [ 90.78487396  52.11666667]
	 [ 85.04785156  39.36666667]
	 [ 96.4733963   23.83333333]]
Train   Epoch: 096 / 600   Loss: 1.203e+04   Precision: 50.699%   Recall: 69.291%
Valid                   Loss: 1.349e+04   Precision: 22.234%   Recall: 73.942%
Train   Epoch: 097 / 600   Loss: 1.12e+04   Precision: 54.706%   Recall: 64.137%
Valid                   Loss: 1.336e+04   Precision: 24.976%   Recall: 61.717%
Train   Epoch: 098 / 600   Loss: 1.122e+04   Precision: 52.823%   Recall: 66.269%
Valid                   Loss: 1.348e+04   Precision: 24.443%   Recall: 65.414%
Train   Epoch: 099 / 600   Loss: 1.113e+04   Precision: 52.962%   Recall: 63.955%
Valid                   Loss: 1.327e+04   Precision: 32.598%   Recall: 30.829%
Train   Epoch: 100 / 600   Loss: 1.142e+04   Precision: 52.368%   Recall: 64.804%
Valid                   Loss: 1.352e+04   Precision: 22.672%   Recall: 72.451%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 65.53492737  55.9       ]
	 [136.41673279  27.01666667]
	 [ 53.44921494  23.85      ]
	 [ 69.68549347  25.63333333]
	 [ 65.92074585  42.        ]]
Train   Epoch: 101 / 600   Loss: 1.215e+04   Precision: 53.112%   Recall: 66.320%
Valid                   Loss: 1.35e+04   Precision: 21.498%   Recall: 77.519%
Train   Epoch: 102 / 600   Loss: 1.183e+04   Precision: 51.466%   Recall: 65.451%
Valid                   Loss: 1.34e+04   Precision: 24.562%   Recall: 63.566%
Train   Epoch: 103 / 600   Loss: 1.176e+04   Precision: 54.125%   Recall: 64.764%
Valid                   Loss: 1.369e+04   Precision: 20.103%   Recall: 83.721%
Train   Epoch: 104 / 600   Loss: 1.093e+04   Precision: 54.218%   Recall: 63.844%
Valid                   Loss: 1.349e+04   Precision: 22.218%   Recall: 73.703%
Train   Epoch: 105 / 600   Loss: 1.123e+04   Precision: 56.014%   Recall: 63.197%
Valid                   Loss: 1.342e+04   Precision: 24.094%   Recall: 63.864%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.83781815 42.75      ]
	 [80.61937714 14.66666667]
	 [56.13688278 29.66666667]
	 [58.59179688 39.75      ]
	 [59.53536606 29.11666667]]
Train   Epoch: 106 / 600   Loss: 1.127e+04   Precision: 53.363%   Recall: 61.015%
Valid                   Loss: 1.354e+04   Precision: 21.128%   Recall: 77.042%
Train   Epoch: 107 / 600   Loss: 1.158e+04   Precision: 53.059%   Recall: 67.300%
Valid                   Loss: 1.395e+04   Precision: 18.282%   Recall: 84.138%
Train   Epoch: 108 / 600   Loss: 1.209e+04   Precision: 55.197%   Recall: 63.157%
Valid                   Loss: 1.39e+04   Precision: 16.474%   Recall: 96.005%
Train   Epoch: 109 / 600   Loss: 1.161e+04   Precision: 54.667%   Recall: 62.086%
Valid                   Loss: 1.377e+04   Precision: 20.006%   Recall: 80.382%
Train   Epoch: 110 / 600   Loss: 1.128e+04   Precision: 55.062%   Recall: 63.692%
Valid                   Loss: 1.327e+04   Precision: 29.747%   Recall: 42.695%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.35224533 53.65      ]
	 [60.38248444 24.26666667]
	 [55.47354889 41.08333333]
	 [66.8270874  93.16666667]
	 [49.90842819 30.78333333]]
Train   Epoch: 111 / 600   Loss: 1.134e+04   Precision: 52.719%   Recall: 61.914%
Valid                   Loss: 1.327e+04   Precision: 27.281%   Recall: 52.952%
Train   Epoch: 112 / 600   Loss: 1.092e+04   Precision: 55.991%   Recall: 61.146%
Valid                   Loss: 1.352e+04   Precision: 22.764%   Recall: 68.754%
Train   Epoch: 113 / 600   Loss: 1.195e+04   Precision: 56.385%   Recall: 59.923%
Valid                   Loss: 1.385e+04   Precision: 19.263%   Recall: 86.941%
Train   Epoch: 114 / 600   Loss: 1.199e+04   Precision: 55.068%   Recall: 61.378%
Valid                   Loss: 1.34e+04   Precision: 23.938%   Recall: 64.878%
Train   Epoch: 115 / 600   Loss: 1.135e+04   Precision: 55.709%   Recall: 61.924%
Valid                   Loss: 1.325e+04   Precision: 28.339%   Recall: 46.691%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 71.65172577  53.56666667]
	 [ 84.26290131  25.66666667]
	 [ 55.95683289  32.23333333]
	 [ 97.03739166 106.61666667]
	 [ 72.24403381 108.31666667]]
Train   Epoch: 116 / 600   Loss: 1.04e+04   Precision: 56.749%   Recall: 58.286%
Valid                   Loss: 1.362e+04   Precision: 22.858%   Recall: 68.873%
Train   Epoch: 117 / 600   Loss: 1.151e+04   Precision: 56.455%   Recall: 59.479%
Valid                   Loss: 1.359e+04   Precision: 21.250%   Recall: 74.180%
Train   Epoch: 118 / 600   Loss: 1.069e+04   Precision: 52.366%   Recall: 59.034%
Valid                   Loss: 1.362e+04   Precision: 20.154%   Recall: 79.487%
Train   Epoch: 119 / 600   Loss: 1.179e+04   Precision: 55.708%   Recall: 58.680%
Valid                   Loss: 1.353e+04   Precision: 21.198%   Recall: 77.221%
Train   Epoch: 120 / 600   Loss: 1.179e+04   Precision: 54.229%   Recall: 63.106%
Valid                   Loss: 1.326e+04   Precision: 27.097%   Recall: 50.089%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[58.13712692 48.46666667]
	 [61.7436409  43.5       ]
	 [59.88844299 25.48333333]
	 [58.97635269 25.91666667]
	 [58.64593506 12.63333333]]
Train   Epoch: 121 / 600   Loss: 1.127e+04   Precision: 55.720%   Recall: 61.520%
Valid                   Loss: 1.34e+04   Precision: 25.692%   Recall: 55.337%
Train   Epoch: 122 / 600   Loss: 1.147e+04   Precision: 56.835%   Recall: 58.438%
Valid                   Loss: 1.366e+04   Precision: 22.443%   Recall: 67.382%
Train   Epoch: 123 / 600   Loss: 1.038e+04   Precision: 57.252%   Recall: 59.115%
Valid                   Loss: 1.339e+04   Precision: 23.715%   Recall: 64.639%
Train   Epoch: 124 / 600   Loss: 1.074e+04   Precision: 56.934%   Recall: 58.498%
Valid                   Loss: 1.319e+04   Precision: 31.760%   Recall: 35.301%
Train   Epoch: 125 / 600   Loss: 1.151e+04   Precision: 56.644%   Recall: 58.973%
Valid                   Loss: 1.381e+04   Precision: 18.430%   Recall: 90.459%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 80.26506805  26.4       ]
	 [ 73.18143463  38.55      ]
	 [123.46315002  62.71666667]
	 [157.35354614  38.7       ]
	 [196.83015442 117.95      ]]
Train   Epoch: 126 / 600   Loss: 1.087e+04   Precision: 56.603%   Recall: 57.993%
Valid                   Loss: 1.319e+04   Precision: 27.746%   Recall: 43.828%
Train   Epoch: 127 / 600   Loss: 1.129e+04   Precision: 56.430%   Recall: 56.710%
Valid                   Loss: 1.364e+04   Precision: 21.295%   Recall: 72.153%
Train   Epoch: 128 / 600   Loss: 1.062e+04   Precision: 57.319%   Recall: 56.467%
Valid                   Loss: 1.36e+04   Precision: 19.418%   Recall: 82.826%
Train   Epoch: 129 / 600   Loss: 1.091e+04   Precision: 55.205%   Recall: 58.781%
Valid                   Loss: 1.344e+04   Precision: 23.050%   Recall: 63.804%
Train   Epoch: 130 / 600   Loss: 1.115e+04   Precision: 58.032%   Recall: 57.165%
Valid                   Loss: 1.328e+04   Precision: 25.382%   Recall: 55.456%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.80448151 23.53333333]
	 [50.2888298  16.25      ]
	 [53.74247742 81.33333333]
	 [48.87397385 40.6       ]
	 [65.72062683 50.91666667]]
Train   Epoch: 131 / 600   Loss: 1.146e+04   Precision: 56.286%   Recall: 58.316%
Valid                   Loss: 1.331e+04   Precision: 22.291%   Recall: 68.933%
Train   Epoch: 132 / 600   Loss: 1.065e+04   Precision: 53.605%   Recall: 59.873%
Valid                   Loss: 1.318e+04   Precision: 35.461%   Recall: 30.471%
Train   Epoch: 133 / 600   Loss: 1.185e+04   Precision: 51.060%   Recall: 61.116%
Valid                   Loss: 1.378e+04   Precision: 16.033%   Recall: 94.753%
Train   Epoch: 134 / 600   Loss: 1.088e+04   Precision: 56.883%   Recall: 57.417%
Valid                   Loss: 1.362e+04   Precision: 20.440%   Recall: 83.065%
Train   Epoch: 135 / 600   Loss: 1.139e+04   Precision: 57.093%   Recall: 60.924%
Valid                   Loss: 1.351e+04   Precision: 20.085%   Recall: 78.473%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[67.72715759 25.16666667]
	 [70.13235474 51.3       ]
	 [73.27074432 24.66666667]
	 [69.74288177 24.        ]
	 [87.46251678 43.85      ]]
Train   Epoch: 136 / 600   Loss: 1.167e+04   Precision: 56.419%   Recall: 61.015%
Valid                   Loss: 1.357e+04   Precision: 20.210%   Recall: 73.584%
Train   Epoch: 137 / 600   Loss: 1.152e+04   Precision: 56.659%   Recall: 58.508%
Valid                   Loss: 1.324e+04   Precision: 30.865%   Recall: 37.030%
Train   Epoch: 138 / 600   Loss: 1.127e+04   Precision: 55.703%   Recall: 57.741%
Valid                   Loss: 1.329e+04   Precision: 22.516%   Recall: 69.589%
Train   Epoch: 139 / 600   Loss: 1.084e+04   Precision: 56.397%   Recall: 60.176%
Valid                   Loss: 1.342e+04   Precision: 24.551%   Recall: 61.896%
Train   Epoch: 140 / 600   Loss: 1.144e+04   Precision: 58.189%   Recall: 59.923%
Valid                   Loss: 1.33e+04   Precision: 24.316%   Recall: 63.029%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.74943542 53.08333333]
	 [57.59941864 27.88333333]
	 [56.30864334 42.78333333]
	 [62.68606186 36.96666667]
	 [68.99975586 48.43333333]]
Train   Epoch: 141 / 600   Loss: 1.099e+04   Precision: 56.906%   Recall: 59.954%
Valid                   Loss: 1.317e+04   Precision: 34.346%   Recall: 31.008%
Train   Epoch: 142 / 600   Loss: 1.11e+04   Precision: 55.980%   Recall: 61.722%
Valid                   Loss: 1.346e+04   Precision: 27.006%   Recall: 57.603%
Train   Epoch: 143 / 600   Loss: 1.165e+04   Precision: 55.690%   Recall: 61.217%
Valid                   Loss: 1.33e+04   Precision: 29.745%   Recall: 49.434%
Train   Epoch: 144 / 600   Loss: 1.031e+04   Precision: 56.773%   Recall: 59.671%
Valid                   Loss: 1.318e+04   Precision: 28.737%   Recall: 51.699%
Train   Epoch: 145 / 600   Loss: 1.12e+04   Precision: 56.321%   Recall: 59.468%
Valid                   Loss: 1.359e+04   Precision: 23.255%   Recall: 66.369%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 47.21730042  44.        ]
	 [ 49.68501282  24.75      ]
	 [128.40827942  36.51666667]
	 [ 65.46537781  14.85      ]
	 [ 74.89265442  18.96666667]]
Train   Epoch: 146 / 600   Loss: 1.092e+04   Precision: 56.874%   Recall: 60.489%
Valid                   Loss: 1.322e+04   Precision: 24.485%   Recall: 61.658%
Train   Epoch: 147 / 600   Loss: 1.107e+04   Precision: 57.779%   Recall: 64.026%
Valid                   Loss: 1.321e+04   Precision: 29.835%   Recall: 50.686%
Train   Epoch: 148 / 600   Loss: 1.043e+04   Precision: 57.621%   Recall: 60.509%
Valid                   Loss: 1.315e+04   Precision: 28.001%   Recall: 51.878%
Train   Epoch: 149 / 600   Loss: 1.055e+04   Precision: 57.778%   Recall: 60.014%
Valid                   Loss: 1.318e+04   Precision: 26.449%   Recall: 59.034%
Train   Epoch: 150 / 600   Loss: 1.123e+04   Precision: 57.711%   Recall: 62.773%
Valid                   Loss: 1.326e+04   Precision: 25.241%   Recall: 60.823%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 74.20008087 111.91666667]
	 [ 51.88977814  46.71666667]
	 [ 60.21299744  50.46666667]
	 [115.52149963  19.46666667]
	 [ 88.51557159 226.13333333]]
Train   Epoch: 151 / 600   Loss: 1.08e+04   Precision: 57.553%   Recall: 61.560%
Valid                   Loss: 1.338e+04   Precision: 20.468%   Recall: 77.162%
Train   Epoch: 152 / 600   Loss: 1.072e+04   Precision: 55.524%   Recall: 61.247%
Valid                   Loss: 1.326e+04   Precision: 26.981%   Recall: 54.204%
Train   Epoch: 153 / 600   Loss: 1.043e+04   Precision: 58.780%   Recall: 59.772%
Valid                   Loss: 1.342e+04   Precision: 23.477%   Recall: 64.103%
Train   Epoch: 154 / 600   Loss: 1.056e+04   Precision: 57.205%   Recall: 60.974%
Valid                   Loss: 1.333e+04   Precision: 22.513%   Recall: 68.813%
Train   Epoch: 155 / 600   Loss: 1.082e+04   Precision: 57.331%   Recall: 63.935%
Valid                   Loss: 1.316e+04   Precision: 23.605%   Recall: 61.061%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 79.5895462   56.95      ]
	 [ 49.60102081  31.71666667]
	 [ 71.16835785  32.75      ]
	 [272.09649658 469.41666667]
	 [ 45.68044662  41.3       ]]
Train   Epoch: 156 / 600   Loss: 1.069e+04   Precision: 57.664%   Recall: 60.560%
Valid                   Loss: 1.325e+04   Precision: 22.505%   Recall: 68.456%
Train   Epoch: 157 / 600   Loss: 1.047e+04   Precision: 57.033%   Recall: 62.197%
Valid                   Loss: 1.314e+04   Precision: 25.873%   Recall: 57.007%
Train   Epoch: 158 / 600   Loss: 1.041e+04   Precision: 57.164%   Recall: 61.722%
Valid                   Loss: 1.308e+04   Precision: 31.259%   Recall: 38.640%
Train   Epoch: 159 / 600   Loss: 1.096e+04   Precision: 58.253%   Recall: 61.267%
Valid                   Loss: 1.359e+04   Precision: 20.486%   Recall: 76.983%
Train   Epoch: 160 / 600   Loss: 1.105e+04   Precision: 57.147%   Recall: 60.398%
Valid                   Loss: 1.327e+04   Precision: 22.296%   Recall: 67.859%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.16756439  14.4       ]
	 [ 73.36560822  32.38333333]
	 [ 50.36896133  37.33333333]
	 [298.08938599  40.9       ]
	 [ 72.18383789  60.16666667]]
Train   Epoch: 161 / 600   Loss: 1.055e+04   Precision: 57.329%   Recall: 63.551%
Valid                   Loss: 1.32e+04   Precision: 26.023%   Recall: 50.447%
Train   Epoch: 162 / 600   Loss: 1.12e+04   Precision: 57.372%   Recall: 62.914%
Valid                   Loss: 1.316e+04   Precision: 25.478%   Recall: 56.470%
Train   Epoch: 163 / 600   Loss: 1.092e+04   Precision: 58.811%   Recall: 61.479%
Valid                   Loss: 1.342e+04   Precision: 23.286%   Recall: 62.791%
Train   Epoch: 164 / 600   Loss: 1.102e+04   Precision: 57.949%   Recall: 59.044%
Valid                   Loss: 1.321e+04   Precision: 23.717%   Recall: 60.346%
Train   Epoch: 165 / 600   Loss: 1.06e+04   Precision: 58.706%   Recall: 60.065%
Valid                   Loss: 1.394e+04   Precision: 17.240%   Recall: 88.790%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[80.83850861 36.23333333]
	 [56.29362869 36.        ]
	 [84.63293457 40.5       ]
	 [81.13173676 45.91666667]
	 [56.3010788  51.11666667]]
Train   Epoch: 166 / 600   Loss: 1.02e+04   Precision: 56.576%   Recall: 63.288%
Valid                   Loss: 1.321e+04   Precision: 24.852%   Recall: 59.928%
Train   Epoch: 167 / 600   Loss: 1.061e+04   Precision: 58.519%   Recall: 61.469%
Valid                   Loss: 1.328e+04   Precision: 22.732%   Recall: 66.786%
Train   Epoch: 168 / 600   Loss: 1.103e+04   Precision: 57.694%   Recall: 62.965%
Valid                   Loss: 1.316e+04   Precision: 24.326%   Recall: 60.823%
Train   Epoch: 169 / 600   Loss: 1.107e+04   Precision: 58.758%   Recall: 61.358%
Valid                   Loss: 1.322e+04   Precision: 26.337%   Recall: 55.516%
Train   Epoch: 170 / 600   Loss: 1.106e+04   Precision: 57.518%   Recall: 61.075%
Valid                   Loss: 1.345e+04   Precision: 20.477%   Recall: 78.772%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[129.47033691  28.2       ]
	 [ 66.93951416  31.51666667]
	 [ 53.096035    27.75      ]
	 [134.70053101  57.95      ]
	 [130.4692688   60.95      ]]
Train   Epoch: 171 / 600   Loss: 1.035e+04   Precision: 57.021%   Recall: 60.853%
Valid                   Loss: 1.331e+04   Precision: 23.329%   Recall: 63.268%
Train   Epoch: 172 / 600   Loss: 1.066e+04   Precision: 57.805%   Recall: 59.458%
Valid                   Loss: 1.341e+04   Precision: 24.507%   Recall: 62.970%
Train   Epoch: 173 / 600   Loss: 1.087e+04   Precision: 58.956%   Recall: 61.762%
Valid                   Loss: 1.318e+04   Precision: 24.927%   Recall: 61.240%
Train   Epoch: 174 / 600   Loss: 1.052e+04   Precision: 58.265%   Recall: 62.298%
Valid                   Loss: 1.33e+04   Precision: 23.516%   Recall: 66.369%
Train   Epoch: 175 / 600   Loss: 1.085e+04   Precision: 58.267%   Recall: 60.681%
Valid                   Loss: 1.339e+04   Precision: 22.008%   Recall: 61.837%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 85.54782867  14.23333333]
	 [ 68.70075989  45.88333333]
	 [ 82.78275299  39.08333333]
	 [264.8508606    3.7       ]
	 [ 84.28040314  65.25      ]]
Train   Epoch: 176 / 600   Loss: 1.064e+04   Precision: 57.323%   Recall: 59.559%
Valid                   Loss: 1.34e+04   Precision: 23.505%   Recall: 63.983%
Train   Epoch: 177 / 600   Loss: 1.084e+04   Precision: 58.955%   Recall: 58.973%
Valid                   Loss: 1.326e+04   Precision: 24.012%   Recall: 59.809%
Train   Epoch: 178 / 600   Loss: 1.059e+04   Precision: 58.814%   Recall: 60.044%
Valid                   Loss: 1.323e+04   Precision: 26.205%   Recall: 51.878%
Train   Epoch: 179 / 600   Loss: 1.079e+04   Precision: 59.360%   Recall: 59.984%
Valid                   Loss: 1.325e+04   Precision: 26.646%   Recall: 46.333%
Train   Epoch: 180 / 600   Loss: 1.047e+04   Precision: 58.986%   Recall: 60.792%
Valid                   Loss: 1.316e+04   Precision: 26.564%   Recall: 55.695%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 67.48985291  23.08333333]
	 [109.31542969  28.01666667]
	 [ 64.49425507  56.11666667]
	 [161.78643799  87.45      ]
	 [ 60.97436905  53.53333333]]
Train   Epoch: 181 / 600   Loss: 1.063e+04   Precision: 58.657%   Recall: 59.772%
Valid                   Loss: 1.306e+04   Precision: 26.116%   Recall: 53.011%
Train   Epoch: 182 / 600   Loss: 1.063e+04   Precision: 57.660%   Recall: 63.207%
Valid                   Loss: 1.346e+04   Precision: 24.874%   Recall: 50.089%
Train   Epoch: 183 / 600   Loss: 1.053e+04   Precision: 59.374%   Recall: 58.084%
Valid                   Loss: 1.312e+04   Precision: 25.717%   Recall: 56.172%
Train   Epoch: 184 / 600   Loss: 1.049e+04   Precision: 58.506%   Recall: 61.166%
Valid                   Loss: 1.332e+04   Precision: 21.231%   Recall: 73.047%
Train   Epoch: 185 / 600   Loss: 1.058e+04   Precision: 59.446%   Recall: 59.873%
Valid                   Loss: 1.298e+04   Precision: 28.658%   Recall: 45.081%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[186.77093506  62.        ]
	 [ 73.96388245 157.7       ]
	 [ 49.03510666  24.8       ]
	 [ 73.59236908  65.11666667]
	 [ 56.47042465  39.41666667]]
Train   Epoch: 186 / 600   Loss: 1.03e+04   Precision: 59.796%   Recall: 62.298%
Valid                   Loss: 1.303e+04   Precision: 29.657%   Recall: 43.769%
Train   Epoch: 187 / 600   Loss: 1.046e+04   Precision: 59.962%   Recall: 57.660%
Valid                   Loss: 1.333e+04   Precision: 21.488%   Recall: 72.153%
Train   Epoch: 188 / 600   Loss: 1.046e+04   Precision: 58.460%   Recall: 57.538%
Valid                   Loss: 1.3e+04   Precision: 26.479%   Recall: 47.227%
Train   Epoch: 189 / 600   Loss: 1.014e+04   Precision: 59.311%   Recall: 59.287%
Valid                   Loss: 1.351e+04   Precision: 23.160%   Recall: 62.671%
Train   Epoch: 190 / 600   Loss: 1.046e+04   Precision: 59.099%   Recall: 60.156%
Valid                   Loss: 1.307e+04   Precision: 26.282%   Recall: 49.195%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.36278534  27.88333333]
	 [ 53.60285568  40.6       ]
	 [ 93.05274963  49.5       ]
	 [107.10801697  17.85      ]
	 [ 64.47864532  28.33333333]]
Train   Epoch: 191 / 600   Loss:    9896   Precision: 60.521%   Recall: 59.378%
Valid                   Loss: 1.324e+04   Precision: 26.762%   Recall: 48.897%
Train   Epoch: 192 / 600   Loss: 1.086e+04   Precision: 59.438%   Recall: 61.126%
Valid                   Loss: 1.308e+04   Precision: 25.118%   Recall: 54.025%
Train   Epoch: 193 / 600   Loss:    9610   Precision: 60.297%   Recall: 59.084%
Valid                   Loss: 1.337e+04   Precision: 23.184%   Recall: 64.520%
Train   Epoch: 194 / 600   Loss: 1.027e+04   Precision: 57.253%   Recall: 59.145%
Valid                   Loss: 1.317e+04   Precision: 26.799%   Recall: 55.754%
Train   Epoch: 195 / 600   Loss: 1.073e+04   Precision: 58.923%   Recall: 56.154%
Valid                   Loss: 1.303e+04   Precision: 25.670%   Recall: 54.860%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 63.36682892  33.05      ]
	 [ 58.40380478  62.        ]
	 [155.99998474  66.45      ]
	 [ 74.70394897  22.38333333]
	 [ 45.60937119  38.55      ]]
Train   Epoch: 196 / 600   Loss: 1.015e+04   Precision: 58.247%   Recall: 61.308%
Valid                   Loss: 1.312e+04   Precision: 30.642%   Recall: 46.392%
Train   Epoch: 197 / 600   Loss: 1.031e+04   Precision: 61.830%   Recall: 55.851%
Valid                   Loss: 1.335e+04   Precision: 22.236%   Recall: 66.070%
Train   Epoch: 198 / 600   Loss: 1.02e+04   Precision: 57.062%   Recall: 60.176%
Valid                   Loss: 1.321e+04   Precision: 26.439%   Recall: 55.337%
Train   Epoch: 199 / 600   Loss:    9886   Precision: 60.783%   Recall: 57.387%
Valid                   Loss: 1.31e+04   Precision: 24.940%   Recall: 55.874%
Train   Epoch: 200 / 600   Loss: 1.015e+04   Precision: 60.513%   Recall: 58.599%
Valid                   Loss: 1.307e+04   Precision: 26.245%   Recall: 57.185%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[51.49321365 23.33333333]
	 [75.17082214 70.55      ]
	 [57.14034271 17.16666667]
	 [45.51813889 26.53333333]
	 [66.98896027 86.16666667]]
Train   Epoch: 201 / 600   Loss: 1.043e+04   Precision: 60.507%   Recall: 59.388%
Valid                   Loss: 1.309e+04   Precision: 24.609%   Recall: 57.185%
Train   Epoch: 202 / 600   Loss: 1.071e+04   Precision: 61.088%   Recall: 58.458%
Valid                   Loss: 1.319e+04   Precision: 24.739%   Recall: 60.644%
Train   Epoch: 203 / 600   Loss:    9601   Precision: 61.476%   Recall: 58.407%
Valid                   Loss: 1.306e+04   Precision: 25.173%   Recall: 58.617%
Train   Epoch: 204 / 600   Loss: 1.06e+04   Precision: 61.130%   Recall: 58.579%
Valid                   Loss: 1.317e+04   Precision: 23.983%   Recall: 64.699%
Train   Epoch: 205 / 600   Loss: 1.026e+04   Precision: 58.879%   Recall: 58.832%
Valid                   Loss: 1.345e+04   Precision: 21.384%   Recall: 69.648%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.13156891  22.91666667]
	 [ 57.64624786  26.85      ]
	 [ 94.78791809  42.05      ]
	 [ 76.30436707  27.8       ]
	 [258.50286865 105.16666667]]
Train   Epoch: 206 / 600   Loss:    9460   Precision: 60.000%   Recall: 56.902%
Valid                   Loss: 1.317e+04   Precision: 28.390%   Recall: 52.057%
Train   Epoch: 207 / 600   Loss: 1.075e+04   Precision: 56.307%   Recall: 61.570%
Valid                   Loss: 1.324e+04   Precision: 22.356%   Recall: 66.547%
Train   Epoch: 208 / 600   Loss: 1.053e+04   Precision: 53.631%   Recall: 58.357%
Valid                   Loss: 1.336e+04   Precision: 24.492%   Recall: 63.924%
Train   Epoch: 209 / 600   Loss: 1.031e+04   Precision: 54.930%   Recall: 57.700%
Valid                   Loss: 1.3e+04   Precision: 26.353%   Recall: 57.782%
Train   Epoch: 210 / 600   Loss: 1.065e+04   Precision: 57.192%   Recall: 64.046%
Valid                   Loss: 1.337e+04   Precision: 20.981%   Recall: 72.928%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.26688766 26.66666667]
	 [61.04497528 32.8       ]
	 [63.12939072 24.26666667]
	 [63.34565353 34.11666667]
	 [60.51707077 93.5       ]]
Train   Epoch: 211 / 600   Loss:    9612   Precision: 60.132%   Recall: 57.094%
Valid                   Loss: 1.291e+04   Precision: 29.400%   Recall: 44.425%
Train   Epoch: 212 / 600   Loss: 1.045e+04   Precision: 59.247%   Recall: 59.600%
Valid                   Loss: 1.3e+04   Precision: 31.250%   Recall: 37.567%
Train   Epoch: 213 / 600   Loss: 1.049e+04   Precision: 61.030%   Recall: 58.205%
Valid                   Loss: 1.333e+04   Precision: 23.975%   Recall: 63.089%
Train   Epoch: 214 / 600   Loss:    9803   Precision: 59.488%   Recall: 58.256%
Valid                   Loss: 1.336e+04   Precision: 22.827%   Recall: 67.323%
Train   Epoch: 215 / 600   Loss: 1.029e+04   Precision: 61.216%   Recall: 55.952%
Valid                   Loss: 1.318e+04   Precision: 28.818%   Recall: 48.718%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[417.20742798 252.16666667]
	 [ 57.42028809  16.78333333]
	 [ 53.62365341  26.78333333]
	 [ 42.5021553   10.76666667]
	 [ 56.7817421   25.36666667]]
Train   Epoch: 216 / 600   Loss: 1.015e+04   Precision: 61.036%   Recall: 60.469%
Valid                   Loss: 1.301e+04   Precision: 24.727%   Recall: 58.080%
Train   Epoch: 217 / 600   Loss:    9886   Precision: 61.927%   Recall: 57.296%
Valid                   Loss: 1.3e+04   Precision: 28.884%   Recall: 43.351%
Train   Epoch: 218 / 600   Loss:    9499   Precision: 60.230%   Recall: 58.124%
Valid                   Loss: 1.336e+04   Precision: 28.024%   Recall: 54.025%
Train   Epoch: 219 / 600   Loss:    9737   Precision: 60.014%   Recall: 58.892%
Valid                   Loss: 1.309e+04   Precision: 29.870%   Recall: 35.659%
Train   Epoch: 220 / 600   Loss:    9588   Precision: 61.910%   Recall: 57.569%
Valid                   Loss: 1.3e+04   Precision: 30.017%   Recall: 42.457%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 32.27339172  27.46666667]
	 [ 86.73157501  74.43333333]
	 [113.09662628  68.4       ]
	 [ 45.31908417  34.81666667]
	 [ 40.7490387   26.2       ]]
Train   Epoch: 221 / 600   Loss:    9837   Precision: 62.660%   Recall: 56.518%
Valid                   Loss: 1.323e+04   Precision: 22.720%   Recall: 70.125%
Train   Epoch: 222 / 600   Loss:    9777   Precision: 61.455%   Recall: 57.357%
Valid                   Loss: 1.302e+04   Precision: 26.221%   Recall: 51.222%
Train   Epoch: 223 / 600   Loss:    9576   Precision: 61.234%   Recall: 58.246%
Valid                   Loss: 1.302e+04   Precision: 27.081%   Recall: 51.401%
Train   Epoch: 224 / 600   Loss:    9506   Precision: 62.174%   Recall: 55.841%
Valid                   Loss: 1.304e+04   Precision: 26.020%   Recall: 57.424%
Train   Epoch: 225 / 600   Loss: 1.007e+04   Precision: 62.336%   Recall: 59.105%
Valid                   Loss: 1.318e+04   Precision: 23.574%   Recall: 59.630%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.11030197 30.61666667]
	 [68.92993927 40.15      ]
	 [40.70427322 31.13333333]
	 [66.70565033 24.        ]
	 [47.77812195 37.55      ]]
Train   Epoch: 226 / 600   Loss:    9606   Precision: 62.618%   Recall: 58.448%
Valid                   Loss: 1.288e+04   Precision: 33.886%   Recall: 36.553%
Train   Epoch: 227 / 600   Loss:    9920   Precision: 61.040%   Recall: 58.246%
Valid                   Loss: 1.323e+04   Precision: 25.006%   Recall: 64.520%
Train   Epoch: 228 / 600   Loss:    9806   Precision: 63.063%   Recall: 57.003%
Valid                   Loss: 1.318e+04   Precision: 25.417%   Recall: 63.626%
Train   Epoch: 229 / 600   Loss: 1.003e+04   Precision: 62.445%   Recall: 58.690%
Valid                   Loss: 1.314e+04   Precision: 28.687%   Recall: 52.892%
Train   Epoch: 230 / 600   Loss: 1.038e+04   Precision: 62.131%   Recall: 59.570%
Valid                   Loss: 1.339e+04   Precision: 27.600%   Recall: 47.943%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 87.32057953  48.75      ]
	 [ 51.79997253  75.66666667]
	 [103.97325897 179.03333333]
	 [ 42.78456116  39.7       ]
	 [112.78964233  35.11666667]]
Train   Epoch: 231 / 600   Loss:    9758   Precision: 63.049%   Recall: 57.468%
Valid                   Loss: 1.317e+04   Precision: 23.402%   Recall: 65.951%
Train   Epoch: 232 / 600   Loss:    9411   Precision: 62.311%   Recall: 61.095%
Valid                   Loss: 1.306e+04   Precision: 28.976%   Recall: 49.434%
Train   Epoch: 233 / 600   Loss:    9458   Precision: 62.673%   Recall: 57.195%
Valid                   Loss: 1.316e+04   Precision: 25.631%   Recall: 63.626%
Train   Epoch: 234 / 600   Loss:    9413   Precision: 64.139%   Recall: 57.781%
Valid                   Loss: 1.326e+04   Precision: 26.893%   Recall: 56.768%
Train   Epoch: 235 / 600   Loss:    9799   Precision: 61.675%   Recall: 56.690%
Valid                   Loss: 1.307e+04   Precision: 24.982%   Recall: 60.883%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[45.91024399 23.11666667]
	 [56.46409607 69.        ]
	 [89.84540558 70.        ]
	 [58.81371689 21.2       ]
	 [59.88710403 25.        ]]
Train   Epoch: 236 / 600   Loss:    9549   Precision: 60.728%   Recall: 58.347%
Valid                   Loss: 1.313e+04   Precision: 29.244%   Recall: 50.745%
Train   Epoch: 237 / 600   Loss:    9681   Precision: 62.901%   Recall: 57.104%
Valid                   Loss: 1.308e+04   Precision: 26.648%   Recall: 55.695%
Train   Epoch: 238 / 600   Loss:    9909   Precision: 63.232%   Recall: 57.852%
Valid                   Loss: 1.292e+04   Precision: 33.057%   Recall: 40.370%
Train   Epoch: 239 / 600   Loss:    9477   Precision: 63.729%   Recall: 56.477%
Valid                   Loss: 1.313e+04   Precision: 29.396%   Recall: 45.558%
Train   Epoch: 240 / 600   Loss:    9174   Precision: 62.490%   Recall: 57.811%
Valid                   Loss: 1.296e+04   Precision: 30.255%   Recall: 43.172%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[77.19258118 42.28333333]
	 [92.60358429 42.71666667]
	 [59.95795822 51.2       ]
	 [55.91254044 92.36666667]
	 [52.45422745 58.45      ]]
Train   Epoch: 241 / 600   Loss:    9557   Precision: 61.642%   Recall: 56.609%
Valid                   Loss: 1.334e+04   Precision: 24.141%   Recall: 62.850%
Train   Epoch: 242 / 600   Loss:    9697   Precision: 59.380%   Recall: 57.094%
Valid                   Loss: 1.311e+04   Precision: 25.654%   Recall: 64.341%
Train   Epoch: 243 / 600   Loss: 1.017e+04   Precision: 61.255%   Recall: 56.922%
Valid                   Loss: 1.306e+04   Precision: 23.411%   Recall: 65.236%
Train   Epoch: 244 / 600   Loss:    9649   Precision: 61.633%   Recall: 58.812%
Valid                   Loss: 1.329e+04   Precision: 24.679%   Recall: 63.029%
Train   Epoch: 245 / 600   Loss:    9110   Precision: 62.967%   Recall: 58.023%
Valid                   Loss: 1.303e+04   Precision: 29.735%   Recall: 44.186%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[138.68399048 113.25      ]
	 [ 74.39031982  26.4       ]
	 [ 74.94272614  93.66666667]
	 [119.91348267  44.2       ]
	 [ 47.9067955   33.65      ]]
Train   Epoch: 246 / 600   Loss:    9027   Precision: 63.008%   Recall: 56.437%
Valid                   Loss: 1.32e+04   Precision: 26.587%   Recall: 54.442%
Train   Epoch: 247 / 600   Loss:    9259   Precision: 64.257%   Recall: 57.407%
Valid                   Loss: 1.306e+04   Precision: 26.698%   Recall: 53.906%
Train   Epoch: 248 / 600   Loss:    9506   Precision: 63.324%   Recall: 57.751%
Valid                   Loss: 1.337e+04   Precision: 23.528%   Recall: 65.057%
Train   Epoch: 249 / 600   Loss:    9230   Precision: 64.282%   Recall: 58.104%
Valid                   Loss: 1.31e+04   Precision: 28.531%   Recall: 48.181%
Train   Epoch: 250 / 600   Loss:    9552   Precision: 63.342%   Recall: 57.427%
Valid                   Loss: 1.306e+04   Precision: 29.086%   Recall: 48.599%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[101.45057678  50.28333333]
	 [ 63.90164948   9.86666667]
	 [299.14004517  61.53333333]
	 [ 58.32354736  24.        ]
	 [ 45.11153412  27.5       ]]
Train   Epoch: 251 / 600   Loss:    9528   Precision: 63.356%   Recall: 57.043%
Valid                   Loss: 1.305e+04   Precision: 27.716%   Recall: 53.548%
Train   Epoch: 252 / 600   Loss:    9833   Precision: 63.840%   Recall: 58.498%
Valid                   Loss: 1.294e+04   Precision: 31.330%   Recall: 39.475%
Train   Epoch: 253 / 600   Loss:    9180   Precision: 62.489%   Recall: 56.882%
Valid                   Loss: 1.313e+04   Precision: 25.006%   Recall: 59.869%
Train   Epoch: 254 / 600   Loss:    9641   Precision: 64.437%   Recall: 58.296%
Valid                   Loss: 1.303e+04   Precision: 26.252%   Recall: 59.392%
Train   Epoch: 255 / 600   Loss:    8900   Precision: 65.217%   Recall: 57.296%
Valid                   Loss: 1.299e+04   Precision: 30.944%   Recall: 43.769%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.88411713 48.91666667]
	 [57.73669434 12.3       ]
	 [34.69868088 11.66666667]
	 [39.88103104 14.08333333]
	 [64.69075012 61.21666667]]
Train   Epoch: 256 / 600   Loss:    9304   Precision: 65.143%   Recall: 56.578%
Valid                   Loss: 1.3e+04   Precision: 28.379%   Recall: 45.200%
Train   Epoch: 257 / 600   Loss:    9523   Precision: 63.738%   Recall: 56.659%
Valid                   Loss: 1.3e+04   Precision: 24.889%   Recall: 60.107%
Train   Epoch: 258 / 600   Loss:    9626   Precision: 64.470%   Recall: 57.336%
Valid                   Loss: 1.303e+04   Precision: 27.043%   Recall: 47.943%
Train   Epoch: 259 / 600   Loss:    8891   Precision: 63.830%   Recall: 58.064%
Valid                   Loss: 1.287e+04   Precision: 33.977%   Recall: 33.572%
Train   Epoch: 260 / 600   Loss:    9168   Precision: 63.370%   Recall: 56.397%
Valid                   Loss: 1.307e+04   Precision: 30.082%   Recall: 48.002%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 50.87892532  31.55      ]
	 [ 45.96125031  32.45      ]
	 [ 72.52658081 128.5       ]
	 [ 38.53321838  24.48333333]
	 [ 43.89410782  61.08333333]]
Train   Epoch: 261 / 600   Loss:    9032   Precision: 64.308%   Recall: 57.225%
Valid                   Loss: 1.311e+04   Precision: 28.331%   Recall: 45.259%
Train   Epoch: 262 / 600   Loss:    9428   Precision: 64.266%   Recall: 57.720%
Valid                   Loss: 1.32e+04   Precision: 26.648%   Recall: 50.626%
Train   Epoch: 263 / 600   Loss:    9403   Precision: 62.316%   Recall: 57.751%
Valid                   Loss: 1.325e+04   Precision: 27.170%   Recall: 48.539%
Train   Epoch: 264 / 600   Loss:    8929   Precision: 63.296%   Recall: 57.821%
Valid                   Loss: 1.306e+04   Precision: 23.993%   Recall: 64.281%
Train   Epoch: 265 / 600   Loss:    8633   Precision: 65.361%   Recall: 55.810%
Valid                   Loss: 1.323e+04   Precision: 27.554%   Recall: 55.814%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.28805923 24.75      ]
	 [55.7056427  20.66666667]
	 [81.89896393 94.03333333]
	 [47.16293716 17.33333333]
	 [93.05885315 62.33333333]]
Train   Epoch: 266 / 600   Loss:    9788   Precision: 62.171%   Recall: 56.730%
Valid                   Loss: 1.323e+04   Precision: 26.403%   Recall: 53.309%
Train   Epoch: 267 / 600   Loss:    8837   Precision: 64.151%   Recall: 58.226%
Valid                   Loss: 1.29e+04   Precision: 28.448%   Recall: 41.324%
Train   Epoch: 268 / 600   Loss:    9118   Precision: 65.651%   Recall: 56.164%
Valid                   Loss: 1.294e+04   Precision: 27.627%   Recall: 46.094%
Train   Epoch: 269 / 600   Loss:    9287   Precision: 65.188%   Recall: 58.488%
Valid                   Loss: 1.304e+04   Precision: 29.315%   Recall: 42.338%
Train   Epoch: 270 / 600   Loss:    8722   Precision: 64.837%   Recall: 56.831%
Valid                   Loss: 1.305e+04   Precision: 28.355%   Recall: 47.883%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.44390869 95.95      ]
	 [38.65460587 36.9       ]
	 [74.44150543 43.5       ]
	 [56.45950699 39.4       ]
	 [67.72429657 43.2       ]]
Train   Epoch: 271 / 600   Loss:    8812   Precision: 64.727%   Recall: 58.448%
Valid                   Loss: 1.278e+04   Precision: 29.679%   Recall: 41.324%
Train   Epoch: 272 / 600   Loss:    9840   Precision: 64.909%   Recall: 56.599%
Valid                   Loss: 1.3e+04   Precision: 27.792%   Recall: 53.131%
Train   Epoch: 273 / 600   Loss:    8569   Precision: 65.057%   Recall: 56.912%
Valid                   Loss: 1.289e+04   Precision: 28.607%   Recall: 40.549%
Train   Epoch: 274 / 600   Loss:    8827   Precision: 66.458%   Recall: 58.003%
Valid                   Loss: 1.306e+04   Precision: 28.556%   Recall: 47.764%
Train   Epoch: 275 / 600   Loss:    8694   Precision: 65.803%   Recall: 57.518%
Valid                   Loss: 1.3e+04   Precision: 27.611%   Recall: 48.718%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 51.58639526  37.58333333]
	 [ 49.99656296  33.83333333]
	 [122.53064728  73.48333333]
	 [ 74.79455566  61.45      ]
	 [ 44.17112732  49.03333333]]
Train   Epoch: 276 / 600   Loss:    8607   Precision: 66.235%   Recall: 55.881%
Valid                   Loss: 1.325e+04   Precision: 22.324%   Recall: 73.763%
Train   Epoch: 277 / 600   Loss:    9074   Precision: 62.798%   Recall: 58.286%
Valid                   Loss: 1.284e+04   Precision: 27.792%   Recall: 44.067%
Train   Epoch: 278 / 600   Loss:    8553   Precision: 67.223%   Recall: 56.952%
Valid                   Loss: 1.303e+04   Precision: 26.616%   Recall: 54.502%
Train   Epoch: 279 / 600   Loss:    8960   Precision: 64.319%   Recall: 58.872%
Valid                   Loss: 1.306e+04   Precision: 31.050%   Recall: 37.567%
Train   Epoch: 280 / 600   Loss:    9148   Precision: 64.442%   Recall: 58.842%
Valid                   Loss: 1.297e+04   Precision: 28.007%   Recall: 45.677%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[61.63062286 41.13333333]
	 [43.23999405 24.11666667]
	 [75.39752197 61.21666667]
	 [41.27864838 56.63333333]
	 [50.46695328 16.91666667]]
Train   Epoch: 281 / 600   Loss:    9423   Precision: 66.011%   Recall: 57.013%
Valid                   Loss: 1.29e+04   Precision: 30.057%   Recall: 40.668%
Train   Epoch: 282 / 600   Loss:    8719   Precision: 64.110%   Recall: 57.276%
Valid                   Loss: 1.291e+04   Precision: 31.329%   Recall: 34.168%
Train   Epoch: 283 / 600   Loss:    9188   Precision: 64.996%   Recall: 56.103%
Valid                   Loss: 1.283e+04   Precision: 33.891%   Recall: 30.173%
Train   Epoch: 284 / 600   Loss:    8990   Precision: 66.404%   Recall: 56.225%
Valid                   Loss: 1.393e+04   Precision: 26.524%   Recall: 57.841%
Train   Epoch: 285 / 600   Loss:    9037   Precision: 66.607%   Recall: 56.882%
Valid                   Loss: 1.29e+04   Precision: 28.524%   Recall: 40.906%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[79.39962006 27.95      ]
	 [33.15042114 31.93333333]
	 [62.79908752 30.4       ]
	 [55.79094696 37.78333333]
	 [48.88573837 49.51666667]]
Train   Epoch: 286 / 600   Loss:    9019   Precision: 65.772%   Recall: 57.943%
Valid                   Loss: 1.292e+04   Precision: 30.252%   Recall: 42.934%
Train   Epoch: 287 / 600   Loss:    8638   Precision: 65.701%   Recall: 55.457%
Valid                   Loss: 1.293e+04   Precision: 28.479%   Recall: 42.099%
Train   Epoch: 288 / 600   Loss:    8479   Precision: 65.833%   Recall: 57.730%
Valid                   Loss: 1.29e+04   Precision: 26.701%   Recall: 46.810%
Train   Epoch: 289 / 600   Loss:    8134   Precision: 67.417%   Recall: 56.326%
Valid                   Loss: 1.291e+04   Precision: 28.024%   Recall: 46.691%
Train   Epoch: 290 / 600   Loss:    8086   Precision: 66.846%   Recall: 56.356%
Valid                   Loss: 1.314e+04   Precision: 24.315%   Recall: 56.052%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 55.2591362   34.28333333]
	 [199.6267395   32.88333333]
	 [ 55.27291107 113.13333333]
	 [ 55.47316742  39.4       ]
	 [ 47.76876068   8.91666667]]
Train   Epoch: 291 / 600   Loss:    8632   Precision: 66.867%   Recall: 57.205%
Valid                   Loss: 1.314e+04   Precision: 27.563%   Recall: 55.158%
Train   Epoch: 292 / 600   Loss:    8602   Precision: 67.629%   Recall: 56.073%
Valid                   Loss: 1.291e+04   Precision: 29.691%   Recall: 42.934%
Train   Epoch: 293 / 600   Loss:    9159   Precision: 67.204%   Recall: 57.336%
Valid                   Loss: 1.297e+04   Precision: 26.773%   Recall: 52.892%
Train   Epoch: 294 / 600   Loss:    8867   Precision: 67.352%   Recall: 57.579%
Valid                   Loss: 1.28e+04   Precision: 34.910%   Recall: 29.040%
Train   Epoch: 295 / 600   Loss:    8612   Precision: 67.479%   Recall: 57.094%
Valid                   Loss: 1.309e+04   Precision: 32.006%   Recall: 38.819%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.25532532 22.        ]
	 [47.7026329  77.41666667]
	 [42.52846146 48.        ]
	 [54.70687103 37.48333333]
	 [51.20683289 55.76666667]]
Train   Epoch: 296 / 600   Loss:    8516   Precision: 67.468%   Recall: 55.891%
Valid                   Loss: 1.306e+04   Precision: 25.969%   Recall: 51.938%
Train   Epoch: 297 / 600   Loss:    8174   Precision: 68.447%   Recall: 58.003%
Valid                   Loss: 1.309e+04   Precision: 26.986%   Recall: 50.447%
Train   Epoch: 298 / 600   Loss:    8548   Precision: 67.613%   Recall: 56.811%
Valid                   Loss: 1.299e+04   Precision: 24.728%   Recall: 58.259%
Train   Epoch: 299 / 600   Loss:    8211   Precision: 67.065%   Recall: 58.994%
Valid                   Loss: 1.296e+04   Precision: 33.876%   Recall: 31.008%
Train   Epoch: 300 / 600   Loss:    9174   Precision: 66.550%   Recall: 55.770%
Valid                   Loss: 1.323e+04   Precision: 27.483%   Recall: 49.493%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 17.88384056  33.48333333]
	 [ 94.19387817  26.        ]
	 [138.27999878  68.91666667]
	 [ 50.60902023  18.85      ]
	 [ 61.59598541  33.45      ]]
Train   Epoch: 301 / 600   Loss:    8722   Precision: 66.062%   Recall: 57.063%
Valid                   Loss: 1.318e+04   Precision: 27.035%   Recall: 51.878%
Train   Epoch: 302 / 600   Loss:    8469   Precision: 68.292%   Recall: 57.872%
Valid                   Loss: 1.308e+04   Precision: 30.257%   Recall: 43.590%
Train   Epoch: 303 / 600   Loss:    8404   Precision: 67.884%   Recall: 58.034%
Valid                   Loss: 1.288e+04   Precision: 30.247%   Recall: 37.984%
Train   Epoch: 304 / 600   Loss:    7932   Precision: 67.153%   Recall: 58.195%
Valid                   Loss: 1.307e+04   Precision: 28.741%   Recall: 47.645%
Train   Epoch: 305 / 600   Loss:    7820   Precision: 68.766%   Recall: 58.155%
Valid                   Loss: 1.293e+04   Precision: 31.592%   Recall: 38.223%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.85716248 41.25      ]
	 [34.48649216 33.21666667]
	 [36.18204117 50.28333333]
	 [47.20297623 72.03333333]
	 [33.84492493 36.33333333]]
Train   Epoch: 306 / 600   Loss:    8441   Precision: 68.761%   Recall: 57.720%
Valid                   Loss: 1.316e+04   Precision: 30.508%   Recall: 40.787%
Train   Epoch: 307 / 600   Loss:    8275   Precision: 68.299%   Recall: 59.064%
Valid                   Loss: 1.287e+04   Precision: 31.369%   Recall: 39.356%
Train   Epoch: 308 / 600   Loss:    8200   Precision: 68.721%   Recall: 58.943%
Valid                   Loss: 1.312e+04   Precision: 27.812%   Recall: 53.071%
Train   Epoch: 309 / 600   Loss:    8616   Precision: 66.772%   Recall: 57.508%
Valid                   Loss: 1.298e+04   Precision: 28.694%   Recall: 43.769%
Train   Epoch: 310 / 600   Loss:    8403   Precision: 68.635%   Recall: 56.498%
Valid                   Loss: 1.306e+04   Precision: 29.231%   Recall: 49.434%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.29935455 179.03333333]
	 [ 71.01174164  25.81666667]
	 [ 53.67776489  31.96666667]
	 [ 47.54614258  55.98333333]
	 [ 53.35219955  38.95      ]]
Train   Epoch: 311 / 600   Loss:    7809   Precision: 67.873%   Recall: 60.247%
Valid                   Loss: 1.292e+04   Precision: 32.725%   Recall: 35.301%
Train   Epoch: 312 / 600   Loss:    8318   Precision: 69.268%   Recall: 56.599%
Valid                   Loss: 1.315e+04   Precision: 27.429%   Recall: 53.190%
Train   Epoch: 313 / 600   Loss:    8086   Precision: 65.127%   Recall: 58.428%
Valid                   Loss: 1.292e+04   Precision: 30.506%   Recall: 37.746%
Train   Epoch: 314 / 600   Loss:    8294   Precision: 66.978%   Recall: 57.205%
Valid                   Loss: 1.297e+04   Precision: 26.169%   Recall: 52.713%
Train   Epoch: 315 / 600   Loss:    8050   Precision: 67.934%   Recall: 57.781%
Valid                   Loss: 1.296e+04   Precision: 29.985%   Recall: 47.346%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.80277634 38.1       ]
	 [42.07436371 13.55      ]
	 [63.55675507  8.91666667]
	 [56.0496788  61.93333333]
	 [48.85025406 34.03333333]]
Train   Epoch: 316 / 600   Loss:    8173   Precision: 68.779%   Recall: 58.236%
Valid                   Loss: 1.289e+04   Precision: 28.777%   Recall: 47.585%
Train   Epoch: 317 / 600   Loss:    8303   Precision: 68.342%   Recall: 57.852%
Valid                   Loss: 1.281e+04   Precision: 30.467%   Recall: 36.971%
Train   Epoch: 318 / 600   Loss:    7827   Precision: 68.469%   Recall: 58.084%
Valid                   Loss: 1.309e+04   Precision: 27.332%   Recall: 52.057%
Train   Epoch: 319 / 600   Loss:    8288   Precision: 68.792%   Recall: 58.226%
Valid                   Loss: 1.298e+04   Precision: 29.815%   Recall: 46.989%
Train   Epoch: 320 / 600   Loss:    7867   Precision: 69.259%   Recall: 57.781%
Valid                   Loss: 1.3e+04   Precision: 29.675%   Recall: 44.663%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 48.46849442  57.63333333]
	 [ 80.06586456 130.        ]
	 [ 75.55486298  81.11666667]
	 [ 64.72519684  50.13333333]
	 [ 89.99903107  58.55      ]]
Train   Epoch: 321 / 600   Loss:    8249   Precision: 68.855%   Recall: 58.084%
Valid                   Loss: 1.302e+04   Precision: 26.943%   Recall: 49.195%
Train   Epoch: 322 / 600   Loss:    8132   Precision: 67.849%   Recall: 57.235%
Valid                   Loss: 1.28e+04   Precision: 31.039%   Recall: 40.608%
Train   Epoch: 323 / 600   Loss:    7835   Precision: 67.728%   Recall: 58.680%
Valid                   Loss: 1.298e+04   Precision: 29.044%   Recall: 47.645%
Train   Epoch: 324 / 600   Loss:    8071   Precision: 67.348%   Recall: 55.942%
Valid                   Loss: 1.303e+04   Precision: 27.378%   Recall: 50.626%
Train   Epoch: 325 / 600   Loss:    8285   Precision: 68.173%   Recall: 57.771%
Valid                   Loss: 1.299e+04   Precision: 30.508%   Recall: 38.640%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[40.07059479 39.4       ]
	 [47.53955078 39.25      ]
	 [46.72156525 53.18333333]
	 [37.72483826 19.33333333]
	 [45.87974548 26.1       ]]
Train   Epoch: 326 / 600   Loss:    7506   Precision: 69.291%   Recall: 56.568%
Valid                   Loss: 1.318e+04   Precision: 26.602%   Recall: 55.695%
Train   Epoch: 327 / 600   Loss:    7783   Precision: 68.834%   Recall: 59.701%
Valid                   Loss: 1.286e+04   Precision: 31.881%   Recall: 40.131%
Train   Epoch: 328 / 600   Loss:    8256   Precision: 69.307%   Recall: 56.770%
Valid                   Loss: 1.304e+04   Precision: 29.697%   Recall: 42.695%
Train   Epoch: 329 / 600   Loss:    8074   Precision: 66.909%   Recall: 59.600%
Valid                   Loss: 1.287e+04   Precision: 35.527%   Recall: 26.714%
Train   Epoch: 330 / 600   Loss:    7933   Precision: 67.856%   Recall: 57.084%
Valid                   Loss: 1.306e+04   Precision: 28.915%   Recall: 45.140%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.67805481  68.4       ]
	 [ 81.10794067  17.58333333]
	 [ 65.41323853  48.13333333]
	 [ 36.57358551  24.73333333]
	 [ 44.18000793  58.15      ]]
Train   Epoch: 331 / 600   Loss:    7994   Precision: 68.794%   Recall: 57.609%
Valid                   Loss: 1.29e+04   Precision: 33.272%   Recall: 32.320%
Train   Epoch: 332 / 600   Loss:    7509   Precision: 68.011%   Recall: 58.458%
Valid                   Loss: 1.301e+04   Precision: 31.484%   Recall: 34.168%
Train   Epoch: 333 / 600   Loss:    7985   Precision: 65.693%   Recall: 55.012%
Valid                   Loss: 1.302e+04   Precision: 30.876%   Recall: 31.723%
Train   Epoch: 334 / 600   Loss:    9055   Precision: 60.929%   Recall: 59.883%
Valid                   Loss: 1.311e+04   Precision: 26.995%   Recall: 53.250%
Train   Epoch: 335 / 600   Loss:    7928   Precision: 67.596%   Recall: 56.578%
Valid                   Loss: 1.297e+04   Precision: 28.735%   Recall: 47.823%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[49.61804581 18.88333333]
	 [48.45002365 34.68333333]
	 [59.8380661  14.75      ]
	 [52.96192551 17.91666667]
	 [47.13459396 25.8       ]]
Train   Epoch: 336 / 600   Loss:    7377   Precision: 67.964%   Recall: 58.418%
Valid                   Loss: 1.299e+04   Precision: 29.124%   Recall: 50.745%
Train   Epoch: 337 / 600   Loss:    7878   Precision: 70.018%   Recall: 58.478%
Valid                   Loss: 1.314e+04   Precision: 26.880%   Recall: 58.199%
Train   Epoch: 338 / 600   Loss:    7249   Precision: 69.038%   Recall: 58.448%
Valid                   Loss: 1.299e+04   Precision: 31.793%   Recall: 45.140%
Train   Epoch: 339 / 600   Loss:    7198   Precision: 70.281%   Recall: 58.094%
Valid                   Loss: 1.295e+04   Precision: 30.645%   Recall: 45.081%
Train   Epoch: 340 / 600   Loss:    7478   Precision: 70.215%   Recall: 59.863%
Valid                   Loss: 1.294e+04   Precision: 33.368%   Recall: 38.581%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 66.72188568 110.86666667]
	 [ 46.97346497  43.16666667]
	 [ 31.36495209  34.5       ]
	 [ 31.14412498  27.5       ]
	 [ 73.54627991  32.71666667]]
Train   Epoch: 341 / 600   Loss:    7147   Precision: 70.618%   Recall: 58.630%
Valid                   Loss: 1.292e+04   Precision: 34.817%   Recall: 34.049%
Train   Epoch: 342 / 600   Loss:    7184   Precision: 70.726%   Recall: 59.398%
Valid                   Loss: 1.294e+04   Precision: 33.809%   Recall: 39.595%
Train   Epoch: 343 / 600   Loss:    7892   Precision: 70.473%   Recall: 59.064%
Valid                   Loss: 1.298e+04   Precision: 31.002%   Recall: 40.966%
Train   Epoch: 344 / 600   Loss:    7640   Precision: 69.761%   Recall: 56.952%
Valid                   Loss: 1.292e+04   Precision: 31.422%   Recall: 40.191%
Train   Epoch: 345 / 600   Loss:    7443   Precision: 69.151%   Recall: 58.781%
Valid                   Loss: 1.297e+04   Precision: 28.634%   Recall: 50.865%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.83028412 37.36666667]
	 [67.51831818 57.4       ]
	 [37.69444656 54.66666667]
	 [76.31023407 30.63333333]
	 [46.83466339 22.88333333]]
Train   Epoch: 346 / 600   Loss:    7594   Precision: 70.189%   Recall: 59.337%
Valid                   Loss: 1.309e+04   Precision: 28.718%   Recall: 53.548%
Train   Epoch: 347 / 600   Loss:    8052   Precision: 71.430%   Recall: 57.932%
Valid                   Loss: 1.296e+04   Precision: 30.425%   Recall: 41.801%
Train   Epoch: 348 / 600   Loss:    7703   Precision: 68.341%   Recall: 57.761%
Valid                   Loss: 1.303e+04   Precision: 28.052%   Recall: 46.035%
Train   Epoch: 349 / 600   Loss:    7473   Precision: 66.464%   Recall: 59.741%
Valid                   Loss: 1.304e+04   Precision: 28.934%   Recall: 49.672%
Train   Epoch: 350 / 600   Loss:    8006   Precision: 64.391%   Recall: 58.327%
Valid                   Loss: 1.331e+04   Precision: 28.882%   Recall: 44.365%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.56318665 89.33333333]
	 [43.70862198 39.9       ]
	 [41.99134064 64.66666667]
	 [48.63794327 20.78333333]
	 [54.48250961 31.05      ]]
Train   Epoch: 351 / 600   Loss:    7530   Precision: 68.151%   Recall: 60.914%
Valid                   Loss: 1.315e+04   Precision: 32.371%   Recall: 40.787%
Train   Epoch: 352 / 600   Loss:    7423   Precision: 70.598%   Recall: 58.306%
Valid                   Loss: 1.318e+04   Precision: 28.054%   Recall: 49.851%
Train   Epoch: 353 / 600   Loss:    7629   Precision: 71.134%   Recall: 58.246%
Valid                   Loss: 1.312e+04   Precision: 31.454%   Recall: 41.920%
Train   Epoch: 354 / 600   Loss:    7423   Precision: 70.172%   Recall: 59.479%
Valid                   Loss: 1.307e+04   Precision: 29.749%   Recall: 49.493%
Train   Epoch: 355 / 600   Loss:    7819   Precision: 71.592%   Recall: 58.852%
Valid                   Loss: 1.302e+04   Precision: 31.562%   Recall: 41.085%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[39.1649704  54.66666667]
	 [62.45090103 24.        ]
	 [44.91477203 47.03333333]
	 [36.58644104 38.61666667]
	 [60.69434738 18.5       ]]
Train   Epoch: 356 / 600   Loss:    7273   Precision: 71.627%   Recall: 58.468%
Valid                   Loss: 1.298e+04   Precision: 30.836%   Recall: 42.457%
Train   Epoch: 357 / 600   Loss:    7294   Precision: 70.527%   Recall: 58.519%
Valid                   Loss: 1.312e+04   Precision: 28.974%   Recall: 47.168%
Train   Epoch: 358 / 600   Loss:    7195   Precision: 70.905%   Recall: 60.065%
Valid                   Loss: 1.299e+04   Precision: 33.863%   Recall: 33.035%
Train   Epoch: 359 / 600   Loss:    7156   Precision: 71.368%   Recall: 58.286%
Valid                   Loss: 1.303e+04   Precision: 33.195%   Recall: 33.333%
Train   Epoch: 360 / 600   Loss:    7315   Precision: 71.772%   Recall: 58.246%
Valid                   Loss: 1.306e+04   Precision: 34.202%   Recall: 32.081%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.58715057  31.58333333]
	 [ 44.78553772  35.88333333]
	 [ 32.9923172   15.86666667]
	 [ 54.99398041 123.21666667]
	 [ 35.21154404  27.5       ]]
Train   Epoch: 361 / 600   Loss:    7480   Precision: 69.363%   Recall: 57.357%
Valid                   Loss: 1.289e+04   Precision: 32.638%   Recall: 32.677%
Train   Epoch: 362 / 600   Loss:    7274   Precision: 71.501%   Recall: 58.185%
Valid                   Loss: 1.332e+04   Precision: 30.204%   Recall: 42.397%
Train   Epoch: 363 / 600   Loss:    7046   Precision: 71.697%   Recall: 58.519%
Valid                   Loss: 1.32e+04   Precision: 27.936%   Recall: 48.658%
Train   Epoch: 364 / 600   Loss:    6762   Precision: 71.744%   Recall: 58.397%
Valid                   Loss: 1.319e+04   Precision: 30.164%   Recall: 40.668%
Train   Epoch: 365 / 600   Loss:    6757   Precision: 72.910%   Recall: 58.610%
Valid                   Loss: 1.321e+04   Precision: 30.114%   Recall: 44.246%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.96315765 37.33333333]
	 [32.10042572 26.7       ]
	 [77.47215271  9.95      ]
	 [61.16509628 37.95      ]
	 [53.97195816 23.        ]]
Train   Epoch: 366 / 600   Loss:    6957   Precision: 71.520%   Recall: 58.771%
Valid                   Loss: 1.297e+04   Precision: 36.104%   Recall: 26.416%
Train   Epoch: 367 / 600   Loss:    7179   Precision: 72.283%   Recall: 58.397%
Valid                   Loss: 1.316e+04   Precision: 31.724%   Recall: 39.177%
Train   Epoch: 368 / 600   Loss:    7469   Precision: 71.767%   Recall: 59.388%
Valid                   Loss: 1.32e+04   Precision: 29.016%   Recall: 47.823%
Train   Epoch: 369 / 600   Loss:    6972   Precision: 70.891%   Recall: 57.831%
Valid                   Loss: 1.312e+04   Precision: 31.910%   Recall: 31.187%
Train   Epoch: 370 / 600   Loss:    6857   Precision: 73.215%   Recall: 58.640%
Valid                   Loss: 1.321e+04   Precision: 34.557%   Recall: 29.756%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.8789978  87.11666667]
	 [57.63444138 27.2       ]
	 [87.25917053 72.51666667]
	 [38.12043762 62.95      ]
	 [56.31330109 55.5       ]]
Train   Epoch: 371 / 600   Loss:    6708   Precision: 71.241%   Recall: 58.023%
Valid                   Loss: 1.33e+04   Precision: 27.414%   Recall: 49.612%
Train   Epoch: 372 / 600   Loss:    6947   Precision: 72.094%   Recall: 58.660%
Valid                   Loss: 1.32e+04   Precision: 31.212%   Recall: 42.993%
Train   Epoch: 373 / 600   Loss:    6718   Precision: 73.418%   Recall: 58.973%
Valid                   Loss: 1.311e+04   Precision: 34.704%   Recall: 32.200%
Train   Epoch: 374 / 600   Loss:    6824   Precision: 71.880%   Recall: 58.660%
Valid                   Loss: 1.336e+04   Precision: 30.444%   Recall: 42.099%
Train   Epoch: 375 / 600   Loss:    6494   Precision: 70.588%   Recall: 58.812%
Valid                   Loss: 1.31e+04   Precision: 29.245%   Recall: 38.819%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.08531189 14.5       ]
	 [58.58893204 61.76666667]
	 [39.12958145 31.3       ]
	 [92.12158203 55.5       ]
	 [64.97428894 21.18333333]]
Train   Epoch: 376 / 600   Loss:    6839   Precision: 71.039%   Recall: 58.721%
Valid                   Loss: 1.313e+04   Precision: 32.766%   Recall: 36.732%
Train   Epoch: 377 / 600   Loss:    7075   Precision: 71.510%   Recall: 59.226%
Valid                   Loss: 1.32e+04   Precision: 32.482%   Recall: 32.618%
Train   Epoch: 378 / 600   Loss:    6589   Precision: 71.549%   Recall: 58.549%
Valid                   Loss: 1.312e+04   Precision: 34.030%   Recall: 32.021%
Train   Epoch: 379 / 600   Loss:    6809   Precision: 72.398%   Recall: 58.761%
Valid                   Loss: 1.306e+04   Precision: 35.439%   Recall: 30.113%
Train   Epoch: 380 / 600   Loss:    6684   Precision: 73.449%   Recall: 58.983%
Valid                   Loss: 1.321e+04   Precision: 28.806%   Recall: 44.902%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.0620575  38.33333333]
	 [45.48880768 36.95      ]
	 [55.60634232  9.21666667]
	 [48.35731125 22.7       ]
	 [75.81142426 39.16666667]]
Train   Epoch: 381 / 600   Loss:    6949   Precision: 73.929%   Recall: 59.489%
Valid                   Loss: 1.342e+04   Precision: 30.772%   Recall: 43.232%
Train   Epoch: 382 / 600   Loss:    6888   Precision: 71.828%   Recall: 57.559%
Valid                   Loss: 1.317e+04   Precision: 38.389%   Recall: 19.320%
Train   Epoch: 383 / 600   Loss:    6941   Precision: 72.696%   Recall: 57.872%
Valid                   Loss: 1.314e+04   Precision: 34.182%   Recall: 34.407%
Train   Epoch: 384 / 600   Loss: 1.005e+04   Precision: 52.936%   Recall: 59.751%
Valid                   Loss: 1.307e+04   Precision: 23.517%   Recall: 51.759%
Train   Epoch: 385 / 600   Loss:    8351   Precision: 61.971%   Recall: 59.297%
Valid                   Loss: 1.321e+04   Precision: 29.794%   Recall: 42.338%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[62.89741516 23.95      ]
	 [58.13913727 91.75      ]
	 [54.03411102 42.21666667]
	 [50.83433533 54.33333333]
	 [39.21249008 46.96666667]]
Train   Epoch: 386 / 600   Loss:    7193   Precision: 69.281%   Recall: 59.004%
Valid                   Loss: 1.322e+04   Precision: 26.640%   Recall: 51.342%
Train   Epoch: 387 / 600   Loss:    6490   Precision: 71.045%   Recall: 59.630%
Valid                   Loss: 1.318e+04   Precision: 30.690%   Recall: 42.457%
Train   Epoch: 388 / 600   Loss:    6733   Precision: 72.573%   Recall: 58.610%
Valid                   Loss: 1.309e+04   Precision: 31.368%   Recall: 37.746%
Train   Epoch: 389 / 600   Loss:    6829   Precision: 71.534%   Recall: 57.569%
Valid                   Loss: 1.314e+04   Precision: 34.326%   Recall: 31.604%
Train   Epoch: 390 / 600   Loss:    6360   Precision: 72.114%   Recall: 57.882%
Valid                   Loss: 1.326e+04   Precision: 28.441%   Recall: 51.998%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[46.63883972 44.5       ]
	 [61.20262146 47.73333333]
	 [61.36274338 97.13333333]
	 [44.92171097 30.        ]
	 [46.24947739 33.21666667]]
Train   Epoch: 391 / 600   Loss:    6735   Precision: 72.062%   Recall: 59.115%
Valid                   Loss: 1.332e+04   Precision: 29.595%   Recall: 41.860%
Train   Epoch: 392 / 600   Loss:    6818   Precision: 73.310%   Recall: 58.205%
Valid                   Loss: 1.329e+04   Precision: 31.844%   Recall: 36.136%
Train   Epoch: 393 / 600   Loss:    6630   Precision: 72.761%   Recall: 60.600%
Valid                   Loss: 1.312e+04   Precision: 32.884%   Recall: 34.943%
Train   Epoch: 394 / 600   Loss:    6465   Precision: 72.922%   Recall: 59.054%
Valid                   Loss: 1.317e+04   Precision: 35.875%   Recall: 27.490%
Train   Epoch: 395 / 600   Loss:    6754   Precision: 71.429%   Recall: 60.176%
Valid                   Loss: 1.306e+04   Precision: 33.124%   Recall: 34.645%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.55341721  8.28333333]
	 [32.22488785 38.96666667]
	 [30.88764191 22.83333333]
	 [40.61130142 25.8       ]
	 [54.65817261 16.6       ]]
Train   Epoch: 396 / 600   Loss:    7538   Precision: 69.828%   Recall: 59.004%
Valid                   Loss: 1.32e+04   Precision: 33.395%   Recall: 32.379%
Train   Epoch: 397 / 600   Loss:    6616   Precision: 72.900%   Recall: 58.145%
Valid                   Loss: 1.318e+04   Precision: 32.450%   Recall: 32.141%
Train   Epoch: 398 / 600   Loss:    6235   Precision: 72.218%   Recall: 60.469%
Valid                   Loss: 1.322e+04   Precision: 31.942%   Recall: 37.865%
Train   Epoch: 399 / 600   Loss:    6471   Precision: 73.254%   Recall: 59.974%
Valid                   Loss: 1.336e+04   Precision: 32.503%   Recall: 36.553%
Train   Epoch: 400 / 600   Loss:    6470   Precision: 73.691%   Recall: 59.863%
Valid                   Loss: 1.331e+04   Precision: 32.684%   Recall: 38.998%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.69631958 98.45      ]
	 [39.22319412  9.66666667]
	 [60.38593292 65.3       ]
	 [86.25447845 89.        ]
	 [52.39445877 26.66666667]]
Train   Epoch: 401 / 600   Loss:    6536   Precision: 74.179%   Recall: 59.570%
Valid                   Loss: 1.326e+04   Precision: 31.830%   Recall: 37.448%
Train   Epoch: 402 / 600   Loss:    6279   Precision: 73.801%   Recall: 58.610%
Valid                   Loss: 1.342e+04   Precision: 35.979%   Recall: 24.866%
Train   Epoch: 403 / 600   Loss:    6546   Precision: 72.527%   Recall: 61.277%
Valid                   Loss: 1.343e+04   Precision: 35.359%   Recall: 30.531%
Train   Epoch: 404 / 600   Loss:    6258   Precision: 74.202%   Recall: 59.206%
Valid                   Loss: 1.332e+04   Precision: 33.092%   Recall: 35.420%
Train   Epoch: 405 / 600   Loss:    6190   Precision: 75.263%   Recall: 61.368%
Valid                   Loss: 1.33e+04   Precision: 30.498%   Recall: 38.700%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.90923309 24.38333333]
	 [71.77947998 38.88333333]
	 [35.07279968 16.3       ]
	 [46.76687622 62.9       ]
	 [44.80635834 54.28333333]]
Train   Epoch: 406 / 600   Loss:    6063   Precision: 75.029%   Recall: 59.145%
Valid                   Loss: 1.33e+04   Precision: 31.598%   Recall: 36.196%
Train   Epoch: 407 / 600   Loss:    6455   Precision: 74.389%   Recall: 61.227%
Valid                   Loss: 1.35e+04   Precision: 29.946%   Recall: 39.893%
Train   Epoch: 408 / 600   Loss:    6412   Precision: 74.486%   Recall: 61.479%
Valid                   Loss: 1.344e+04   Precision: 29.671%   Recall: 42.993%
Train   Epoch: 409 / 600   Loss:    6200   Precision: 73.957%   Recall: 61.237%
Valid                   Loss: 1.339e+04   Precision: 30.804%   Recall: 36.315%
Train   Epoch: 410 / 600   Loss:    6257   Precision: 74.864%   Recall: 61.217%
Valid                   Loss: 1.342e+04   Precision: 32.593%   Recall: 28.861%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[123.86209106 160.28333333]
	 [ 35.72421265  39.33333333]
	 [ 93.26179504  69.91666667]
	 [ 26.87696266  25.45      ]
	 [ 21.80999565  42.36666667]]
Train   Epoch: 411 / 600   Loss:    5969   Precision: 75.278%   Recall: 61.661%
Valid                   Loss: 1.338e+04   Precision: 29.415%   Recall: 34.168%
Train   Epoch: 412 / 600   Loss:    5849   Precision: 74.712%   Recall: 60.338%
Valid                   Loss: 1.345e+04   Precision: 30.215%   Recall: 34.287%
Train   Epoch: 413 / 600   Loss:    6030   Precision: 73.801%   Recall: 61.914%
Valid                   Loss: 1.323e+04   Precision: 33.097%   Recall: 30.590%
Train   Epoch: 414 / 600   Loss:    5887   Precision: 74.859%   Recall: 60.570%
Valid                   Loss: 1.345e+04   Precision: 29.195%   Recall: 41.085%
Train   Epoch: 415 / 600   Loss:    6359   Precision: 74.726%   Recall: 59.246%
Valid                   Loss: 1.338e+04   Precision: 35.385%   Recall: 27.430%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[50.30600739 32.4       ]
	 [42.52453995 25.66666667]
	 [43.18172836 58.33333333]
	 [37.24944305 35.16666667]
	 [35.27648544 40.        ]]
Train   Epoch: 416 / 600   Loss:    5837   Precision: 76.344%   Recall: 59.711%
Valid                   Loss: 1.34e+04   Precision: 32.239%   Recall: 32.797%
Train   Epoch: 417 / 600   Loss:    6070   Precision: 71.472%   Recall: 61.166%
Valid                   Loss: 1.363e+04   Precision: 30.889%   Recall: 28.384%
Train   Epoch: 418 / 600   Loss:    6086   Precision: 73.165%   Recall: 60.449%
Valid                   Loss: 1.345e+04   Precision: 32.343%   Recall: 35.719%
Train   Epoch: 419 / 600   Loss:    6004   Precision: 71.948%   Recall: 61.399%
Valid                   Loss: 1.32e+04   Precision: 37.011%   Recall: 25.403%
Train   Epoch: 420 / 600   Loss:    6394   Precision: 74.349%   Recall: 59.751%
Valid                   Loss: 1.333e+04   Precision: 30.694%   Recall: 37.448%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[64.29846191 76.        ]
	 [32.31668091 15.76666667]
	 [58.42963028 46.48333333]
	 [76.43598938 38.58333333]
	 [22.64370346 53.81666667]]
Train   Epoch: 421 / 600   Loss:    6277   Precision: 71.339%   Recall: 60.014%
Valid                   Loss: 1.336e+04   Precision: 33.849%   Recall: 24.806%
Train   Epoch: 422 / 600   Loss:    6293   Precision: 73.268%   Recall: 59.327%
Valid                   Loss: 1.326e+04   Precision: 29.787%   Recall: 40.906%
Train   Epoch: 423 / 600   Loss:    5814   Precision: 74.882%   Recall: 59.408%
Valid                   Loss: 1.329e+04   Precision: 34.009%   Recall: 32.021%
Train   Epoch: 424 / 600   Loss:    6019   Precision: 74.904%   Recall: 60.924%
Valid                   Loss: 1.321e+04   Precision: 32.992%   Recall: 32.677%
Train   Epoch: 425 / 600   Loss:    5646   Precision: 76.815%   Recall: 59.994%
Valid                   Loss: 1.331e+04   Precision: 28.242%   Recall: 43.113%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.47258377 88.3       ]
	 [72.90426636 24.2       ]
	 [39.51081848 52.25      ]
	 [28.61973    30.53333333]
	 [34.90009689 37.28333333]]
Train   Epoch: 426 / 600   Loss:    6075   Precision: 74.290%   Recall: 61.025%
Valid                   Loss: 1.341e+04   Precision: 25.880%   Recall: 37.686%
Train   Epoch: 427 / 600   Loss:    7893   Precision: 66.125%   Recall: 60.873%
Valid                   Loss: 1.358e+04   Precision: 30.452%   Recall: 37.388%
Train   Epoch: 428 / 600   Loss:    6118   Precision: 68.989%   Recall: 60.135%
Valid                   Loss: 1.341e+04   Precision: 33.478%   Recall: 32.200%
Train   Epoch: 429 / 600   Loss:    5721   Precision: 74.309%   Recall: 59.741%
Valid                   Loss: 1.335e+04   Precision: 30.623%   Recall: 41.324%
Train   Epoch: 430 / 600   Loss:    5920   Precision: 74.645%   Recall: 62.743%
Valid                   Loss: 1.354e+04   Precision: 34.111%   Recall: 30.531%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 25.99691963  41.13333333]
	 [338.98165894 338.7       ]
	 [ 30.65161705  36.13333333]
	 [ 40.98484421  45.96666667]
	 [207.16506958 172.8       ]]
Train   Epoch: 431 / 600   Loss:    5603   Precision: 75.646%   Recall: 60.893%
Valid                   Loss: 1.347e+04   Precision: 32.909%   Recall: 37.030%
Train   Epoch: 432 / 600   Loss:    5610   Precision: 74.229%   Recall: 60.540%
Valid                   Loss: 1.361e+04   Precision: 25.339%   Recall: 45.736%
Train   Epoch: 433 / 600   Loss:    5805   Precision: 74.873%   Recall: 62.480%
Valid                   Loss: 1.33e+04   Precision: 31.783%   Recall: 33.810%
Train   Epoch: 434 / 600   Loss:    5811   Precision: 75.738%   Recall: 61.166%
Valid                   Loss: 1.349e+04   Precision: 32.298%   Recall: 38.462%
Train   Epoch: 435 / 600   Loss:    5798   Precision: 75.076%   Recall: 62.187%
Valid                   Loss: 1.348e+04   Precision: 32.225%   Recall: 37.567%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 62.48829269  49.51666667]
	 [ 55.68233871  24.15      ]
	 [ 32.05419922  25.45      ]
	 [122.19451904 167.73333333]
	 [ 69.74695587  44.5       ]]
Train   Epoch: 436 / 600   Loss:    5764   Precision: 75.736%   Recall: 60.024%
Valid                   Loss: 1.344e+04   Precision: 26.292%   Recall: 57.961%
Train   Epoch: 437 / 600   Loss:    8963   Precision: 51.720%   Recall: 60.934%
Valid                   Loss: 1.361e+04   Precision: 31.861%   Recall: 30.113%
Train   Epoch: 438 / 600   Loss:    6209   Precision: 72.135%   Recall: 58.074%
Valid                   Loss: 1.356e+04   Precision: 29.428%   Recall: 38.044%
Train   Epoch: 439 / 600   Loss:    5921   Precision: 74.302%   Recall: 59.691%
Valid                   Loss: 1.355e+04   Precision: 30.769%   Recall: 42.695%
Train   Epoch: 440 / 600   Loss:    5777   Precision: 73.837%   Recall: 60.489%
Valid                   Loss: 1.35e+04   Precision: 32.697%   Recall: 26.535%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[47.26802063 16.        ]
	 [72.01099396 18.88333333]
	 [34.89992523 55.16666667]
	 [28.79470825 31.8       ]
	 [89.5026474  51.35      ]]
Train   Epoch: 441 / 600   Loss:    5945   Precision: 74.984%   Recall: 60.459%
Valid                   Loss: 1.352e+04   Precision: 30.264%   Recall: 36.852%
Train   Epoch: 442 / 600   Loss:    5892   Precision: 76.245%   Recall: 61.560%
Valid                   Loss: 1.337e+04   Precision: 34.090%   Recall: 33.154%
Train   Epoch: 443 / 600   Loss:    5797   Precision: 75.914%   Recall: 61.247%
Valid                   Loss: 1.334e+04   Precision: 33.148%   Recall: 32.081%
Train   Epoch: 444 / 600   Loss:    5708   Precision: 76.056%   Recall: 61.853%
Valid                   Loss: 1.335e+04   Precision: 33.277%   Recall: 35.122%
Train   Epoch: 445 / 600   Loss:    5605   Precision: 75.840%   Recall: 60.681%
Valid                   Loss: 1.329e+04   Precision: 34.906%   Recall: 34.407%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 44.43147659  54.6       ]
	 [ 25.19104004  34.91666667]
	 [483.02505493 550.23333333]
	 [ 38.69351959  12.95      ]
	 [ 87.45594788  83.51666667]]
Train   Epoch: 446 / 600   Loss:    5480   Precision: 76.661%   Recall: 61.439%
Valid                   Loss: 1.337e+04   Precision: 33.774%   Recall: 30.471%
Train   Epoch: 447 / 600   Loss:    5621   Precision: 75.445%   Recall: 61.661%
Valid                   Loss: 1.332e+04   Precision: 32.113%   Recall: 34.526%
Train   Epoch: 448 / 600   Loss:    5365   Precision: 77.154%   Recall: 60.812%
Valid                   Loss: 1.335e+04   Precision: 32.405%   Recall: 36.792%
Train   Epoch: 449 / 600   Loss:    5349   Precision: 75.931%   Recall: 61.207%
Valid                   Loss: 1.351e+04   Precision: 33.376%   Recall: 31.187%
Train   Epoch: 450 / 600   Loss:    5384   Precision: 76.729%   Recall: 61.308%
Valid                   Loss: 1.359e+04   Precision: 31.389%   Recall: 39.475%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.06289291 73.41666667]
	 [67.63435364 32.11666667]
	 [39.90452957 43.25      ]
	 [82.80216217 72.11666667]
	 [49.28898621 66.11666667]]
Train   Epoch: 451 / 600   Loss:    5413   Precision: 76.969%   Recall: 61.833%
Valid                   Loss: 1.349e+04   Precision: 33.716%   Recall: 31.485%
Train   Epoch: 452 / 600   Loss:    5286   Precision: 76.653%   Recall: 61.146%
Valid                   Loss: 1.343e+04   Precision: 33.293%   Recall: 32.558%
Train   Epoch: 453 / 600   Loss:    4874   Precision: 78.276%   Recall: 60.914%
Valid                   Loss: 1.347e+04   Precision: 32.350%   Recall: 35.301%
Train   Epoch: 454 / 600   Loss:    5200   Precision: 76.050%   Recall: 63.308%
Valid                   Loss: 1.349e+04   Precision: 31.659%   Recall: 34.586%
Train   Epoch: 455 / 600   Loss:    5029   Precision: 76.846%   Recall: 62.985%
Valid                   Loss: 1.355e+04   Precision: 30.237%   Recall: 43.292%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 18.19217682   7.28333333]
	 [ 29.84514618  48.96666667]
	 [ 53.13691711  17.66666667]
	 [210.3274231  107.55      ]
	 [ 45.38725281  62.38333333]]
Train   Epoch: 456 / 600   Loss:    5293   Precision: 76.965%   Recall: 61.348%
Valid                   Loss: 1.347e+04   Precision: 30.263%   Recall: 38.402%
Train   Epoch: 457 / 600   Loss:    5325   Precision: 76.727%   Recall: 63.430%
Valid                   Loss: 1.358e+04   Precision: 29.286%   Recall: 42.576%
Train   Epoch: 458 / 600   Loss:    5341   Precision: 77.555%   Recall: 62.257%
Valid                   Loss: 1.374e+04   Precision: 27.699%   Recall: 45.438%
Train   Epoch: 459 / 600   Loss:    5315   Precision: 77.033%   Recall: 61.651%
Valid                   Loss: 1.349e+04   Precision: 32.932%   Recall: 35.838%
Train   Epoch: 460 / 600   Loss:    5212   Precision: 77.388%   Recall: 62.803%
Valid                   Loss: 1.342e+04   Precision: 30.717%   Recall: 40.131%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.53017426 43.91666667]
	 [18.23194504 19.46666667]
	 [48.54514694 24.83333333]
	 [40.27958679 21.38333333]
	 [45.81420135 27.28333333]]
Train   Epoch: 461 / 600   Loss:    5174   Precision: 77.680%   Recall: 61.722%
Valid                   Loss: 1.345e+04   Precision: 32.139%   Recall: 34.764%
Train   Epoch: 462 / 600   Loss:    5253   Precision: 78.720%   Recall: 63.511%
Valid                   Loss: 1.354e+04   Precision: 33.091%   Recall: 38.044%
Train   Epoch: 463 / 600   Loss:    5095   Precision: 77.819%   Recall: 61.651%
Valid                   Loss: 1.355e+04   Precision: 32.090%   Recall: 28.205%
Train   Epoch: 464 / 600   Loss:    5025   Precision: 76.791%   Recall: 63.359%
Valid                   Loss: 1.359e+04   Precision: 29.127%   Recall: 41.562%
Train   Epoch: 465 / 600   Loss:    5109   Precision: 77.729%   Recall: 63.238%
Valid                   Loss: 1.338e+04   Precision: 31.135%   Recall: 39.416%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 59.52359772  91.43333333]
	 [ 48.17501068  47.46666667]
	 [101.965065   113.33333333]
	 [ 33.02223587  18.5       ]
	 [ 32.06813049  31.13333333]]
Train   Epoch: 466 / 600   Loss:    5183   Precision: 78.181%   Recall: 61.227%
Valid                   Loss: 1.367e+04   Precision: 32.565%   Recall: 34.526%
Train   Epoch: 467 / 600   Loss:    5162   Precision: 77.066%   Recall: 63.601%
Valid                   Loss: 1.361e+04   Precision: 30.128%   Recall: 36.613%
Train   Epoch: 468 / 600   Loss:    4890   Precision: 78.189%   Recall: 63.864%
Valid                   Loss: 1.357e+04   Precision: 34.068%   Recall: 27.668%
Train   Epoch: 469 / 600   Loss:    5214   Precision: 77.499%   Recall: 62.753%
Valid                   Loss: 1.364e+04   Precision: 30.370%   Recall: 36.673%
Train   Epoch: 470 / 600   Loss:    4999   Precision: 76.795%   Recall: 63.238%
Valid                   Loss: 1.356e+04   Precision: 36.777%   Recall: 28.444%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 21.59514618  38.38333333]
	 [ 33.11470795  49.95      ]
	 [ 47.69926453  30.66666667]
	 [ 38.26560211   8.51666667]
	 [460.69406128 471.325     ]]
Train   Epoch: 471 / 600   Loss:    5563   Precision: 74.912%   Recall: 60.499%
Valid                   Loss: 1.351e+04   Precision: 32.928%   Recall: 33.870%
Train   Epoch: 472 / 600   Loss:    5328   Precision: 75.945%   Recall: 63.329%
Valid                   Loss: 1.37e+04   Precision: 34.669%   Recall: 22.182%
Train   Epoch: 473 / 600   Loss:    5197   Precision: 77.216%   Recall: 61.439%
Valid                   Loss: 1.367e+04   Precision: 27.896%   Recall: 44.663%
Train   Epoch: 474 / 600   Loss:    5070   Precision: 76.611%   Recall: 60.772%
Valid                   Loss: 1.346e+04   Precision: 29.867%   Recall: 41.622%
Train   Epoch: 475 / 600   Loss:    5173   Precision: 75.625%   Recall: 62.015%
Valid                   Loss: 1.332e+04   Precision: 31.567%   Recall: 40.847%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 46.05381775  31.68333333]
	 [ 98.99914551 123.33333333]
	 [ 41.85896301  35.25      ]
	 [ 46.521698    43.08333333]
	 [ 71.09644318 101.91666667]]
Train   Epoch: 476 / 600   Loss:    4984   Precision: 78.829%   Recall: 63.288%
Valid                   Loss: 1.362e+04   Precision: 31.926%   Recall: 35.182%
Train   Epoch: 477 / 600   Loss:    5050   Precision: 77.313%   Recall: 64.188%
Valid                   Loss: 1.342e+04   Precision: 33.308%   Recall: 25.999%
Train   Epoch: 478 / 600   Loss:    5201   Precision: 76.987%   Recall: 60.479%
Valid                   Loss: 1.339e+04   Precision: 30.988%   Recall: 38.342%
Train   Epoch: 479 / 600   Loss:    5671   Precision: 74.687%   Recall: 60.853%
Valid                   Loss: 1.368e+04   Precision: 32.599%   Recall: 31.783%
Train   Epoch: 480 / 600   Loss:    5128   Precision: 75.737%   Recall: 62.298%
Valid                   Loss: 1.359e+04   Precision: 33.616%   Recall: 25.999%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.61238098 60.66666667]
	 [62.75309372 42.85      ]
	 [55.98952103 20.55      ]
	 [49.2129364  48.78333333]
	 [42.81420517 46.15      ]]
Train   Epoch: 481 / 600   Loss:    4793   Precision: 77.584%   Recall: 62.813%
Valid                   Loss: 1.374e+04   Precision: 34.199%   Recall: 31.425%
Train   Epoch: 482 / 600   Loss:    5021   Precision: 76.997%   Recall: 62.439%
Valid                   Loss: 1.368e+04   Precision: 32.993%   Recall: 34.645%
Train   Epoch: 483 / 600   Loss:    5134   Precision: 76.915%   Recall: 61.783%
Valid                   Loss: 1.369e+04   Precision: 29.556%   Recall: 38.879%
Train   Epoch: 484 / 600   Loss:    4818   Precision: 78.505%   Recall: 62.520%
Valid                   Loss: 1.367e+04   Precision: 28.342%   Recall: 41.085%
Train   Epoch: 485 / 600   Loss:    4905   Precision: 78.369%   Recall: 63.228%
Valid                   Loss: 1.363e+04   Precision: 32.124%   Recall: 29.577%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[57.40192413 62.16666667]
	 [35.791008   25.        ]
	 [52.36260986 39.78333333]
	 [41.2120285  59.03333333]
	 [42.33180618 38.88333333]]
Train   Epoch: 486 / 600   Loss:    4719   Precision: 76.796%   Recall: 62.207%
Valid                   Loss: 1.344e+04   Precision: 29.710%   Recall: 47.108%
Train   Epoch: 487 / 600   Loss:    5198   Precision: 77.360%   Recall: 62.945%
Valid                   Loss: 1.368e+04   Precision: 29.800%   Recall: 40.072%
Train   Epoch: 488 / 600   Loss:    4901   Precision: 79.288%   Recall: 62.126%
Valid                   Loss: 1.358e+04   Precision: 33.989%   Recall: 32.976%
Train   Epoch: 489 / 600   Loss:    4639   Precision: 78.914%   Recall: 63.612%
Valid                   Loss: 1.371e+04   Precision: 31.465%   Recall: 31.127%
Train   Epoch: 490 / 600   Loss:    4564   Precision: 79.008%   Recall: 63.248%
Valid                   Loss: 1.348e+04   Precision: 36.685%   Recall: 28.503%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.22548294 17.55      ]
	 [40.30406952 33.41666667]
	 [30.15967751 32.83333333]
	 [37.14579391 58.        ]
	 [73.49197388 29.66666667]]
Train   Epoch: 491 / 600   Loss:    4788   Precision: 78.587%   Recall: 63.864%
Valid                   Loss: 1.357e+04   Precision: 35.858%   Recall: 27.668%
Train   Epoch: 492 / 600   Loss:    4857   Precision: 78.099%   Recall: 64.430%
Valid                   Loss: 1.378e+04   Precision: 33.401%   Recall: 29.219%
Train   Epoch: 493 / 600   Loss:    4903   Precision: 79.819%   Recall: 63.349%
Valid                   Loss: 1.354e+04   Precision: 35.507%   Recall: 24.031%
Train   Epoch: 494 / 600   Loss:    4764   Precision: 78.270%   Recall: 63.076%
Valid                   Loss: 1.361e+04   Precision: 28.160%   Recall: 49.016%
Train   Epoch: 495 / 600   Loss:    5075   Precision: 78.595%   Recall: 63.632%
Valid                   Loss: 1.343e+04   Precision: 36.000%   Recall: 26.834%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 49.55132294  34.23333333]
	 [ 35.20061874  20.6       ]
	 [ 19.50905037  35.        ]
	 [102.24607086 112.38333333]
	 [ 16.2672596   25.65      ]]
Train   Epoch: 496 / 600   Loss:    4542   Precision: 78.809%   Recall: 64.188%
Valid                   Loss: 1.369e+04   Precision: 34.185%   Recall: 27.132%
Train   Epoch: 497 / 600   Loss:    4632   Precision: 79.513%   Recall: 62.985%
Valid                   Loss: 1.357e+04   Precision: 30.159%   Recall: 39.475%
Train   Epoch: 498 / 600   Loss:    4667   Precision: 79.286%   Recall: 63.046%
Valid                   Loss: 1.36e+04   Precision: 30.878%   Recall: 36.494%
Train   Epoch: 499 / 600   Loss:    4639   Precision: 79.302%   Recall: 64.541%
Valid                   Loss: 1.377e+04   Precision: 30.853%   Recall: 35.599%
Train   Epoch: 500 / 600   Loss:    4797   Precision: 79.485%   Recall: 64.289%
Valid                   Loss: 1.34e+04   Precision: 32.592%   Recall: 38.462%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[32.03884125 28.21666667]
	 [36.58562851 40.21666667]
	 [35.05920792 43.03333333]
	 [62.6278038  29.11666667]
	 [58.233078   35.88333333]]
Train   Epoch: 501 / 600   Loss:    4582   Precision: 77.283%   Recall: 64.218%
Valid                   Loss: 1.387e+04   Precision: 32.624%   Recall: 31.067%
Train   Epoch: 502 / 600   Loss:    4678   Precision: 79.853%   Recall: 65.724%
Valid                   Loss: 1.373e+04   Precision: 35.495%   Recall: 29.696%
Train   Epoch: 503 / 600   Loss:    4413   Precision: 79.651%   Recall: 64.945%
Valid                   Loss: 1.364e+04   Precision: 33.804%   Recall: 29.994%
Train   Epoch: 504 / 600   Loss:    4472   Precision: 79.957%   Recall: 63.814%
Valid                   Loss: 1.363e+04   Precision: 34.704%   Recall: 32.200%
Train   Epoch: 505 / 600   Loss:    4753   Precision: 77.920%   Recall: 63.834%
Valid                   Loss: 1.359e+04   Precision: 34.764%   Recall: 28.980%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.60959244 60.16666667]
	 [38.0557518  14.33333333]
	 [43.97090912 24.        ]
	 [38.88834381 27.28333333]
	 [24.48709106 41.2       ]]
Train   Epoch: 506 / 600   Loss:    4889   Precision: 79.082%   Recall: 61.964%
Valid                   Loss: 1.363e+04   Precision: 33.590%   Recall: 36.374%
Train   Epoch: 507 / 600   Loss:    4640   Precision: 80.400%   Recall: 64.208%
Valid                   Loss: 1.362e+04   Precision: 32.886%   Recall: 33.631%
Train   Epoch: 508 / 600   Loss:    4501   Precision: 78.643%   Recall: 63.480%
Valid                   Loss: 1.364e+04   Precision: 34.180%   Recall: 31.306%
Train   Epoch: 509 / 600   Loss:    4383   Precision: 80.314%   Recall: 64.026%
Valid                   Loss: 1.364e+04   Precision: 32.400%   Recall: 31.723%
Train   Epoch: 510 / 600   Loss:    4476   Precision: 79.344%   Recall: 65.794%
Valid                   Loss: 1.352e+04   Precision: 30.725%   Recall: 41.682%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[42.62987137 14.        ]
	 [38.84865952 12.03333333]
	 [31.90225601 52.13333333]
	 [53.20345688 41.13333333]
	 [76.60693359 80.3       ]]
Train   Epoch: 511 / 600   Loss:    4630   Precision: 79.200%   Recall: 64.410%
Valid                   Loss: 1.363e+04   Precision: 30.939%   Recall: 32.618%
Train   Epoch: 512 / 600   Loss:    4695   Precision: 77.623%   Recall: 63.025%
Valid                   Loss: 1.366e+04   Precision: 31.911%   Recall: 33.453%
Train   Epoch: 513 / 600   Loss:    4811   Precision: 79.585%   Recall: 65.825%
Valid                   Loss: 1.365e+04   Precision: 30.665%   Recall: 39.058%
Train   Epoch: 514 / 600   Loss:    4429   Precision: 79.327%   Recall: 65.067%
Valid                   Loss: 1.379e+04   Precision: 32.071%   Recall: 29.278%
Train   Epoch: 515 / 600   Loss:    4338   Precision: 79.709%   Recall: 63.591%
Valid                   Loss: 1.363e+04   Precision: 31.327%   Recall: 30.411%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[56.67062378 56.11666667]
	 [52.94843292 18.25      ]
	 [26.83740616 49.5       ]
	 [39.41529846 23.06666667]
	 [73.60940552 87.33333333]]
Train   Epoch: 516 / 600   Loss:    4540   Precision: 78.136%   Recall: 64.713%
Valid                   Loss: 1.364e+04   Precision: 33.333%   Recall: 26.058%
Train   Epoch: 517 / 600   Loss:    4875   Precision: 76.477%   Recall: 62.652%
Valid                   Loss: 1.375e+04   Precision: 31.712%   Recall: 30.710%
Train   Epoch: 518 / 600   Loss:    4830   Precision: 76.805%   Recall: 63.945%
Valid                   Loss: 1.354e+04   Precision: 33.039%   Recall: 26.774%
Train   Epoch: 519 / 600   Loss:    4554   Precision: 79.433%   Recall: 63.187%
Valid                   Loss: 1.388e+04   Precision: 32.538%   Recall: 30.888%
Train   Epoch: 520 / 600   Loss:    4404   Precision: 76.947%   Recall: 63.005%
Valid                   Loss: 1.376e+04   Precision: 33.159%   Recall: 26.416%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[31.39236641 37.41666667]
	 [51.63700867 43.2       ]
	 [33.72610855 29.53333333]
	 [28.12769318 39.7       ]
	 [33.39256668 43.55      ]]
Train   Epoch: 521 / 600   Loss:    4662   Precision: 79.512%   Recall: 64.592%
Valid                   Loss: 1.364e+04   Precision: 29.859%   Recall: 34.168%
Train   Epoch: 522 / 600   Loss:    4360   Precision: 80.463%   Recall: 65.340%
Valid                   Loss: 1.354e+04   Precision: 31.091%   Recall: 30.590%
Train   Epoch: 523 / 600   Loss:    4301   Precision: 80.167%   Recall: 65.067%
Valid                   Loss: 1.388e+04   Precision: 31.534%   Recall: 28.563%
Train   Epoch: 524 / 600   Loss:    4761   Precision: 81.034%   Recall: 64.764%
Valid                   Loss: 1.351e+04   Precision: 29.768%   Recall: 39.833%
Train   Epoch: 525 / 600   Loss:    4433   Precision: 78.759%   Recall: 63.996%
Valid                   Loss: 1.373e+04   Precision: 30.736%   Recall: 29.398%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[1264.8302002  1529.8       ]
	 [  36.49836349   35.05      ]
	 [  32.3968277    32.        ]
	 [  76.9814682    25.        ]
	 [  50.60600281   33.43333333]]
Train   Epoch: 526 / 600   Loss:    4585   Precision: 78.872%   Recall: 64.167%
Valid                   Loss: 1.385e+04   Precision: 29.273%   Recall: 43.709%
Train   Epoch: 527 / 600   Loss:    4479   Precision: 80.260%   Recall: 65.612%
Valid                   Loss: 1.362e+04   Precision: 34.876%   Recall: 31.008%
Train   Epoch: 528 / 600   Loss:    4370   Precision: 81.680%   Recall: 65.057%
Valid                   Loss: 1.371e+04   Precision: 31.923%   Recall: 28.801%
Train   Epoch: 529 / 600   Loss:    4411   Precision: 80.412%   Recall: 66.209%
Valid                   Loss: 1.374e+04   Precision: 31.246%   Recall: 30.650%
Train   Epoch: 530 / 600   Loss:    4175   Precision: 81.639%   Recall: 64.743%
Valid                   Loss: 1.362e+04   Precision: 34.777%   Recall: 30.173%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 42.55682755  20.88333333]
	 [382.90100098 376.88333333]
	 [ 36.42222214  36.63333333]
	 [ 42.71028137  23.86666667]
	 [ 32.09169769  49.81666667]]
Train   Epoch: 531 / 600   Loss:    4462   Precision: 80.974%   Recall: 67.047%
Valid                   Loss: 1.357e+04   Precision: 35.583%   Recall: 31.127%
Train   Epoch: 532 / 600   Loss:    4200   Precision: 81.587%   Recall: 65.956%
Valid                   Loss: 1.393e+04   Precision: 29.599%   Recall: 37.806%
Train   Epoch: 533 / 600   Loss:    4166   Precision: 80.674%   Recall: 66.310%
Valid                   Loss: 1.374e+04   Precision: 33.577%   Recall: 30.173%
Train   Epoch: 534 / 600   Loss:    4170   Precision: 81.652%   Recall: 65.744%
Valid                   Loss: 1.364e+04   Precision: 31.761%   Recall: 38.939%
Train   Epoch: 535 / 600   Loss:    4231   Precision: 80.081%   Recall: 66.219%
Valid                   Loss: 1.386e+04   Precision: 39.801%   Recall: 19.082%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[68.31739044 79.9       ]
	 [ 4.61998558 20.4       ]
	 [49.95678711 39.3       ]
	 [17.59426498 45.33333333]
	 [17.48649597 46.        ]]
Train   Epoch: 536 / 600   Loss:    4355   Precision: 79.541%   Recall: 64.784%
Valid                   Loss: 1.383e+04   Precision: 30.505%   Recall: 36.017%
Train   Epoch: 537 / 600   Loss:    4406   Precision: 80.509%   Recall: 64.905%
Valid                   Loss: 1.367e+04   Precision: 32.124%   Recall: 32.200%
Train   Epoch: 538 / 600   Loss:    4088   Precision: 79.230%   Recall: 65.299%
Valid                   Loss: 1.37e+04   Precision: 32.410%   Recall: 32.081%
Train   Epoch: 539 / 600   Loss:    4216   Precision: 80.165%   Recall: 65.592%
Valid                   Loss: 1.357e+04   Precision: 34.670%   Recall: 27.847%
Train   Epoch: 540 / 600   Loss:    4291   Precision: 77.871%   Recall: 64.470%
Valid                   Loss: 1.36e+04   Precision: 33.534%   Recall: 26.595%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[36.84667969 63.        ]
	 [38.84912109 41.71666667]
	 [23.16898537 71.7       ]
	 [46.95195389 72.56666667]
	 [41.97867966 75.08333333]]
Train   Epoch: 541 / 600   Loss:    4347   Precision: 81.019%   Recall: 64.137%
Valid                   Loss: 1.367e+04   Precision: 34.231%   Recall: 28.801%
Train   Epoch: 542 / 600   Loss:    4105   Precision: 80.464%   Recall: 65.471%
Valid                   Loss: 1.41e+04   Precision: 34.431%   Recall: 27.430%
Train   Epoch: 543 / 600   Loss:    4214   Precision: 81.509%   Recall: 66.279%
Valid                   Loss: 1.362e+04   Precision: 30.009%   Recall: 38.402%
Train   Epoch: 544 / 600   Loss:    4036   Precision: 81.106%   Recall: 67.017%
Valid                   Loss: 1.382e+04   Precision: 34.137%   Recall: 27.847%
Train   Epoch: 545 / 600   Loss:    4020   Precision: 81.395%   Recall: 66.138%
Valid                   Loss: 1.404e+04   Precision: 30.518%   Recall: 28.444%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.53695679  9.58333333]
	 [36.8899765  48.61666667]
	 [32.07143402 31.83333333]
	 [ 0.49616152 38.75      ]
	 [27.42705154 23.11666667]]
Train   Epoch: 546 / 600   Loss:    4252   Precision: 82.171%   Recall: 66.320%
Valid                   Loss: 1.396e+04   Precision: 33.754%   Recall: 28.742%
Train   Epoch: 547 / 600   Loss:    4188   Precision: 80.763%   Recall: 67.623%
Valid                   Loss: 1.367e+04   Precision: 31.813%   Recall: 30.769%
Train   Epoch: 548 / 600   Loss:    4126   Precision: 81.864%   Recall: 65.228%
Valid                   Loss: 1.368e+04   Precision: 32.998%   Recall: 35.242%
Train   Epoch: 549 / 600   Loss:    4001   Precision: 81.904%   Recall: 68.058%
Valid                   Loss: 1.385e+04   Precision: 31.993%   Recall: 31.783%
Train   Epoch: 550 / 600   Loss:    4272   Precision: 80.130%   Recall: 66.057%
Valid                   Loss: 1.353e+04   Precision: 31.840%   Recall: 32.200%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[52.9228096  36.58333333]
	 [35.06057739 20.63333333]
	 [18.37443542 22.23333333]
	 [53.24869156 37.6       ]
	 [39.53158951 93.66666667]]
Train   Epoch: 551 / 600   Loss:    3892   Precision: 81.654%   Recall: 65.532%
Valid                   Loss: 1.403e+04   Precision: 33.487%   Recall: 25.939%
Train   Epoch: 552 / 600   Loss:    4056   Precision: 81.278%   Recall: 66.198%
Valid                   Loss: 1.368e+04   Precision: 34.217%   Recall: 33.095%
Train   Epoch: 553 / 600   Loss:    3796   Precision: 81.913%   Recall: 64.986%
Valid                   Loss: 1.362e+04   Precision: 32.170%   Recall: 37.925%
Train   Epoch: 554 / 600   Loss:    3856   Precision: 81.989%   Recall: 67.896%
Valid                   Loss: 1.364e+04   Precision: 32.481%   Recall: 33.333%
Train   Epoch: 555 / 600   Loss:    4057   Precision: 81.457%   Recall: 66.229%
Valid                   Loss: 1.378e+04   Precision: 29.027%   Recall: 33.631%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.99008942 84.83333333]
	 [53.01721191 39.85      ]
	 [39.32213974 41.95      ]
	 [43.98840714 50.5       ]
	 [57.59391403 82.7       ]]
Train   Epoch: 556 / 600   Loss:    4166   Precision: 78.116%   Recall: 65.107%
Valid                   Loss: 1.38e+04   Precision: 31.620%   Recall: 33.751%
Train   Epoch: 557 / 600   Loss:    4167   Precision: 80.531%   Recall: 67.421%
Valid                   Loss: 1.37e+04   Precision: 32.787%   Recall: 34.586%
Train   Epoch: 558 / 600   Loss:    3894   Precision: 82.003%   Recall: 65.936%
Valid                   Loss: 1.366e+04   Precision: 33.352%   Recall: 35.003%
Train   Epoch: 559 / 600   Loss:    4014   Precision: 81.778%   Recall: 67.027%
Valid                   Loss: 1.423e+04   Precision: 32.129%   Recall: 29.159%
Train   Epoch: 560 / 600   Loss:    4516   Precision: 76.079%   Recall: 64.117%
Valid                   Loss: 1.425e+04   Precision: 27.145%   Recall: 33.393%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[33.43876648 36.66666667]
	 [46.8687706  23.8       ]
	 [39.37348557 31.46666667]
	 [34.14148712 20.11666667]
	 [26.04086685 14.88333333]]
Train   Epoch: 561 / 600   Loss:    4524   Precision: 77.455%   Recall: 63.046%
Valid                   Loss: 1.397e+04   Precision: 32.912%   Recall: 29.517%
Train   Epoch: 562 / 600   Loss:    3963   Precision: 79.690%   Recall: 64.865%
Valid                   Loss: 1.367e+04   Precision: 33.333%   Recall: 29.875%
Train   Epoch: 563 / 600   Loss:    3926   Precision: 81.960%   Recall: 65.420%
Valid                   Loss: 1.402e+04   Precision: 30.922%   Recall: 35.182%
Train   Epoch: 564 / 600   Loss:    4123   Precision: 78.663%   Recall: 66.613%
Valid                   Loss: 1.4e+04   Precision: 28.377%   Recall: 32.320%
Train   Epoch: 565 / 600   Loss:    4300   Precision: 78.945%   Recall: 65.057%
Valid                   Loss: 1.375e+04   Precision: 35.513%   Recall: 25.581%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.2640419  39.46666667]
	 [32.82574081 31.2       ]
	 [29.18875885 21.18333333]
	 [31.90762711 38.45      ]
	 [37.24150848 25.98333333]]
Train   Epoch: 566 / 600   Loss:    4106   Precision: 79.645%   Recall: 65.794%
Valid                   Loss: 1.389e+04   Precision: 31.829%   Recall: 33.214%
Train   Epoch: 567 / 600   Loss:    3774   Precision: 82.328%   Recall: 67.694%
Valid                   Loss: 1.38e+04   Precision: 32.163%   Recall: 33.333%
Train   Epoch: 568 / 600   Loss:    3738   Precision: 81.958%   Recall: 66.421%
Valid                   Loss: 1.356e+04   Precision: 37.126%   Recall: 22.182%
Train   Epoch: 569 / 600   Loss:    3823   Precision: 82.011%   Recall: 67.583%
Valid                   Loss: 1.363e+04   Precision: 33.147%   Recall: 35.420%
Train   Epoch: 570 / 600   Loss:    3659   Precision: 83.427%   Recall: 67.805%
Valid                   Loss: 1.378e+04   Precision: 34.550%   Recall: 26.535%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[41.61159897 44.85      ]
	 [ 4.20515347 13.05      ]
	 [48.18412781 20.26666667]
	 [29.53164673 46.38333333]
	 [64.43985748 37.35      ]]
Train   Epoch: 571 / 600   Loss:    3462   Precision: 82.901%   Recall: 67.512%
Valid                   Loss: 1.364e+04   Precision: 37.027%   Recall: 25.104%
Train   Epoch: 572 / 600   Loss:    3484   Precision: 83.119%   Recall: 66.673%
Valid                   Loss: 1.381e+04   Precision: 29.830%   Recall: 35.540%
Train   Epoch: 573 / 600   Loss:    3598   Precision: 82.559%   Recall: 67.017%
Valid                   Loss: 1.378e+04   Precision: 33.950%   Recall: 27.370%
Train   Epoch: 574 / 600   Loss:    3728   Precision: 82.268%   Recall: 66.997%
Valid                   Loss: 1.366e+04   Precision: 33.221%   Recall: 29.278%
Train   Epoch: 575 / 600   Loss:    3716   Precision: 82.735%   Recall: 66.582%
Valid                   Loss: 1.355e+04   Precision: 32.730%   Recall: 30.173%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[55.82770157 92.83333333]
	 [95.33073425 63.43333333]
	 [38.24614334 17.83333333]
	 [21.3167038  23.71666667]
	 [30.92884064 47.        ]]
Train   Epoch: 576 / 600   Loss:    3666   Precision: 82.182%   Recall: 68.189%
Valid                   Loss: 1.379e+04   Precision: 33.845%   Recall: 27.609%
Train   Epoch: 577 / 600   Loss:    3635   Precision: 82.403%   Recall: 65.966%
Valid                   Loss: 1.369e+04   Precision: 35.720%   Recall: 25.283%
Train   Epoch: 578 / 600   Loss:    3791   Precision: 81.326%   Recall: 67.815%
Valid                   Loss: 1.371e+04   Precision: 32.851%   Recall: 21.646%
Train   Epoch: 579 / 600   Loss:    3922   Precision: 82.657%   Recall: 68.149%
Valid                   Loss: 1.364e+04   Precision: 29.374%   Recall: 38.342%
Train   Epoch: 580 / 600   Loss:    3843   Precision: 82.977%   Recall: 67.138%
Valid                   Loss: 1.36e+04   Precision: 32.971%   Recall: 27.132%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.56175232 32.83333333]
	 [47.70275879 20.3       ]
	 [11.40560722 18.66666667]
	 [54.1908989  33.9       ]
	 [76.01257324 91.76666667]]
Train   Epoch: 581 / 600   Loss:    3683   Precision: 82.257%   Recall: 67.694%
Valid                   Loss: 1.375e+04   Precision: 33.537%   Recall: 26.237%
Train   Epoch: 582 / 600   Loss:    3639   Precision: 83.051%   Recall: 67.290%
Valid                   Loss: 1.37e+04   Precision: 34.335%   Recall: 22.481%
Train   Epoch: 583 / 600   Loss:    3459   Precision: 82.611%   Recall: 68.553%
Valid                   Loss: 1.362e+04   Precision: 34.080%   Recall: 29.040%
Train   Epoch: 584 / 600   Loss:    3939   Precision: 82.311%   Recall: 66.067%
Valid                   Loss: 1.373e+04   Precision: 28.757%   Recall: 34.347%
Train   Epoch: 585 / 600   Loss:    3555   Precision: 82.806%   Recall: 67.401%
Valid                   Loss: 1.364e+04   Precision: 33.960%   Recall: 31.246%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[186.99298096 192.2       ]
	 [ 38.30716324  19.33333333]
	 [ 35.77151871  28.53333333]
	 [ 38.96433258  67.16666667]
	 [ 81.18697357  50.        ]]
Train   Epoch: 586 / 600   Loss:    3562   Precision: 81.992%   Recall: 66.209%
Valid                   Loss: 1.385e+04   Precision: 32.682%   Recall: 24.925%
Train   Epoch: 587 / 600   Loss:    3445   Precision: 82.351%   Recall: 67.947%
Valid                   Loss: 1.389e+04   Precision: 32.883%   Recall: 36.255%
Train   Epoch: 588 / 600   Loss:    3711   Precision: 82.181%   Recall: 67.714%
Valid                   Loss: 1.361e+04   Precision: 31.796%   Recall: 39.058%
Train   Epoch: 589 / 600   Loss:    3679   Precision: 82.566%   Recall: 68.149%
Valid                   Loss: 1.374e+04   Precision: 33.161%   Recall: 26.774%
Train   Epoch: 590 / 600   Loss:    3805   Precision: 81.487%   Recall: 67.431%
Valid                   Loss: 1.376e+04   Precision: 34.165%   Recall: 26.953%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[161.54066467 138.71666667]
	 [ 26.2057724   17.11666667]
	 [ 49.08049774  32.5       ]
	 [ 26.14977837  17.33333333]
	 [ 42.79640198  49.88333333]]
Train   Epoch: 591 / 600   Loss:    3632   Precision: 82.107%   Recall: 67.007%
Valid                   Loss: 1.377e+04   Precision: 38.732%   Recall: 22.958%
Train   Epoch: 592 / 600   Loss:    3518   Precision: 82.736%   Recall: 66.977%
Valid                   Loss: 1.373e+04   Precision: 33.857%   Recall: 25.701%
Train   Epoch: 593 / 600   Loss:    3490   Precision: 83.752%   Recall: 67.765%
Valid                   Loss: 1.371e+04   Precision: 33.223%   Recall: 23.912%
Train   Epoch: 594 / 600   Loss:    3702   Precision: 81.649%   Recall: 68.250%
Valid                   Loss: 1.403e+04   Precision: 36.237%   Recall: 18.605%
Train   Epoch: 595 / 600   Loss:    3523   Precision: 81.278%   Recall: 67.078%
Valid                   Loss: 1.401e+04   Precision: 32.129%   Recall: 28.623%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[43.2964859  19.55      ]
	 [40.86700058 27.58333333]
	 [37.25402069 29.08333333]
	 [33.46020126 46.73333333]
	 [ 1.43036389 11.9       ]]
Train   Epoch: 596 / 600   Loss:    3598   Precision: 82.433%   Recall: 67.047%
Valid                   Loss: 1.381e+04   Precision: 33.841%   Recall: 27.847%
Train   Epoch: 597 / 600   Loss:    3397   Precision: 82.806%   Recall: 67.694%
Valid                   Loss: 1.376e+04   Precision: 31.483%   Recall: 30.769%
Train   Epoch: 598 / 600   Loss:    3547   Precision: 82.002%   Recall: 68.371%
Valid                   Loss: 1.444e+04   Precision: 28.046%   Recall: 33.214%
Train   Epoch: 599 / 600   Loss:    4119   Precision: 77.736%   Recall: 64.390%
Valid                   Loss: 1.361e+04   Precision: 31.179%   Recall: 24.448%
Train   Epoch: 600 / 600   Loss:    4039   Precision: 81.763%   Recall: 66.643%
Valid                   Loss: 1.367e+04   Precision: 31.409%   Recall: 33.095%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[44.29912567 37.66666667]
	 [27.00792694 42.4       ]
	 [27.99789047 22.25      ]
	 [52.45505142 33.48333333]
	 [34.71301651 64.03333333]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split2 - different lr, more epchs, larger batch, dropout, loss with decaying_weights</span>

<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split2</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model_split2</span> <span class="o">=</span> <span class="n">model_split2</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>

<span class="n">train_losses_split2</span><span class="p">,</span><span class="n">valid_losses_split2</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split2</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 600   Loss: 2.101e+04   Precision: 39.841%   Recall: 68.228%
Valid                   Loss:    5167   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 002 / 600   Loss: 1.923e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4798   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 003 / 600   Loss: 1.854e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4694   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 004 / 600   Loss: 2.091e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5026   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 005 / 600   Loss: 1.853e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4781   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.83914948  35.46666667]
	 [ 93.83911896  29.21666667]
	 [ 93.83911896  60.08333333]
	 [ 93.83908844 103.66666667]
	 [ 93.83921814  70.45      ]]
Train   Epoch: 006 / 600   Loss: 1.9e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4959   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 007 / 600   Loss: 1.965e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4929   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 008 / 600   Loss: 1.919e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4920   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 009 / 600   Loss: 2.061e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4976   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 010 / 600   Loss: 1.976e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5097   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[104.31526184  21.        ]
	 [104.31542969  37.28333333]
	 [104.31533051  35.5       ]
	 [104.31537628  30.5       ]
	 [104.31539154  64.98333333]]
Train   Epoch: 011 / 600   Loss: 2.105e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4991   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 012 / 600   Loss: 1.835e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4759   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 013 / 600   Loss: 2.004e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4941   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 014 / 600   Loss: 2.043e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4946   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 015 / 600   Loss: 1.904e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4898   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.76109314 42.58333333]
	 [97.76107025 69.55      ]
	 [97.76110077 34.31666667]
	 [97.76108551 11.33333333]
	 [97.76107025 32.75      ]]
Train   Epoch: 016 / 600   Loss: 1.905e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4957   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 017 / 600   Loss: 1.888e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4802   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 018 / 600   Loss: 1.872e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4787   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 019 / 600   Loss: 1.889e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4905   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 020 / 600   Loss: 1.986e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4743   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.52920532 59.31666667]
	 [93.52913666 41.5       ]
	 [93.52919769 44.76666667]
	 [93.5291214  22.5       ]
	 [93.52911377 62.5       ]]
Train   Epoch: 021 / 600   Loss: 2.034e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4873   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 022 / 600   Loss: 1.873e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4639   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 023 / 600   Loss: 1.979e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5001   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 024 / 600   Loss: 1.916e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4959   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 025 / 600   Loss: 1.891e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5101   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.55818176  19.71666667]
	 [103.55821228  50.46666667]
	 [103.55821228 151.9       ]
	 [103.55727386  26.2       ]
	 [103.55819702  25.66666667]]
Train   Epoch: 026 / 600   Loss: 1.948e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4830   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 027 / 600   Loss: 1.865e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4640   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 028 / 600   Loss: 1.941e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5164   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 029 / 600   Loss: 2.008e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4824   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 030 / 600   Loss: 1.909e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5096   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[104.58660126 148.        ]
	 [104.58660889  40.33333333]
	 [104.58663177  51.38333333]
	 [104.58660126  55.63333333]
	 [104.58659363  32.33333333]]
Train   Epoch: 031 / 600   Loss: 1.874e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5049   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 032 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4687   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 033 / 600   Loss: 1.858e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4729   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 034 / 600   Loss: 2.014e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4658   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 035 / 600   Loss: 1.714e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4712   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.16506958 15.43333333]
	 [91.16501617 29.23333333]
	 [91.16506958 21.31666667]
	 [91.16506195 42.15      ]
	 [91.16506195 29.93333333]]
Train   Epoch: 036 / 600   Loss: 1.851e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4740   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 037 / 600   Loss: 1.855e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4700   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 038 / 600   Loss: 1.891e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4667   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 039 / 600   Loss: 2.074e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4706   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 040 / 600   Loss: 2.054e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4762   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.17420959  69.55      ]
	 [ 93.17418671  26.51666667]
	 [ 93.17424011 104.35      ]
	 [ 93.17420197  60.76666667]
	 [ 93.17418671 107.5       ]]
Train   Epoch: 041 / 600   Loss: 2.03e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4860   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 042 / 600   Loss: 1.8e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5075   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 043 / 600   Loss: 2.05e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4789   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 044 / 600   Loss: 1.842e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4901   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 045 / 600   Loss: 1.922e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4880   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.80725861  28.95      ]
	 [ 97.80722809  30.46666667]
	 [ 97.80725098  22.16666667]
	 [ 97.80725098  70.8       ]
	 [ 97.80725098 124.2       ]]
Train   Epoch: 046 / 600   Loss: 1.932e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4816   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 047 / 600   Loss: 1.871e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4813   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 048 / 600   Loss: 1.823e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4996   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 049 / 600   Loss: 1.959e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4714   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 050 / 600   Loss: 2.015e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4865   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 94.84004211  37.58333333]
	 [ 94.84003448  44.06666667]
	 [ 94.84004211  24.81666667]
	 [ 94.840065   318.4       ]
	 [ 94.84003448  25.21666667]]
Train   Epoch: 051 / 600   Loss: 1.953e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5019   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 052 / 600   Loss: 2.017e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4958   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 053 / 600   Loss: 1.833e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4810   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 054 / 600   Loss: 1.851e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5053   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 055 / 600   Loss: 2.049e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4992   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[101.93553162  20.58333333]
	 [101.93553925  26.75      ]
	 [101.93554688  49.88333333]
	 [101.93552399  30.66666667]
	 [101.93550873  36.83333333]]
Train   Epoch: 056 / 600   Loss: 2.124e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4724   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 057 / 600   Loss: 1.854e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4684   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 058 / 600   Loss: 1.959e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4844   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 059 / 600   Loss: 1.92e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4819   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 060 / 600   Loss: 1.743e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5177   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[104.19286346  45.06666667]
	 [104.19286346  43.7       ]
	 [104.19287109  65.96666667]
	 [104.19284821  70.45      ]
	 [104.19285583  58.38333333]]
Train   Epoch: 061 / 600   Loss: 1.974e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4864   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 062 / 600   Loss: 1.923e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4960   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 063 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4897   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 064 / 600   Loss: 2.06e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4875   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 065 / 600   Loss: 1.895e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4835   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 94.74849701 132.06666667]
	 [ 94.74849701 652.63333333]
	 [ 94.7485199   41.91666667]
	 [ 94.74849701  34.        ]
	 [ 94.74850464  51.6       ]]
Train   Epoch: 066 / 600   Loss: 1.753e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4977   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 067 / 600   Loss: 1.86e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4988   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 068 / 600   Loss: 1.898e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4917   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 069 / 600   Loss: 1.952e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4892   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 070 / 600   Loss: 1.943e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4807   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 94.81415558  24.15      ]
	 [ 94.81414795 338.7       ]
	 [ 94.81415558  27.95      ]
	 [ 94.81417084  40.61666667]
	 [ 94.81415558 106.13333333]]
Train   Epoch: 071 / 600   Loss: 2.144e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4827   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 072 / 600   Loss: 1.903e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4714   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 073 / 600   Loss: 2.071e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4644   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 074 / 600   Loss: 1.834e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4854   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 075 / 600   Loss: 1.84e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4900   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.78052521 20.78333333]
	 [99.78052521 46.91666667]
	 [99.78050995 24.5       ]
	 [99.78052521 34.85      ]
	 [99.78052521 54.16666667]]
Train   Epoch: 076 / 600   Loss: 1.793e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4707   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 077 / 600   Loss: 1.816e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4923   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 078 / 600   Loss: 1.95e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4700   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 079 / 600   Loss: 2.005e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4647   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 080 / 600   Loss: 1.827e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4829   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.96039581 39.55      ]
	 [95.96039581 11.41666667]
	 [95.96038055 33.58333333]
	 [95.96043396 42.5       ]
	 [95.96040344 82.2       ]]
Train   Epoch: 081 / 600   Loss: 1.942e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4870   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 082 / 600   Loss: 1.936e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4653   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 083 / 600   Loss: 1.915e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4857   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 084 / 600   Loss: 1.769e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4696   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 085 / 600   Loss: 2.054e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4597   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.10441589 28.75      ]
	 [87.10440826 23.33333333]
	 [87.10442352 36.9       ]
	 [87.10445404 37.4       ]
	 [87.10440826 14.1       ]]
Train   Epoch: 086 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4833   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 087 / 600   Loss: 1.973e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4801   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 088 / 600   Loss: 1.733e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4830   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 089 / 600   Loss: 1.889e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4676   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 090 / 600   Loss: 1.845e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4899   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.02091217 60.76666667]
	 [97.02090454 59.71666667]
	 [97.02090454 49.7       ]
	 [97.02089691 28.        ]
	 [97.02091217 33.53333333]]
Train   Epoch: 091 / 600   Loss: 1.78e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4942   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 092 / 600   Loss: 1.887e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4984   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 093 / 600   Loss: 1.968e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5128   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 094 / 600   Loss: 1.882e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4859   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 095 / 600   Loss: 1.858e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4956   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[101.51996613  85.73333333]
	 [101.51995087  40.        ]
	 [101.51994324  43.        ]
	 [101.51994324  43.18333333]
	 [101.51996613  37.96666667]]
Train   Epoch: 096 / 600   Loss: 1.861e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5014   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 097 / 600   Loss: 1.979e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4738   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 098 / 600   Loss: 2.038e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5134   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 099 / 600   Loss: 2.007e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4818   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 100 / 600   Loss: 2.008e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5507   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[113.15605164  32.46666667]
	 [113.15608215  50.08333333]
	 [113.15608215  28.93333333]
	 [113.15606689  41.38333333]
	 [113.15605164  24.        ]]
Train   Epoch: 101 / 600   Loss: 1.806e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4977   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 102 / 600   Loss: 1.942e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4895   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 103 / 600   Loss: 1.82e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4786   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 104 / 600   Loss: 2.018e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5101   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 105 / 600   Loss: 1.851e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4775   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.03540039 47.6       ]
	 [95.03536224 38.5       ]
	 [95.03538513 14.        ]
	 [95.03539276 38.13333333]
	 [95.03543091 19.        ]]
Train   Epoch: 106 / 600   Loss: 1.876e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5009   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 107 / 600   Loss: 1.919e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4862   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 108 / 600   Loss: 1.823e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4727   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 109 / 600   Loss: 1.923e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4937   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 110 / 600   Loss: 1.881e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5084   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.25619507  16.23333333]
	 [103.2562027   46.05      ]
	 [103.25617981  49.93333333]
	 [103.25617218  13.91666667]
	 [103.25617981  20.83333333]]
Train   Epoch: 111 / 600   Loss: 1.832e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4864   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 112 / 600   Loss: 2.04e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 113 / 600   Loss: 2.058e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4865   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 114 / 600   Loss: 1.912e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4949   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 115 / 600   Loss: 1.946e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4941   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.05994415 54.78333333]
	 [99.05996704 49.05      ]
	 [99.05994415 44.        ]
	 [99.05996704 16.8       ]
	 [99.05993652 53.26666667]]
Train   Epoch: 116 / 600   Loss: 2.079e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4802   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 117 / 600   Loss: 1.797e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4831   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 118 / 600   Loss: 2.026e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4805   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 119 / 600   Loss: 1.774e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4707   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 120 / 600   Loss: 1.963e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4842   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.62452698 14.61666667]
	 [95.62454987 66.31666667]
	 [95.62450409 37.4       ]
	 [95.62453461 35.33333333]
	 [95.62453461  8.13333333]]
Train   Epoch: 121 / 600   Loss: 1.839e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4741   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 122 / 600   Loss: 1.81e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4845   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 123 / 600   Loss: 1.941e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4850   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 124 / 600   Loss: 1.959e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4915   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 125 / 600   Loss: 1.895e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4646   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.53935242  21.13333333]
	 [ 88.53932953  20.96666667]
	 [ 88.53939819  40.7       ]
	 [ 88.53935242  21.28333333]
	 [ 88.53934479 199.51666667]]
Train   Epoch: 126 / 600   Loss: 1.986e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4899   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 127 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4795   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 128 / 600   Loss: 1.96e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4941   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 129 / 600   Loss: 1.941e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4713   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 130 / 600   Loss: 1.924e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4622   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[85.81736755 30.2       ]
	 [85.81735992 56.45      ]
	 [85.81734467 27.61666667]
	 [85.81734467 45.5       ]
	 [85.81734467 28.        ]]
Train   Epoch: 131 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4962   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 132 / 600   Loss: 2.098e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4874   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 133 / 600   Loss: 1.944e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4909   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 134 / 600   Loss: 2.008e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4831   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 135 / 600   Loss: 1.968e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4855   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.17868042 59.36666667]
	 [97.17868805 11.3       ]
	 [97.17868042 24.        ]
	 [97.17868042 67.5       ]
	 [97.17868042 17.33333333]]
Train   Epoch: 136 / 600   Loss: 1.909e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5001   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 137 / 600   Loss: 1.829e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4979   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 138 / 600   Loss: 1.736e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5006   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 139 / 600   Loss: 1.883e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5098   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 140 / 600   Loss: 1.808e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4689   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.95935822 34.38333333]
	 [90.9593277  40.65      ]
	 [90.95928192 76.63333333]
	 [90.95934296 46.45      ]
	 [90.95931244 45.16666667]]
Train   Epoch: 141 / 600   Loss: 1.977e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4848   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 142 / 600   Loss: 1.941e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4755   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 143 / 600   Loss: 1.812e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4864   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 144 / 600   Loss: 1.787e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4674   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 145 / 600   Loss: 1.967e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4931   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.77677155 74.88333333]
	 [99.77677155 57.5       ]
	 [99.77675629 22.41666667]
	 [99.77676392 58.75      ]
	 [99.77676392 74.91666667]]
Train   Epoch: 146 / 600   Loss: 2.049e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4923   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 147 / 600   Loss: 1.913e+04   Precision: 39.839%   Recall: 99.969%
Valid                   Loss:    4960   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 148 / 600   Loss: 1.782e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5028   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 149 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5015   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 150 / 600   Loss: 1.884e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5010   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.62722015  53.08333333]
	 [102.62725067  62.66666667]
	 [102.62719727  60.13333333]
	 [102.62721252  28.45      ]
	 [102.62721252  64.51666667]]
Train   Epoch: 151 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4811   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 152 / 600   Loss: 1.952e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4890   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 153 / 600   Loss: 2.091e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4950   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 154 / 600   Loss: 2.039e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4671   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 155 / 600   Loss: 1.931e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4849   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.07887268  37.71666667]
	 [ 97.07885742 119.5       ]
	 [ 97.07884979  40.7       ]
	 [ 97.07887268 220.83333333]
	 [ 97.07889557  25.        ]]
Train   Epoch: 156 / 600   Loss: 1.993e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5048   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 157 / 600   Loss: 2.024e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4798   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 158 / 600   Loss: 1.757e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4910   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 159 / 600   Loss: 1.934e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4933   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 160 / 600   Loss: 2.018e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4774   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.59286499 130.36666667]
	 [ 93.59283447  98.91666667]
	 [ 93.59284973  13.        ]
	 [ 93.59287262  23.2       ]
	 [ 93.59284973  52.25      ]]
Train   Epoch: 161 / 600   Loss: 1.909e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4944   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 162 / 600   Loss: 1.921e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5304   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 163 / 600   Loss: 2.05e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5022   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 164 / 600   Loss: 2.02e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4835   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 165 / 600   Loss: 1.872e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5012   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.28638458  74.33333333]
	 [102.2864151   37.86666667]
	 [102.28634644  22.        ]
	 [102.28637695  44.91666667]
	 [102.28642273  47.1       ]]
Train   Epoch: 166 / 600   Loss: 1.976e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4890   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 167 / 600   Loss: 1.897e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4800   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 168 / 600   Loss: 1.937e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4943   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 169 / 600   Loss: 1.946e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4939   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 170 / 600   Loss: 1.86e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4682   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 89.61534119  19.75      ]
	 [ 89.61533356 176.03333333]
	 [ 89.61536407  29.86666667]
	 [ 89.61536407  43.83333333]
	 [ 89.61533356  67.9       ]]
Train   Epoch: 171 / 600   Loss: 1.973e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4910   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 172 / 600   Loss: 1.966e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4875   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 173 / 600   Loss: 1.809e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4767   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 174 / 600   Loss: 1.963e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4842   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 175 / 600   Loss: 1.87e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5052   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.63156128  22.36666667]
	 [103.63153076  37.23333333]
	 [103.63156128  25.96666667]
	 [103.63158417 143.75      ]
	 [103.63156128  26.425     ]]
Train   Epoch: 176 / 600   Loss: 1.983e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4803   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 177 / 600   Loss: 1.916e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4995   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 178 / 600   Loss: 1.942e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4704   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 179 / 600   Loss: 1.961e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5038   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 180 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5088   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.37030792  59.58333333]
	 [103.37033081  30.58333333]
	 [103.37033844  32.        ]
	 [103.37031555  32.1       ]
	 [103.37032318  19.08333333]]
Train   Epoch: 181 / 600   Loss: 1.897e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4851   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 182 / 600   Loss: 2.003e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5162   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 183 / 600   Loss: 2.143e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4791   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 184 / 600   Loss: 1.949e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4849   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 185 / 600   Loss: 2.214e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4971   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.07183838  40.88333333]
	 [100.07183838  61.05      ]
	 [100.07183838  32.25      ]
	 [100.07183838  12.61666667]
	 [100.07183838  27.11666667]]
Train   Epoch: 186 / 600   Loss: 1.894e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4939   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 187 / 600   Loss: 1.99e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5016   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 188 / 600   Loss: 1.994e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4812   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 189 / 600   Loss: 1.955e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4923   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 190 / 600   Loss: 1.883e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4905   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.66674042  75.61666667]
	 [ 98.66669464  31.55      ]
	 [ 98.66675568  27.4       ]
	 [ 98.66673279  19.31666667]
	 [ 98.6667099  687.23333333]]
Train   Epoch: 191 / 600   Loss: 2.095e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5181   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 192 / 600   Loss: 2.034e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4962   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 193 / 600   Loss: 1.964e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4612   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 194 / 600   Loss: 1.86e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5028   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 195 / 600   Loss: 1.941e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4986   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.08816528  34.11666667]
	 [100.08811951  13.16666667]
	 [100.08570862  52.63333333]
	 [100.08817291  45.2       ]
	 [100.0881958   16.5       ]]
Train   Epoch: 196 / 600   Loss: 2.189e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4740   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 197 / 600   Loss: 1.905e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5137   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 198 / 600   Loss: 1.859e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5100   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 199 / 600   Loss: 1.845e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4891   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 200 / 600   Loss: 1.831e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.61418152 60.58333333]
	 [96.61421967 52.        ]
	 [96.61421204 41.        ]
	 [96.61418152 57.4       ]
	 [96.61419678 31.26666667]]
Train   Epoch: 201 / 600   Loss: 1.962e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5022   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 202 / 600   Loss: 1.854e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4632   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 203 / 600   Loss: 1.837e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4883   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 204 / 600   Loss: 2.022e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5097   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 205 / 600   Loss: 1.848e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4850   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.38314056 39.66666667]
	 [96.38314056 52.16666667]
	 [96.38314056 25.96666667]
	 [96.38314056 40.38333333]
	 [96.38312531 36.56666667]]
Train   Epoch: 206 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4921   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 207 / 600   Loss: 1.908e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4894   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 208 / 600   Loss: 1.844e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4842   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 209 / 600   Loss: 1.999e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5082   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 210 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5030   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.75138092  17.28333333]
	 [100.75135803  35.55      ]
	 [100.75131989  16.26666667]
	 [100.7513504   46.56666667]
	 [100.7513504   39.        ]]
Train   Epoch: 211 / 600   Loss: 1.901e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4673   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 212 / 600   Loss: 1.942e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 213 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4718   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 214 / 600   Loss: 1.696e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4867   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 215 / 600   Loss: 1.837e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4677   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.62321472 26.56666667]
	 [88.62322998 21.41666667]
	 [88.62323761 19.25      ]
	 [88.62324524 81.48333333]
	 [88.62323761 33.43333333]]
Train   Epoch: 216 / 600   Loss: 1.897e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4786   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 217 / 600   Loss: 1.904e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4844   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 218 / 600   Loss: 1.883e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4901   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 219 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4680   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 220 / 600   Loss: 1.97e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4943   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.22653961 44.21666667]
	 [99.2265625  31.66666667]
	 [99.22658539 20.78333333]
	 [99.22657013 24.66666667]
	 [99.22652435 24.3       ]]
Train   Epoch: 221 / 600   Loss: 1.807e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4730   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 222 / 600   Loss: 1.89e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4776   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 223 / 600   Loss: 1.912e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4859   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 224 / 600   Loss: 1.786e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 225 / 600   Loss: 1.862e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4915   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 100.37968445   35.55      ]
	 [ 100.37969208 1105.46666667]
	 [ 100.3797226    28.5       ]
	 [ 100.37966919    6.68333333]
	 [ 100.38132477   27.35      ]]
Train   Epoch: 226 / 600   Loss: 2.026e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5203   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 227 / 600   Loss: 1.956e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4924   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 228 / 600   Loss: 2.005e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4864   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 229 / 600   Loss: 1.99e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5074   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 230 / 600   Loss: 1.906e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4849   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.58056641  27.55      ]
	 [ 96.58057404 235.46666667]
	 [ 96.58056641  54.16666667]
	 [ 96.58058167  26.86666667]
	 [ 96.58056641  63.16666667]]
Train   Epoch: 231 / 600   Loss: 1.889e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4720   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 232 / 600   Loss: 1.895e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4970   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 233 / 600   Loss: 1.828e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4704   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 234 / 600   Loss: 1.863e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4887   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 235 / 600   Loss: 1.968e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4794   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.15628815  50.03333333]
	 [ 96.15629578 107.76666667]
	 [ 96.15628052  27.03333333]
	 [ 96.15628815  24.11666667]
	 [ 96.15630341  19.        ]]
Train   Epoch: 236 / 600   Loss: 1.981e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4931   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 237 / 600   Loss: 1.924e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4700   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 238 / 600   Loss: 1.963e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4703   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 239 / 600   Loss: 1.78e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4871   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 240 / 600   Loss: 1.887e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4849   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.82324982 26.95      ]
	 [94.82326508 39.36666667]
	 [94.82323456 25.03333333]
	 [94.82324982 52.25      ]
	 [94.82324982 18.63333333]]
Train   Epoch: 241 / 600   Loss: 1.797e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4689   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 242 / 600   Loss: 1.936e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4906   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 243 / 600   Loss: 1.826e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4868   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 244 / 600   Loss: 1.824e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4815   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 245 / 600   Loss: 1.839e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4862   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.94514465  50.45      ]
	 [ 95.94512939  15.33333333]
	 [ 95.94514465  58.38333333]
	 [ 95.94515228  35.61666667]
	 [ 95.94514465 257.53333333]]
Train   Epoch: 246 / 600   Loss: 1.847e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4916   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 247 / 600   Loss: 1.93e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5091   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 248 / 600   Loss: 2.156e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4854   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 249 / 600   Loss: 1.904e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4868   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 250 / 600   Loss: 1.925e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4891   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.18370819 56.01666667]
	 [97.18373108 33.25      ]
	 [97.18372345 46.51666667]
	 [97.18373108 45.55      ]
	 [97.18370819 26.71666667]]
Train   Epoch: 251 / 600   Loss: 2.027e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4743   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 252 / 600   Loss: 2.11e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4822   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 253 / 600   Loss: 1.827e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4824   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 254 / 600   Loss: 2.015e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5004   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 255 / 600   Loss: 1.874e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.49913025 32.63333333]
	 [96.49913025 67.33333333]
	 [96.49911499 29.13333333]
	 [96.49913025 13.85      ]
	 [96.49913025 29.65      ]]
Train   Epoch: 256 / 600   Loss: 1.881e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4808   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 257 / 600   Loss: 1.913e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5226   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 258 / 600   Loss: 1.982e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4716   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 259 / 600   Loss: 2.039e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4851   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 260 / 600   Loss: 1.957e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4878   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.46216583 17.06666667]
	 [96.46215057 66.        ]
	 [96.46214294 16.5       ]
	 [96.46214294 60.51666667]
	 [96.46216583 44.28333333]]
Train   Epoch: 261 / 600   Loss: 1.928e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5036   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 262 / 600   Loss: 2.078e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4802   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 263 / 600   Loss: 1.879e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4927   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 264 / 600   Loss: 1.748e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4671   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 265 / 600   Loss: 1.946e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5120   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 103.31938934   64.5       ]
	 [ 103.31939697  141.13333333]
	 [ 103.3194046    26.96666667]
	 [ 103.31938934 1484.26666667]
	 [ 103.31939697   41.05      ]]
Train   Epoch: 266 / 600   Loss: 1.974e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4962   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 267 / 600   Loss: 1.954e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4808   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 268 / 600   Loss: 2.059e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4793   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 269 / 600   Loss: 1.843e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5060   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 270 / 600   Loss: 1.73e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4918   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.07746887 24.38333333]
	 [99.07748413 39.16666667]
	 [99.0774765  41.3       ]
	 [99.07746124 61.21666667]
	 [99.07745361 31.5       ]]
Train   Epoch: 271 / 600   Loss: 2.015e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4998   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 272 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4920   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 273 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5013   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 274 / 600   Loss: 1.945e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4883   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 275 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4956   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.56924438 52.28333333]
	 [99.56925964 27.61666667]
	 [99.56922913 49.9       ]
	 [99.56929016 79.9       ]
	 [99.56924438 23.55      ]]
Train   Epoch: 276 / 600   Loss: 1.825e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4852   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 277 / 600   Loss: 1.929e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4906   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 278 / 600   Loss: 1.897e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4869   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 279 / 600   Loss: 1.925e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4825   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 280 / 600   Loss: 1.832e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4809   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.14718628 41.28333333]
	 [94.14718628 84.        ]
	 [94.14719391 36.        ]
	 [94.14720154 43.66666667]
	 [94.14720154 96.08333333]]
Train   Epoch: 281 / 600   Loss: 1.975e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4814   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 282 / 600   Loss: 1.937e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4793   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 283 / 600   Loss: 1.924e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4938   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 284 / 600   Loss: 1.968e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4753   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 285 / 600   Loss: 2.011e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4746   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.30947876 17.66666667]
	 [92.30941772 36.06666667]
	 [92.30951691 45.66666667]
	 [92.30947876 56.        ]
	 [92.30951691 20.83333333]]
Train   Epoch: 286 / 600   Loss: 1.991e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4901   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 287 / 600   Loss: 1.834e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4787   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 288 / 600   Loss: 2.015e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4861   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 289 / 600   Loss: 1.851e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4791   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 290 / 600   Loss: 1.715e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4818   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.58657074 66.03333333]
	 [94.58656311 47.33333333]
	 [94.58654785 53.08333333]
	 [94.58654022 26.38333333]
	 [94.58654785 32.55      ]]
Train   Epoch: 291 / 600   Loss: 1.975e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4855   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 292 / 600   Loss: 1.95e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4856   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 293 / 600   Loss: 2.075e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4716   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 294 / 600   Loss: 1.99e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4666   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 295 / 600   Loss: 1.909e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4943   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.65785217 80.43333333]
	 [97.65785217 32.51666667]
	 [97.65786743 29.        ]
	 [97.6578598  31.75      ]
	 [97.65787506 20.21666667]]
Train   Epoch: 296 / 600   Loss: 1.907e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5028   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 297 / 600   Loss: 1.804e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 298 / 600   Loss: 2.083e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4854   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 299 / 600   Loss: 1.999e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4991   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 300 / 600   Loss: 2.027e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4785   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.0700531   34.03333333]
	 [ 96.07003784 171.63333333]
	 [ 96.0700531   41.61666667]
	 [ 96.07002258  34.95      ]
	 [ 96.0700531   26.75      ]]
Train   Epoch: 301 / 600   Loss: 2.034e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4870   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 302 / 600   Loss: 1.831e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5040   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 303 / 600   Loss: 2.064e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4898   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 304 / 600   Loss: 1.902e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5123   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 305 / 600   Loss: 2.191e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5165   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[106.5898819  585.        ]
	 [106.5898819   27.26666667]
	 [106.58997345  43.85      ]
	 [106.5899353   20.        ]
	 [106.58985138  32.25      ]]
Train   Epoch: 306 / 600   Loss: 1.852e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4702   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 307 / 600   Loss: 2.157e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4953   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 308 / 600   Loss: 1.824e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4932   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 309 / 600   Loss: 2.046e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5044   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 310 / 600   Loss: 1.819e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4713   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.85629272 61.        ]
	 [91.8562851  34.3       ]
	 [91.85629272 28.08333333]
	 [91.8562851  44.41666667]
	 [91.85626984 31.41666667]]
Train   Epoch: 311 / 600   Loss: 1.89e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4860   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 312 / 600   Loss: 2.088e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4650   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 313 / 600   Loss: 1.997e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4798   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 314 / 600   Loss: 1.829e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4836   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 315 / 600   Loss: 1.879e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5139   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.35720825  40.41666667]
	 [103.35720825  69.        ]
	 [103.35720825  16.11666667]
	 [103.35720062  54.8       ]
	 [103.35723114  50.8       ]]
Train   Epoch: 316 / 600   Loss: 1.79e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4753   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 317 / 600   Loss: 1.857e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4813   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 318 / 600   Loss: 1.899e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4742   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 319 / 600   Loss: 1.913e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4791   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 320 / 600   Loss: 1.944e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5057   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[103.12939453 211.95      ]
	 [103.12942505  22.7       ]
	 [103.1294632   72.58333333]
	 [103.12943268  36.71666667]
	 [103.12945557  91.93333333]]
Train   Epoch: 321 / 600   Loss: 1.967e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4986   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 322 / 600   Loss: 1.927e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4855   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 323 / 600   Loss: 1.802e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4949   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 324 / 600   Loss: 2.002e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4779   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 325 / 600   Loss: 2.122e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4955   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.79432678  15.33333333]
	 [ 99.79431915  58.78333333]
	 [ 99.79438019  41.25      ]
	 [ 99.79437256  72.56666667]
	 [ 99.79439545 107.66666667]]
Train   Epoch: 326 / 600   Loss: 1.949e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4856   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 327 / 600   Loss: 1.85e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4925   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 328 / 600   Loss: 1.853e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4969   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 329 / 600   Loss: 1.983e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4738   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 330 / 600   Loss: 1.978e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4751   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.94178772 50.26666667]
	 [92.94180298 46.75      ]
	 [92.94178009 39.83333333]
	 [92.94177246  9.83333333]
	 [92.94177246  9.38333333]]
Train   Epoch: 331 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4797   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 332 / 600   Loss: 1.976e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4862   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 333 / 600   Loss: 1.89e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4871   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 334 / 600   Loss: 2.121e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4866   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 335 / 600   Loss: 2.051e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4821   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.84533691 64.96666667]
	 [93.84527588 93.28333333]
	 [93.84525299 12.5       ]
	 [93.84526062 53.33333333]
	 [93.84528351 49.63333333]]
Train   Epoch: 336 / 600   Loss: 1.878e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4653   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 337 / 600   Loss: 1.791e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4724   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 338 / 600   Loss: 1.999e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4865   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 339 / 600   Loss: 1.923e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4744   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 340 / 600   Loss: 1.82e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4911   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.85690308 24.2       ]
	 [97.8568573  39.46666667]
	 [97.85682678 48.75      ]
	 [97.8568573  45.68333333]
	 [97.85683441 22.11666667]]
Train   Epoch: 341 / 600   Loss: 1.92e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4757   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 342 / 600   Loss: 1.96e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4879   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 343 / 600   Loss: 2.086e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4815   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 344 / 600   Loss: 1.94e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5054   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 345 / 600   Loss: 2.017e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4823   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.5361557  40.95      ]
	 [94.53607941 41.7       ]
	 [94.53614044 67.63333333]
	 [94.53616333 35.        ]
	 [94.5361557  53.18333333]]
Train   Epoch: 346 / 600   Loss: 1.836e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4825   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 347 / 600   Loss: 1.897e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4900   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 348 / 600   Loss: 1.997e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4727   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 349 / 600   Loss: 2.016e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4680   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 350 / 600   Loss: 1.866e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4981   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.23950958 36.56666667]
	 [99.23949432 42.91666667]
	 [99.23951721 27.03333333]
	 [99.23958588 29.58333333]
	 [99.23953247 36.91666667]]
Train   Epoch: 351 / 600   Loss: 1.947e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4820   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 352 / 600   Loss: 1.847e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4890   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 353 / 600   Loss: 1.784e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4829   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 354 / 600   Loss: 1.744e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4710   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 355 / 600   Loss: 1.973e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4825   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.45211029 53.33333333]
	 [96.45211029 38.16666667]
	 [96.45211029 22.48333333]
	 [96.45211792 86.38333333]
	 [96.45209503 18.83333333]]
Train   Epoch: 356 / 600   Loss: 1.899e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5060   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 357 / 600   Loss: 1.805e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4880   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 358 / 600   Loss: 1.837e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4823   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 359 / 600   Loss: 1.961e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5066   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 360 / 600   Loss: 1.803e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4890   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.26728058  42.65      ]
	 [ 98.26726532  33.95      ]
	 [ 98.26730347 152.91666667]
	 [ 98.2673111   43.91666667]
	 [ 98.26731873  10.28333333]]
Train   Epoch: 361 / 600   Loss: 1.961e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4855   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 362 / 600   Loss: 2.102e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4987   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 363 / 600   Loss: 1.876e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4838   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 364 / 600   Loss: 2.082e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5021   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 365 / 600   Loss: 1.808e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4764   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.32801056 44.85      ]
	 [93.32806396 62.2       ]
	 [93.32815552 16.06666667]
	 [93.32797241 62.71666667]
	 [93.32801819 43.06666667]]
Train   Epoch: 366 / 600   Loss: 2.095e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4885   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 367 / 600   Loss: 1.891e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4947   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 368 / 600   Loss: 1.884e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4792   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 369 / 600   Loss: 1.933e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4749   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 370 / 600   Loss: 1.838e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4887   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.8534317  47.73333333]
	 [96.8534317  27.6       ]
	 [96.8534317  78.58333333]
	 [96.85344696 44.38333333]
	 [96.8534317  18.33333333]]
Train   Epoch: 371 / 600   Loss: 1.947e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4899   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 372 / 600   Loss: 1.741e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4727   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 373 / 600   Loss: 2.083e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4783   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 374 / 600   Loss: 2.022e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4854   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 375 / 600   Loss: 1.95e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4757   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.94611359 42.63333333]
	 [92.9460907  21.03333333]
	 [92.9460907  30.58333333]
	 [92.94612885 29.95      ]
	 [92.9460907  40.11666667]]
Train   Epoch: 376 / 600   Loss: 1.892e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4972   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 377 / 600   Loss: 1.897e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4804   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 378 / 600   Loss: 1.873e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4799   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 379 / 600   Loss: 1.974e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5102   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 380 / 600   Loss: 1.851e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4858   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.9572525   27.86666667]
	 [ 97.95718384 103.65      ]
	 [ 97.9571991   47.5       ]
	 [ 97.95722961  23.        ]
	 [ 97.95726776  46.35      ]]
Train   Epoch: 381 / 600   Loss: 2.173e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5119   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 382 / 600   Loss: 1.95e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5000   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 383 / 600   Loss: 2.032e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5001   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 384 / 600   Loss: 1.874e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4825   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 385 / 600   Loss: 1.948e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4954   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.27819061   8.21666667]
	 [ 99.27812195  28.        ]
	 [ 99.27826691 224.26666667]
	 [ 99.27804565  53.7       ]
	 [ 99.27803802  33.55      ]]
Train   Epoch: 386 / 600   Loss: 1.841e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5025   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 387 / 600   Loss: 1.832e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4798   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 388 / 600   Loss: 2.058e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4713   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 389 / 600   Loss: 1.947e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4825   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 390 / 600   Loss: 2.023e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4907   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[97.72940826 41.13333333]
	 [97.72930145 50.28333333]
	 [97.72949219 96.        ]
	 [97.72939301 29.45      ]
	 [97.72925568 19.71666667]]
Train   Epoch: 391 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5000   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 392 / 600   Loss: 2.047e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4867   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 393 / 600   Loss: 1.771e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4768   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 394 / 600   Loss: 1.724e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4801   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 395 / 600   Loss: 1.981e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4745   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.39521027 23.03333333]
	 [93.39510345 42.31666667]
	 [93.39496613 36.78333333]
	 [93.39519501  8.41666667]
	 [93.3950882  52.26666667]]
Train   Epoch: 396 / 600   Loss: 1.908e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4793   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 397 / 600   Loss: 2.034e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4820   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 398 / 600   Loss: 2.038e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4717   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 399 / 600   Loss: 2.024e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4887   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 400 / 600   Loss: 2.011e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4974   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.66892242 70.05      ]
	 [98.6688385  23.13333333]
	 [98.66885376 47.08333333]
	 [98.66898346 31.6       ]
	 [98.6688385  25.46666667]]
Train   Epoch: 401 / 600   Loss: 1.96e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4865   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 402 / 600   Loss: 1.92e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4777   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 403 / 600   Loss: 1.899e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4847   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 404 / 600   Loss: 1.93e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4823   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 405 / 600   Loss: 1.919e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4891   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.6844635   32.8       ]
	 [ 95.68461609  12.9       ]
	 [ 95.68450928  46.38333333]
	 [ 95.68445587 130.86666667]
	 [ 95.68444061  53.18333333]]
Train   Epoch: 406 / 600   Loss: 1.999e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4901   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 407 / 600   Loss: 1.866e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4688   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 408 / 600   Loss: 1.857e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4937   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 409 / 600   Loss: 1.941e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4737   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 410 / 600   Loss: 1.92e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4805   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.92343903 56.68333333]
	 [94.92341614 44.83333333]
	 [94.92346954 51.11666667]
	 [94.92340851 35.58333333]
	 [94.9234314  29.        ]]
Train   Epoch: 411 / 600   Loss: 1.991e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4758   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 412 / 600   Loss: 2.166e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4921   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 413 / 600   Loss: 1.958e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4939   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 414 / 600   Loss: 1.958e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4956   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 415 / 600   Loss: 2.08e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5149   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[105.07989502  51.8       ]
	 [105.07989502  36.41666667]
	 [105.07997131  27.11666667]
	 [105.07996368  42.66666667]
	 [105.0799408   53.48333333]]
Train   Epoch: 416 / 600   Loss: 1.655e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4785   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 417 / 600   Loss: 2.053e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4840   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 418 / 600   Loss: 1.944e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4876   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 419 / 600   Loss: 1.997e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5017   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 420 / 600   Loss: 1.826e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4846   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.33747864  23.3       ]
	 [ 96.33748627  32.41666667]
	 [ 96.33744049  41.76666667]
	 [ 96.33746338  40.71666667]
	 [ 96.33747864 165.33333333]]
Train   Epoch: 421 / 600   Loss: 2.036e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5016   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 422 / 600   Loss: 1.833e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4979   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 423 / 600   Loss: 1.934e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4835   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 424 / 600   Loss: 2.033e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5014   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 425 / 600   Loss: 1.852e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5070   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[102.11549377  38.91666667]
	 [102.11542511  21.15      ]
	 [102.11547089 536.66666667]
	 [102.11545563  23.7       ]
	 [102.11547089 131.9       ]]
Train   Epoch: 426 / 600   Loss: 1.875e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4778   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 427 / 600   Loss: 1.859e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4828   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 428 / 600   Loss: 1.743e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4895   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 429 / 600   Loss:   2e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4759   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 430 / 600   Loss: 2.06e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4932   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.00341797 49.05      ]
	 [98.00343323 14.        ]
	 [98.00352478 20.83333333]
	 [98.003479   20.66666667]
	 [98.00350189 45.18333333]]
Train   Epoch: 431 / 600   Loss: 2.017e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5048   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 432 / 600   Loss: 1.915e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4991   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 433 / 600   Loss: 1.988e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5039   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 434 / 600   Loss: 1.914e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4750   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 435 / 600   Loss: 2.08e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4934   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.88063049 17.        ]
	 [99.88071442 20.41666667]
	 [99.8808136  31.13333333]
	 [99.88053894 39.75      ]
	 [99.88063049 31.46666667]]
Train   Epoch: 436 / 600   Loss: 1.834e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4769   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 437 / 600   Loss: 2.018e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4762   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 438 / 600   Loss: 2.076e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4923   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 439 / 600   Loss: 2.051e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4950   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 440 / 600   Loss: 1.982e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4819   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.18952942 39.75      ]
	 [95.18960571 54.5       ]
	 [95.18965912 45.16666667]
	 [95.18948364 35.2       ]
	 [95.18955231 22.88333333]]
Train   Epoch: 441 / 600   Loss: 1.86e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4940   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 442 / 600   Loss: 2.101e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4904   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 443 / 600   Loss: 2.071e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4825   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 444 / 600   Loss: 1.846e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4780   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 445 / 600   Loss: 1.854e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4954   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.35263062 120.75      ]
	 [ 98.35247803  49.05      ]
	 [ 98.35270691  32.06666667]
	 [ 98.35249329  11.        ]
	 [ 98.35231781  31.93333333]]
Train   Epoch: 446 / 600   Loss: 1.938e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4999   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 447 / 600   Loss: 2.051e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4822   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 448 / 600   Loss: 1.975e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4771   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 449 / 600   Loss: 1.867e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4886   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 450 / 600   Loss: 1.833e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4859   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.91346741 117.        ]
	 [ 96.91360474  40.71666667]
	 [ 96.91378021  52.58333333]
	 [ 96.91343689  46.28333333]
	 [ 96.91319275  26.21666667]]
Train   Epoch: 451 / 600   Loss: 1.788e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 452 / 600   Loss: 2.08e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4953   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 453 / 600   Loss: 1.951e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4786   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 454 / 600   Loss: 1.986e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4924   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 455 / 600   Loss: 1.935e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4909   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.49517059 58.71666667]
	 [98.49525452 37.51666667]
	 [98.49578857 18.31666667]
	 [98.49539948 15.83333333]
	 [98.49593353 23.        ]]
Train   Epoch: 456 / 600   Loss: 1.9e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4919   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 457 / 600   Loss: 2.022e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4900   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 458 / 600   Loss: 1.88e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4853   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 459 / 600   Loss: 1.898e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4763   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 460 / 600   Loss: 1.896e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4765   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.41059875 27.71666667]
	 [94.41053009 18.13333333]
	 [94.4109726  43.78333333]
	 [94.41035461 26.71666667]
	 [94.41072083 35.25      ]]
Train   Epoch: 461 / 600   Loss: 1.955e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4790   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 462 / 600   Loss: 1.965e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4814   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 463 / 600   Loss: 1.959e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4938   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 464 / 600   Loss: 1.91e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4802   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 465 / 600   Loss: 1.954e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4989   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[100.04625702  27.61666667]
	 [100.04615784  46.91666667]
	 [100.04589844  30.71666667]
	 [100.04618073  25.26666667]
	 [100.04625702  47.88333333]]
Train   Epoch: 466 / 600   Loss: 2.03e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4840   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 467 / 600   Loss: 1.972e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4717   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 468 / 600   Loss: 1.932e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4821   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 469 / 600   Loss: 2.037e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4879   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 470 / 600   Loss: 1.904e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4762   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.15014648  20.11666667]
	 [ 93.14988708  37.4       ]
	 [ 93.15007019  29.66666667]
	 [ 93.1499939   38.58333333]
	 [ 93.15002441 105.06666667]]
Train   Epoch: 471 / 600   Loss: 1.904e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4807   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 472 / 600   Loss: 1.837e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4789   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 473 / 600   Loss: 1.901e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4808   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 474 / 600   Loss: 1.844e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4901   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 475 / 600   Loss: 1.925e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4871   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.66590118 38.21666667]
	 [96.6656189  36.33333333]
	 [96.66580963 45.33333333]
	 [96.66570282 27.36666667]
	 [96.66571045 29.56666667]]
Train   Epoch: 476 / 600   Loss: 1.894e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4861   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 477 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4823   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 478 / 600   Loss: 2.095e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4838   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 479 / 600   Loss: 1.773e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4913   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 480 / 600   Loss: 1.873e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4771   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[93.7684021   9.33333333]
	 [93.76844025 57.91666667]
	 [93.76843262 19.18333333]
	 [93.76830292 33.4       ]
	 [93.76864624 12.38333333]]
Train   Epoch: 481 / 600   Loss: 1.86e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4887   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 482 / 600   Loss: 2.042e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4859   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 483 / 600   Loss: 1.91e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4767   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 484 / 600   Loss: 1.966e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4803   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 485 / 600   Loss: 1.856e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4826   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.84222412  33.93333333]
	 [ 96.84227753  91.06666667]
	 [ 96.84243011  66.3       ]
	 [ 96.8423233  368.05      ]
	 [ 96.84220123  26.5       ]]
Train   Epoch: 486 / 600   Loss: 1.884e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4745   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 487 / 600   Loss: 1.904e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4688   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 488 / 600   Loss: 1.972e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4847   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 489 / 600   Loss: 1.971e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4874   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 490 / 600   Loss: 1.768e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4858   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.36173248 64.66666667]
	 [95.36186218 21.9       ]
	 [95.36151123 49.16666667]
	 [95.36164856 24.75      ]
	 [95.3616333  23.11666667]]
Train   Epoch: 491 / 600   Loss: 1.98e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4848   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 492 / 600   Loss: 1.832e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4830   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 493 / 600   Loss: 1.901e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4812   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 494 / 600   Loss: 2.18e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4962   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 495 / 600   Loss: 2.008e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4820   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 95.70962524 131.33333333]
	 [ 95.70968628 164.55      ]
	 [ 95.70968628  52.76666667]
	 [ 95.70957947 569.46666667]
	 [ 95.70943451  50.7       ]]
Train   Epoch: 496 / 600   Loss: 1.728e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4781   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 497 / 600   Loss: 1.86e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4794   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 498 / 600   Loss: 1.994e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4969   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 499 / 600   Loss: 1.79e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4906   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 500 / 600   Loss: 1.921e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4823   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.70366669 13.61666667]
	 [94.70336151 37.38333333]
	 [94.70365906 47.55      ]
	 [94.7036972  22.        ]
	 [94.70371246 10.21666667]]
Train   Epoch: 501 / 600   Loss: 2.067e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4870   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 502 / 600   Loss: 1.919e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4878   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 503 / 600   Loss: 1.973e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5037   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 504 / 600   Loss: 2.098e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4853   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 505 / 600   Loss: 1.962e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4912   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 99.43022156 242.45      ]
	 [ 99.43045807 311.21666667]
	 [ 99.42996216  41.5       ]
	 [ 99.43001556  21.13333333]
	 [ 99.43035889  33.38333333]]
Train   Epoch: 506 / 600   Loss: 1.973e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4732   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 507 / 600   Loss: 2.052e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4897   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 508 / 600   Loss: 1.977e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4887   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 509 / 600   Loss: 2.024e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4844   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 510 / 600   Loss: 1.989e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4797   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[95.09285736 23.55      ]
	 [95.09295654 33.61666667]
	 [95.09314728 82.85      ]
	 [95.09313965 33.33333333]
	 [95.0931015  17.25      ]]
Train   Epoch: 511 / 600   Loss: 2.066e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4938   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 512 / 600   Loss: 1.835e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4864   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 513 / 600   Loss: 2.042e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4824   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 514 / 600   Loss: 1.993e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4931   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 515 / 600   Loss: 1.916e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4914   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.44353485 116.5       ]
	 [ 97.4436264   47.        ]
	 [ 97.44332123  65.18333333]
	 [ 97.44345093  85.45      ]
	 [ 97.44347382  21.25      ]]
Train   Epoch: 516 / 600   Loss: 1.972e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4947   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 517 / 600   Loss: 1.921e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4850   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 518 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4981   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 519 / 600   Loss: 1.84e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4772   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 520 / 600   Loss: 1.987e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4955   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.59359741 75.        ]
	 [99.5930481  37.71666667]
	 [99.59302521 25.33333333]
	 [99.59341431 18.5       ]
	 [99.59341431 45.        ]]
Train   Epoch: 521 / 600   Loss: 2.041e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4808   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 522 / 600   Loss: 1.877e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4695   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 523 / 600   Loss: 2.043e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4865   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 524 / 600   Loss: 2.007e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4915   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 525 / 600   Loss: 1.773e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4811   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 96.43849945 102.66666667]
	 [ 96.43843079  41.03333333]
	 [ 96.43850708  22.66666667]
	 [ 96.43864441  26.11666667]
	 [ 96.43852997 263.93333333]]
Train   Epoch: 526 / 600   Loss: 1.838e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4918   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 527 / 600   Loss: 1.966e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4945   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 528 / 600   Loss: 1.843e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4760   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 529 / 600   Loss: 2.029e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4960   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 530 / 600   Loss: 1.824e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5027   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.34399414 10.45      ]
	 [99.34360504 38.15      ]
	 [99.34384155 61.73333333]
	 [99.34366608 81.58333333]
	 [99.34381104 28.6       ]]
Train   Epoch: 531 / 600   Loss: 1.91e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4750   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 532 / 600   Loss: 1.899e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4798   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 533 / 600   Loss: 1.933e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4820   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 534 / 600   Loss: 1.874e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4800   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 535 / 600   Loss: 1.962e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4821   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.22401428 56.33333333]
	 [96.22425842 59.06666667]
	 [96.22428894 34.61666667]
	 [96.22428131 25.7       ]
	 [96.2243042  20.25      ]]
Train   Epoch: 536 / 600   Loss: 2.027e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4904   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 537 / 600   Loss: 2.1e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4942   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 538 / 600   Loss: 1.965e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4890   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 539 / 600   Loss: 1.894e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5028   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 540 / 600   Loss: 1.88e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4899   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 98.55561829  50.81666667]
	 [ 98.55580902  70.11666667]
	 [ 98.55573273 141.05      ]
	 [ 98.55574036  13.88333333]
	 [ 98.55584717  32.4       ]]
Train   Epoch: 541 / 600   Loss: 1.845e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4861   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 542 / 600   Loss: 1.982e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4732   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 543 / 600   Loss: 1.955e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4909   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 544 / 600   Loss: 2.177e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4981   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 545 / 600   Loss: 1.958e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5041   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[101.90729523 115.03333333]
	 [101.90718079  31.11666667]
	 [101.90695953  46.73333333]
	 [101.90724945  45.33333333]
	 [101.90744019 322.51666667]]
Train   Epoch: 546 / 600   Loss: 1.971e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4863   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 547 / 600   Loss: 2.058e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4853   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 548 / 600   Loss: 1.886e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4735   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 549 / 600   Loss: 2.091e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4937   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 550 / 600   Loss: 1.983e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4775   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.22114563  24.21666667]
	 [ 93.22128296  21.46666667]
	 [ 93.2215271  115.78333333]
	 [ 93.22110748  65.01666667]
	 [ 93.2208252   36.83333333]]
Train   Epoch: 551 / 600   Loss: 1.918e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4979   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 552 / 600   Loss: 2.064e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5002   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 553 / 600   Loss: 1.947e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4841   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 554 / 600   Loss: 2.09e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4991   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 555 / 600   Loss: 1.942e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4911   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.50779724 43.3       ]
	 [98.50798035 52.6       ]
	 [98.50779724 18.23333333]
	 [98.50753784 45.56666667]
	 [98.50763702 32.71666667]]
Train   Epoch: 556 / 600   Loss: 1.834e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4779   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 557 / 600   Loss: 1.899e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4991   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 558 / 600   Loss: 2.002e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4929   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 559 / 600   Loss: 1.851e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4805   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 560 / 600   Loss: 2.005e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4913   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.19634247 21.16666667]
	 [98.19654846 20.4       ]
	 [98.1962204  32.45      ]
	 [98.19582367 51.61666667]
	 [98.19602203 29.21666667]]
Train   Epoch: 561 / 600   Loss: 1.926e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4883   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 562 / 600   Loss: 1.862e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4842   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 563 / 600   Loss: 1.924e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4902   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 564 / 600   Loss: 2.075e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4987   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 565 / 600   Loss: 2.092e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4828   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 97.04096222  39.46666667]
	 [ 97.04077911  44.91666667]
	 [ 97.04071808 110.33333333]
	 [ 97.04081726  34.63333333]
	 [ 97.04023743  52.53333333]]
Train   Epoch: 566 / 600   Loss: 1.841e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4905   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 567 / 600   Loss: 1.984e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4740   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 568 / 600   Loss: 1.878e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4896   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 569 / 600   Loss: 1.889e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4890   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 570 / 600   Loss: 1.98e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4929   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[98.57366943 25.46666667]
	 [98.57354736 55.93333333]
	 [98.57344055 40.65      ]
	 [98.5734024  52.65      ]
	 [98.57373047 53.05      ]]
Train   Epoch: 571 / 600   Loss: 1.832e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4866   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 572 / 600   Loss: 2.039e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4809   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 573 / 600   Loss: 2.002e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4854   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 574 / 600   Loss: 1.988e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4879   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 575 / 600   Loss: 1.838e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4818   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[94.38925171 61.8       ]
	 [94.38928223 61.33333333]
	 [94.3892746  22.21666667]
	 [94.38934326 75.96666667]
	 [94.38925171 50.43333333]]
Train   Epoch: 576 / 600   Loss: 1.899e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4738   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 577 / 600   Loss: 1.926e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4895   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 578 / 600   Loss: 1.82e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5055   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 579 / 600   Loss: 1.981e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4863   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 580 / 600   Loss: 1.891e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4730   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[92.47290039 15.83333333]
	 [92.47301483 61.76666667]
	 [92.47309875 58.35      ]
	 [92.47324371 26.25      ]
	 [92.47349548 17.45      ]]
Train   Epoch: 581 / 600   Loss: 1.9e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4874   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 582 / 600   Loss: 1.882e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4883   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 583 / 600   Loss: 1.962e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4719   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 584 / 600   Loss: 2.035e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4910   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 585 / 600   Loss: 1.992e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4859   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[96.252388   14.8       ]
	 [96.25251007 46.33333333]
	 [96.252388   51.83333333]
	 [96.25237274 28.4       ]
	 [96.25237274 54.58333333]]
Train   Epoch: 586 / 600   Loss: 1.973e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5012   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 587 / 600   Loss: 1.871e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4906   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 588 / 600   Loss: 1.873e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4725   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 589 / 600   Loss: 1.995e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4812   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 590 / 600   Loss: 1.907e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4770   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 93.48592377  13.88333333]
	 [ 93.48588562 108.        ]
	 [ 93.48564911  45.75      ]
	 [ 93.4855423   44.41666667]
	 [ 93.48570251  24.5       ]]
Train   Epoch: 591 / 600   Loss: 1.864e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4806   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 592 / 600   Loss: 2.009e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4824   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 593 / 600   Loss: 1.997e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4801   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 594 / 600   Loss: 1.908e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4753   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 595 / 600   Loss: 1.836e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4991   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[101.31708527  28.        ]
	 [101.31667328  39.7       ]
	 [101.31661224  62.3       ]
	 [101.31658936  25.55      ]
	 [101.31686401  34.31666667]]
Train   Epoch: 596 / 600   Loss: 2.32e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4985   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 597 / 600   Loss: 1.999e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    5006   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 598 / 600   Loss: 1.774e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4916   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 599 / 600   Loss: 2.038e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4935   Precision: 12.620%   Recall: 100.000%
Train   Epoch: 600 / 600   Loss: 1.882e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss:    4972   Precision: 12.620%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[99.7092514  37.86666667]
	 [99.70359039 60.76666667]
	 [99.70259857 27.25      ]
	 [99.70911407 20.96666667]
	 [99.7032547  31.5       ]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split5 </span>

<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">dust_prediction_collate</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split5</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model_split5</span> <span class="o">=</span> <span class="n">model_split5</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split5</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>

<span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 600   Loss: 1.436e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.406e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 600   Loss: 1.279e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 600   Loss: 1.203e+04   Precision: 39.840%   Recall: 42.280%
Valid                   Loss: 1.366e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 004 / 600   Loss: 1.19e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 005 / 600   Loss: 1.261e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.21205139 10.25      ]
	 [89.2120285  38.95      ]
	 [89.21198273 39.31666667]
	 [89.21203613 31.03333333]
	 [89.21201324 25.06666667]]
Train   Epoch: 006 / 600   Loss: 1.165e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 007 / 600   Loss: 1.199e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 008 / 600   Loss: 1.283e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 009 / 600   Loss: 1.166e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.373e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 010 / 600   Loss: 1.192e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.72013092  44.81666667]
	 [ 88.7201767   29.68333333]
	 [ 88.72018433 157.31666667]
	 [ 88.72011566  21.21666667]
	 [ 88.72018433  51.83333333]]
Train   Epoch: 011 / 600   Loss: 1.178e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 012 / 600   Loss: 1.135e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.372e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 013 / 600   Loss: 1.196e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 014 / 600   Loss: 1.227e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 015 / 600   Loss: 1.267e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.67120361 37.5       ]
	 [91.67128754 79.38333333]
	 [91.67124939 30.        ]
	 [91.67124939 33.96666667]
	 [91.67127228 23.63333333]]
Train   Epoch: 016 / 600   Loss: 1.178e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 017 / 600   Loss: 1.2e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 018 / 600   Loss: 1.208e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 019 / 600   Loss: 1.163e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 020 / 600   Loss: 1.222e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.96572113 10.21666667]
	 [89.96572876 28.66666667]
	 [89.96566772 22.03333333]
	 [89.96570587  6.1       ]
	 [89.96569824 39.11666667]]
Train   Epoch: 021 / 600   Loss: 1.242e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 022 / 600   Loss: 1.231e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 023 / 600   Loss: 1.238e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 024 / 600   Loss: 1.208e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 025 / 600   Loss: 1.19e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.26791382 23.55      ]
	 [88.26786804 40.51666667]
	 [88.26789856 73.61666667]
	 [88.26790619 23.11666667]
	 [88.2678833  32.28333333]]
Train   Epoch: 026 / 600   Loss: 1.215e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 027 / 600   Loss: 1.292e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 028 / 600   Loss: 1.183e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 029 / 600   Loss: 1.132e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.372e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 030 / 600   Loss: 1.228e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.65080261 46.75      ]
	 [90.65080261 25.06666667]
	 [90.65077972 80.45      ]
	 [90.65079498 37.35      ]
	 [90.65080261 29.        ]]
Train   Epoch: 031 / 600   Loss: 1.239e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 032 / 600   Loss: 1.271e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 033 / 600   Loss: 1.202e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 034 / 600   Loss: 1.179e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 035 / 600   Loss: 1.2e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[91.04020691 31.75      ]
	 [91.04022217 90.25      ]
	 [91.0402298  29.33333333]
	 [91.04020691 33.55      ]
	 [91.04019928 13.5       ]]
Train   Epoch: 036 / 600   Loss: 1.149e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 037 / 600   Loss: 1.177e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 038 / 600   Loss: 1.219e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 039 / 600   Loss: 1.213e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 040 / 600   Loss: 1.191e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.373e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[86.86468506 31.8       ]
	 [86.86444855 41.13333333]
	 [86.86456299 29.55      ]
	 [86.86456299 29.5       ]
	 [86.86446381 32.08333333]]
Train   Epoch: 041 / 600   Loss: 1.215e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 042 / 600   Loss: 1.278e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.383e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 043 / 600   Loss: 1.229e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 044 / 600   Loss: 1.212e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.383e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 045 / 600   Loss: 1.191e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.58115387 75.25      ]
	 [89.58113098 39.78333333]
	 [89.58110809 85.13333333]
	 [89.58113861 26.71666667]
	 [89.58117676 49.36666667]]
Train   Epoch: 046 / 600   Loss: 1.228e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 047 / 600   Loss: 1.234e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 048 / 600   Loss: 1.213e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 049 / 600   Loss: 1.219e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 050 / 600   Loss: 1.283e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.63420868 26.08333333]
	 [89.63421631 65.05      ]
	 [89.63424683 67.36666667]
	 [89.63430023 22.        ]
	 [89.63426971 22.23333333]]
Train   Epoch: 051 / 600   Loss: 1.202e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 052 / 600   Loss: 1.186e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 053 / 600   Loss: 1.227e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 054 / 600   Loss: 1.235e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 055 / 600   Loss: 1.173e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.30535126  40.        ]
	 [ 88.30549622  15.78333333]
	 [ 88.30536652 100.33333333]
	 [ 88.30538177  28.76666667]
	 [ 88.30545044  85.96666667]]
Train   Epoch: 056 / 600   Loss: 1.231e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 057 / 600   Loss: 1.209e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 058 / 600   Loss: 1.132e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.373e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 059 / 600   Loss: 1.162e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 060 / 600   Loss: 1.176e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 88.32776642  33.93333333]
	 [ 88.32769012  52.83333333]
	 [ 88.32770538  35.5       ]
	 [ 88.32769775  32.31666667]
	 [ 88.32777405 250.5       ]]
Train   Epoch: 061 / 600   Loss: 1.221e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 062 / 600   Loss: 1.186e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 063 / 600   Loss: 1.223e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 064 / 600   Loss: 1.285e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 065 / 600   Loss: 1.141e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[88.11180878 34.05      ]
	 [88.11182404 24.78333333]
	 [88.11192322 35.45      ]
	 [88.11186218 32.73333333]
	 [88.11192322 64.        ]]
Train   Epoch: 066 / 600   Loss: 1.185e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 067 / 600   Loss: 1.216e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 068 / 600   Loss: 1.242e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 069 / 600   Loss: 1.194e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 070 / 600   Loss: 1.211e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.10179901 47.16666667]
	 [90.1019516  55.01666667]
	 [90.10198212 35.5       ]
	 [90.10180664 39.55      ]
	 [90.10183716 27.46666667]]
Train   Epoch: 071 / 600   Loss: 1.218e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 072 / 600   Loss: 1.267e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 073 / 600   Loss: 1.229e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 074 / 600   Loss: 1.213e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 075 / 600   Loss: 1.194e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.94510651 36.7       ]
	 [87.94510651 40.83333333]
	 [87.94512939 13.38333333]
	 [87.94512177 43.05      ]
	 [87.94509125 47.21666667]]
Train   Epoch: 076 / 600   Loss: 1.238e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 077 / 600   Loss: 1.214e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 078 / 600   Loss: 1.153e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 079 / 600   Loss: 1.249e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 080 / 600   Loss: 1.281e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.382e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 91.80579376  15.        ]
	 [ 91.80596161  32.5       ]
	 [ 91.80773163  26.76666667]
	 [ 91.80679321  24.83333333]
	 [ 91.80784607 896.58333333]]
Train   Epoch: 081 / 600   Loss: 1.212e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 082 / 600   Loss: 1.234e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 083 / 600   Loss: 1.268e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 084 / 600   Loss: 1.22e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 085 / 600   Loss: 1.2e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[90.90238953 20.16666667]
	 [90.90234375 29.5       ]
	 [90.90226746 69.05      ]
	 [90.90203857 13.18333333]
	 [90.90258789 39.55      ]]
Train   Epoch: 086 / 600   Loss: 1.224e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 087 / 600   Loss: 1.196e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 088 / 600   Loss: 1.209e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 089 / 600   Loss: 1.166e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 090 / 600   Loss: 1.163e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.375e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.98146057 11.8       ]
	 [87.98144531 49.16666667]
	 [87.98143768 21.        ]
	 [87.98145294 24.88333333]
	 [87.98145294 40.66666667]]
Train   Epoch: 091 / 600   Loss: 1.194e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 092 / 600   Loss: 1.265e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.383e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 093 / 600   Loss: 1.15e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.372e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 094 / 600   Loss: 1.22e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 095 / 600   Loss: 1.222e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.35726929 38.53333333]
	 [89.35729218 27.01666667]
	 [89.35729218  9.5       ]
	 [89.35728455 63.71666667]
	 [89.35725403 19.45      ]]
Train   Epoch: 096 / 600   Loss: 1.173e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.38e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 097 / 600   Loss: 1.202e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.379e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 098 / 600   Loss: 1.208e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.381e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 099 / 600   Loss: 1.274e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 100 / 600   Loss: 1.196e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[89.59230804 35.86666667]
	 [89.59227753 42.        ]
	 [89.59229279 42.43333333]
	 [89.59224701 55.93333333]
	 [89.59228516 43.45      ]]
Train   Epoch: 101 / 600   Loss: 1.18e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 102 / 600   Loss: 1.233e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.377e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 103 / 600   Loss: 1.172e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.376e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 104 / 600   Loss: 1.167e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.378e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 105 / 600   Loss: 1.185e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.374e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[87.01732635 35.68333333]
	 [87.01725769 47.41666667]
	 [87.01757812 23.4       ]
	 [87.01724243 45.05      ]
	 [87.01735687 20.33333333]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-10-0b6453266cd4&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span> num_epochs <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">600</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span> 
<span class="ansi-green-fg">---&gt; 28</span><span class="ansi-red-fg"> train_losses_split5,valid_losses_split5 = train_loop(model_split5, optimizer, train_loader, valid_loader, 
</span><span class="ansi-green-intense-fg ansi-bold">     29</span>                                                      device<span class="ansi-blue-fg">,</span> epochs<span class="ansi-blue-fg">=</span>num_epochs<span class="ansi-blue-fg">,</span> valid_every<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>loss_cfg<span class="ansi-blue-fg">=</span>loss_cfg<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>                                                      sample_predictions_every<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">,</span> sample_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">,</span> sample_cols<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/work/projects/dust/dust_prediction_using_vit/packages/training/train_model.py</span> in <span class="ansi-cyan-fg">train_loop</span><span class="ansi-blue-fg">(model, optimizer, train_loader, valid_loader, device, epochs, valid_every, loss_cfg, sample_predictions_every, sample_size, sample_cols, loss_plot_end, debug)</span>
<span class="ansi-green-intense-fg ansi-bold">     59</span>     <span class="ansi-green-fg">if</span> loss_cfg <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> loss_cfg <span class="ansi-blue-fg">=</span> LossConfig<span class="ansi-blue-fg">(</span>device<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     60</span>     <span class="ansi-green-fg">for</span> epoch <span class="ansi-green-fg">in</span> range<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> epochs <span class="ansi-blue-fg">+</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 61</span><span class="ansi-red-fg">         </span>train_loss<span class="ansi-blue-fg">,</span>train_prec<span class="ansi-blue-fg">,</span>train_recall <span class="ansi-blue-fg">=</span> train_epoch<span class="ansi-blue-fg">(</span>model<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> train_loader<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">,</span> loss_cfg<span class="ansi-blue-fg">,</span> debug<span class="ansi-blue-fg">=</span>debug<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     62</span>         train_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>train_loss<span class="ansi-blue-fg">.</span>avg<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     63</span>         print(&#39;Train&#39;, f&#39;Epoch: {epoch:03d} / {epochs:03d}&#39;,

<span class="ansi-green-fg">~/work/projects/dust/dust_prediction_using_vit/packages/training/train_model.py</span> in <span class="ansi-cyan-fg">train_epoch</span><span class="ansi-blue-fg">(model, optimizer, loader, device, loss_cfg, debug)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span>         loss <span class="ansi-blue-fg">=</span> dust_loss<span class="ansi-blue-fg">(</span>pred<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> loss_cfg<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     23</span>         loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">         </span>loss_metric<span class="ansi-blue-fg">.</span>update<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>         optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>         tp<span class="ansi-blue-fg">,</span>fp<span class="ansi-blue-fg">,</span>fn <span class="ansi-blue-fg">=</span> tp_fp_fn_batch<span class="ansi-blue-fg">(</span>pred<span class="ansi-blue-fg">,</span>y<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split5 - changing learning rates</span>

<span class="kn">from</span> <span class="nn">training.dust_loss</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split5</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model_split5</span> <span class="o">=</span> <span class="n">model_split5</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split5</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span> <span class="o">=</span> <span class="p">[],[]</span>


<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_losses_split5</span><span class="o">+=</span><span class="n">train_losses</span>
<span class="n">valid_losses_split5</span><span class="o">+=</span><span class="n">valid_losses</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">70</span>
<span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_losses_split5</span><span class="o">+=</span><span class="n">train_losses</span>
<span class="n">valid_losses_split5</span><span class="o">+=</span><span class="n">valid_losses</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.000001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">train_losses</span><span class="p">,</span><span class="n">valid_losses</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_losses_split5</span><span class="o">+=</span><span class="n">train_losses</span>
<span class="n">valid_losses_split5</span><span class="o">+=</span><span class="n">valid_losses</span>

<span class="kn">from</span> <span class="nn">utils.training_loop_plotting</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plot_train_valid</span><span class="p">(</span><span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 010   Loss: 1.595e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.549e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 010   Loss: 1.497e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.548e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 010   Loss: 1.495e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.546e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 010   Loss: 1.544e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.544e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 005 / 010   Loss: 1.612e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.543e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[   8.03948689   38.33333333]
	 [   8.03992081 2162.        ]
	 [   8.03834343   26.        ]
	 [   8.03960514   63.65      ]
	 [   8.03843689   40.03333333]]
Train   Epoch: 006 / 010   Loss: 1.498e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.541e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 007 / 010   Loss: 1.519e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.539e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 010   Loss: 1.56e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.538e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 009 / 010   Loss: 1.545e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.536e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 010   Loss: 1.436e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.534e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[  9.17464447  49.2       ]
	 [  9.17542839 188.88333333]
	 [  9.1746273   51.93333333]
	 [  9.17443657  36.        ]
	 [  9.17459679  31.5       ]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 070   Loss: 1.537e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.532e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 070   Loss: 1.564e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.531e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 070   Loss: 1.471e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.529e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 070   Loss: 1.465e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.527e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 005 / 070   Loss: 1.503e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.525e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[10.30138874 35.88333333]
	 [10.30169868 36.28333333]
	 [10.30262089  8.98333333]
	 [10.30182076 23.11666667]
	 [10.30095482 47.33333333]]
Train   Epoch: 006 / 070   Loss: 1.473e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.523e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 007 / 070   Loss: 1.522e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.521e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 070   Loss: 1.523e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.519e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 009 / 070   Loss: 1.509e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.517e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 070   Loss: 1.526e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.516e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[11.68853283 19.83333333]
	 [11.68941021 38.21666667]
	 [11.68930149 70.66666667]
	 [11.6898489  30.33333333]
	 [11.69071102 14.76666667]]
Train   Epoch: 011 / 070   Loss: 1.478e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.514e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 070   Loss: 1.54e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.512e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 013 / 070   Loss: 1.594e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.51e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 070   Loss: 1.5e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.508e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 015 / 070   Loss: 1.572e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.506e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[13.02407646 27.5       ]
	 [13.02425098 41.23333333]
	 [13.02550983 16.88333333]
	 [13.02421474 43.93333333]
	 [13.02395344 25.83333333]]
Train   Epoch: 016 / 070   Loss: 1.428e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.504e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 017 / 070   Loss: 1.533e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.503e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 070   Loss: 1.489e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.501e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 019 / 070   Loss: 1.511e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.499e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 020 / 070   Loss: 1.429e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.497e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[14.1399498  26.33333333]
	 [14.13867474 46.46666667]
	 [14.13801575 74.25      ]
	 [14.1401577  31.91666667]
	 [14.14078045 42.75      ]]
Train   Epoch: 021 / 070   Loss: 1.439e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.495e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 022 / 070   Loss: 1.488e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.494e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 023 / 070   Loss: 1.453e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.492e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 024 / 070   Loss: 1.439e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.49e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 025 / 070   Loss: 1.44e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.488e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[15.64174271 56.13333333]
	 [15.64435387 30.78333333]
	 [15.64181614 50.78333333]
	 [15.64417171 15.65      ]
	 [15.64121914 32.        ]]
Train   Epoch: 026 / 070   Loss: 1.438e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.487e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 027 / 070   Loss: 1.483e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.485e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 028 / 070   Loss: 1.39e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.483e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 029 / 070   Loss: 1.417e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.481e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 030 / 070   Loss: 1.428e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.48e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 16.95978546  43.91666667]
	 [ 16.96245384  31.83333333]
	 [ 16.96230125  34.5       ]
	 [ 16.96108055  26.28333333]
	 [ 16.9646759  447.91666667]]
Train   Epoch: 031 / 070   Loss: 1.401e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.478e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 032 / 070   Loss: 1.46e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.476e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 033 / 070   Loss: 1.378e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.475e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 034 / 070   Loss: 1.462e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.473e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 035 / 070   Loss: 1.482e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.472e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[18.40486336 21.61666667]
	 [18.40770531 20.        ]
	 [18.40602303 25.01666667]
	 [18.41023064  7.58333333]
	 [18.40791893 24.65      ]]
Train   Epoch: 036 / 070   Loss: 1.5e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.47e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 037 / 070   Loss: 1.474e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.468e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 038 / 070   Loss: 1.39e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.467e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 039 / 070   Loss: 1.494e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.465e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 040 / 070   Loss: 1.432e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.463e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[19.7557373  16.91666667]
	 [19.75442314 40.93333333]
	 [19.75309753 25.41666667]
	 [19.75417328 25.41666667]
	 [19.75790977 22.85      ]]
Train   Epoch: 041 / 070   Loss: 1.371e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.462e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 042 / 070   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.46e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 043 / 070   Loss: 1.416e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.459e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 044 / 070   Loss: 1.41e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.457e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 045 / 070   Loss: 1.392e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.456e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[21.13384819 84.5       ]
	 [21.1293602  25.85      ]
	 [21.12927818 25.        ]
	 [21.13012695  8.13333333]
	 [21.12964821 52.55      ]]
Train   Epoch: 046 / 070   Loss: 1.38e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.454e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 047 / 070   Loss: 1.466e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.453e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 048 / 070   Loss: 1.335e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.451e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 049 / 070   Loss: 1.445e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.45e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 050 / 070   Loss: 1.501e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.448e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[22.58285332 32.11666667]
	 [22.58276558 54.5       ]
	 [22.58367348 14.46666667]
	 [22.58394051 49.78333333]
	 [22.58212471 27.83333333]]
Train   Epoch: 051 / 070   Loss: 1.423e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.447e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 052 / 070   Loss: 1.371e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.445e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 053 / 070   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.444e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 054 / 070   Loss: 1.448e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.442e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 055 / 070   Loss: 1.42e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.441e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 23.84814072  24.8       ]
	 [ 23.8437767   36.88333333]
	 [ 23.84711075  48.66666667]
	 [ 23.84734535  39.21666667]
	 [ 23.84285927 214.01666667]]
Train   Epoch: 056 / 070   Loss: 1.367e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.44e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 057 / 070   Loss: 1.423e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.438e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 058 / 070   Loss: 1.329e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.437e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 059 / 070   Loss: 1.377e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.435e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 060 / 070   Loss: 1.424e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.434e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[25.32695389 25.9       ]
	 [25.32507515 30.2       ]
	 [25.33243179 16.45      ]
	 [25.3279171  37.16666667]
	 [25.3268795  26.56666667]]
Train   Epoch: 061 / 070   Loss: 1.371e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.433e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 062 / 070   Loss: 1.375e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.431e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 063 / 070   Loss: 1.371e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.43e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 064 / 070   Loss: 1.347e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.429e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 065 / 070   Loss: 1.364e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.427e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[26.68669128 44.58333333]
	 [26.68363762 12.45      ]
	 [26.68887138 48.66666667]
	 [26.68455124 28.43333333]
	 [26.68680763 14.75      ]]
Train   Epoch: 066 / 070   Loss: 1.348e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.426e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 067 / 070   Loss: 1.385e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.425e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 068 / 070   Loss: 1.39e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.423e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 069 / 070   Loss: 1.395e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.422e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 070 / 070   Loss: 1.408e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.421e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[28.07581139 11.91666667]
	 [28.07323647 37.23333333]
	 [28.07658768 17.11666667]
	 [28.07481956 27.33333333]
	 [28.0797863  46.21666667]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 012   Loss: 1.397e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.42e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 012   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.418e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 012   Loss: 1.373e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.417e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 012   Loss: 1.291e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.416e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 005 / 012   Loss: 1.351e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.415e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[29.51345634 36.66666667]
	 [29.51219559 32.5       ]
	 [29.51989365  9.21666667]
	 [29.51305199 40.33333333]
	 [29.5141201  63.78333333]]
Train   Epoch: 006 / 012   Loss: 1.496e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.414e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 007 / 012   Loss: 1.406e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.412e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 012   Loss: 1.472e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.411e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 009 / 012   Loss: 1.297e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.41e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 012   Loss: 1.374e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.409e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[ 31.00121498 221.98333333]
	 [ 31.00292397  24.53333333]
	 [ 31.01077461   9.21666667]
	 [ 31.00582314  29.05      ]
	 [ 31.00196838  58.8       ]]
Train   Epoch: 011 / 012   Loss: 1.363e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.408e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 012   Loss: 1.33e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.407e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[31.47332001 65.83333333]
	 [31.47240829 44.58333333]
	 [31.46573257 41.66666667]
	 [31.47375488 19.08333333]
	 [31.47436333 30.25      ]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split5 - different lr, more epchs, larger batch, dropout, loss with decaying_weights</span>

<span class="kn">from</span> <span class="nn">training.dust_loss</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">LossConfig</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">decaying_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_train_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">DustPredictionDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meteorology_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">dust_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">metadata_times_valid_paths</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">collate_fn</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">sample_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample data loading:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model_split5</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">81</span><span class="p">,</span><span class="mi">81</span><span class="p">),</span> <span class="n">patch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">representation_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distilled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">attn_drop_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model_split5</span> <span class="o">=</span> <span class="n">model_split5</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># to be used inside the dust_loss</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_split5</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">30</span>

<span class="n">train_losses_split5</span><span class="p">,</span><span class="n">valid_losses_split5</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">model_split5</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> 
                                                     <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">valid_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">loss_cfg</span><span class="o">=</span><span class="n">loss_cfg</span><span class="p">,</span>
                                                     <span class="n">sample_predictions_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sample_cols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                     <span class="n">loss_plot_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Sample data loading:
torch.Size([512, 17, 81, 81]) torch.Size([512, 10]) 512
Training... (Precision = out all of predicted events, &lt;&gt; were correct, Recall = out of all events, predicted &lt;&gt;)


Train   Epoch: 001 / 030   Loss: 1.509e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.533e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 002 / 030   Loss: 1.52e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.516e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 003 / 030   Loss: 1.415e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.498e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 004 / 030   Loss: 1.439e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.479e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 005 / 030   Loss: 1.395e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.463e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[19.53932571 36.45      ]
	 [19.54071236  6.05      ]
	 [19.5401268  34.66666667]
	 [19.5392189  97.7       ]
	 [19.53933907 21.86666667]]
Train   Epoch: 006 / 030   Loss: 1.395e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.447e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 007 / 030   Loss: 1.382e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.432e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 008 / 030   Loss: 1.423e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.419e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 009 / 030   Loss: 1.375e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.407e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 010 / 030   Loss: 1.427e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.396e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[34.2904892  66.66666667]
	 [34.28915405 35.08333333]
	 [34.2910614  17.28333333]
	 [34.29016113 35.38333333]
	 [34.29010391 45.55      ]]
Train   Epoch: 011 / 030   Loss: 1.4e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.387e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 012 / 030   Loss: 1.342e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.379e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 013 / 030   Loss: 1.362e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.372e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 014 / 030   Loss: 1.322e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.366e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 015 / 030   Loss: 1.191e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.362e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[48.26428223 33.        ]
	 [48.2647171  28.66666667]
	 [48.26360321 40.03333333]
	 [48.26534653 28.38333333]
	 [48.26403809 25.15      ]]
Train   Epoch: 016 / 030   Loss: 1.31e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.358e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 017 / 030   Loss: 1.303e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.356e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 018 / 030   Loss: 1.214e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.354e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 019 / 030   Loss: 1.313e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 020 / 030   Loss: 1.202e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[60.60235214 21.31666667]
	 [60.60061646 70.88333333]
	 [60.60036469 26.88333333]
	 [60.60070419 67.33333333]
	 [60.60091019 69.96666667]]
Train   Epoch: 021 / 030   Loss: 1.251e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 022 / 030   Loss: 1.261e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.352e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 023 / 030   Loss: 1.251e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.353e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 024 / 030   Loss: 1.245e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.354e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 025 / 030   Loss: 1.165e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.355e+04   Precision: 0.000%   Recall: 0.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[70.63002777 38.        ]
	 [70.63056946 41.38333333]
	 [70.63008881 63.36666667]
	 [70.63033295 55.48333333]
	 [70.63040161 22.08333333]]
Train   Epoch: 026 / 030   Loss: 1.21e+04   Precision: 0.000%   Recall: 0.000%
Valid                   Loss: 1.357e+04   Precision: 0.000%   Recall: 0.000%
Train   Epoch: 027 / 030   Loss: 1.259e+04   Precision: 39.307%   Recall: 13.521%
Valid                   Loss: 1.358e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 028 / 030   Loss: 1.219e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.36e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 029 / 030   Loss: 1.297e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.361e+04   Precision: 14.085%   Recall: 100.000%
Train   Epoch: 030 / 030   Loss: 1.229e+04   Precision: 39.842%   Recall: 100.000%
Valid                   Loss: 1.363e+04   Precision: 14.085%   Recall: 100.000%
        [Sample predictions | targets] (cols [:1] |  cols [1:]):
	[[78.01647186 97.5       ]
	 [78.01647949 22.2       ]
	 [78.01637268 33.91666667]
	 [78.01661682  9.75      ]
	 [78.01683044 50.65      ]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # some tests with loss</span>
<span class="c1"># import torch.nn as nn</span>
<span class="c1"># loss_test = nn.MSELoss(reduction=&quot;none&quot;)</span>
<span class="c1"># t1 = torch.ones([3,5])</span>
<span class="c1"># t1[1,:] = 2</span>
<span class="c1"># t1[2,2] = 5</span>
<span class="c1"># w = torch.tensor([2.,1.,1.,1.,1.])</span>
<span class="c1"># t2 = torch.zeros([3,5])</span>
<span class="c1"># print(t1, loss_test(t1,t2), loss_test(t1,t2).mean())</span>
<span class="c1"># print(w*loss_test(t1,t2), (w*loss_test(t1,t2)).mean())</span>
<span class="c1"># print(loss_test(t1,t2).mean(0))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from utils.metrics import *</span>
<span class="c1"># tensor_metric = Metric()</span>
<span class="c1"># tensor_metric.update(torch.ones([5]),1)</span>
<span class="c1"># tensor_metric.update(torch.zeros([5]),1)</span>
<span class="c1"># tensor_metric.update(torch.ones([5])*2.,3)</span>
<span class="c1"># losses_list = [tensor_metric.avg,2*tensor_metric.avg,2*tensor_metric.avg]</span>
<span class="c1"># print(losses_list)</span>
<span class="c1"># all_losses = torch.stack(losses_list,0).cpu().detach().numpy()</span>
<span class="c1"># print(all_losses)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
